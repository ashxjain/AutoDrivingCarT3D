{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AutoDrivingCarT3D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.16"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WXu1r8qvSzWf"
      },
      "source": [
        "# Twin-Delayed DDPG\n",
        "\n",
        "Complete credit goes to this [awesome Deep Reinforcement Learning 2.0 Course on Udemy](https://www.udemy.com/course/deep-reinforcement-learning/) for the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIKSm5DiTOIC",
        "colab_type": "text"
      },
      "source": [
        "# Understanding Implementation\n",
        "* Because we can't run Kivy on Colab and because we don't have GPUs on our desktop, following is the strategy to train\n",
        "* Load the sand image and move the coordinates (Vector) in env.Step() for simulating car movement.\n",
        "* State is captured by cropping a portion of sand image from car's position. And then rotating it the direction of car in such a way that car's orientation is horizontal i.e 0 degrees from x-axis. This state is passed to Actor network\n",
        "* Action is 2 dimensional. 1st being velocity-x value and 2nd being angle\n",
        "* To concat action with state for Critic network, we repeat velocity-x value state_dim's height times, as one row and angle as another row. This way critic network input dimension becomes (state_height + 2 * state_width) where state_height & state_width comes from state dimension and 2 comes from action dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xjm2onHdT-Av"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1oeheFtR9ZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "90e5f2ff-3c01-41c1-b3e4-7e56f7becd41"
      },
      "source": [
        "!pip install kivy"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kivy in /usr/local/lib/python2.7/dist-packages (1.11.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python2.7/dist-packages (from kivy) (2.1.3)\n",
            "Requirement already satisfied: Kivy-Garden>=0.1.4 in /usr/local/lib/python2.7/dist-packages (from kivy) (0.1.4)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python2.7/dist-packages (from kivy) (0.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python2.7/dist-packages (from Kivy-Garden>=0.1.4->kivy) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests->Kivy-Garden>=0.1.4->kivy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests->Kivy-Garden>=0.1.4->kivy) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests->Kivy-Garden>=0.1.4->kivy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests->Kivy-Garden>=0.1.4->kivy) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ikr2p0Js8iB4",
        "colab": {}
      },
      "source": [
        "%matplotlib notebook\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n",
        "from PIL import Image as PILImage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y2nGdtlKVydr"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u5rW0IDB8nTO",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state.cpu(), copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppZ7G3DACBPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jb7TTaHxWbQD"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rb1ukP8YS-YV",
        "colab": {}
      },
      "source": [
        "def conv2d_block(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.BatchNorm2d(in_channels),\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3, 3), padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(3, 3), padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.Dropout2d(0.1)\n",
        "    )\n",
        "\n",
        "def transition_block(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1, 1), padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(0.1)\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8GcWn3yUQpkl",
        "colab": {}
      },
      "source": [
        "# Creating the architecture of the Neural Network\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        # size after below block = input_size - 2\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=0),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # size after below block = input_size - 2 - 4\n",
        "        self.convblock2 = conv2d_block(in_channels=16, out_channels=16)\n",
        "        # size after below block = (input_size - 2 - 4) / 2\n",
        "        self.transitionblock = transition_block(in_channels=16, out_channels=10)\n",
        "        # size after below block = ((input_size - 2 - 4) / 2) - 4\n",
        "        self.convblock3 = conv2d_block(in_channels=10, out_channels=16)\n",
        "        # size after below block = ((input_size - 2 - 4) / 2) - 4\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10)\n",
        "        )\n",
        "        conv_output_size_h = ((state_dim[0] - 2 - 4) / 2) - 4\n",
        "        conv_output_size_w = ((state_dim[1] - 2 - 4) / 2) - 4\n",
        "        linear_input_size = conv_output_size_w * conv_output_size_h * 10\n",
        "        self.fc = nn.Linear(linear_input_size, action_dim)\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.transitionblock(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.fc(x.view(x.size(0), -1))\n",
        "        x = self.max_action * torch.tanh(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4VLNPzfW9wP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "outputId": "99c1211b-2077-4dcf-e7e2-8c6ebd18934a"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = Actor((32,32),2,10).to(device)\n",
        "summary(model, input_size=(1, 32, 32))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 30, 30]             160\n",
            "              ReLU-2           [-1, 16, 30, 30]               0\n",
            "       BatchNorm2d-3           [-1, 16, 30, 30]              32\n",
            "            Conv2d-4           [-1, 16, 28, 28]           2,320\n",
            "              ReLU-5           [-1, 16, 28, 28]               0\n",
            "       BatchNorm2d-6           [-1, 16, 28, 28]              32\n",
            "            Conv2d-7           [-1, 16, 26, 26]           2,320\n",
            "              ReLU-8           [-1, 16, 26, 26]               0\n",
            "       BatchNorm2d-9           [-1, 16, 26, 26]              32\n",
            "        Dropout2d-10           [-1, 16, 26, 26]               0\n",
            "           Conv2d-11           [-1, 10, 26, 26]             170\n",
            "             ReLU-12           [-1, 10, 26, 26]               0\n",
            "        MaxPool2d-13           [-1, 10, 13, 13]               0\n",
            "        Dropout2d-14           [-1, 10, 13, 13]               0\n",
            "      BatchNorm2d-15           [-1, 10, 13, 13]              20\n",
            "           Conv2d-16           [-1, 16, 11, 11]           1,456\n",
            "             ReLU-17           [-1, 16, 11, 11]               0\n",
            "      BatchNorm2d-18           [-1, 16, 11, 11]              32\n",
            "           Conv2d-19             [-1, 16, 9, 9]           2,320\n",
            "             ReLU-20             [-1, 16, 9, 9]               0\n",
            "      BatchNorm2d-21             [-1, 16, 9, 9]              32\n",
            "        Dropout2d-22             [-1, 16, 9, 9]               0\n",
            "           Conv2d-23             [-1, 10, 9, 9]             170\n",
            "             ReLU-24             [-1, 10, 9, 9]               0\n",
            "      BatchNorm2d-25             [-1, 10, 9, 9]              20\n",
            "           Linear-26                    [-1, 2]           1,622\n",
            "================================================================\n",
            "Total params: 10,738\n",
            "Trainable params: 10,738\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.19\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 1.24\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HRDDce8FXef7"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5svIwtleSxLW",
        "colab": {}
      },
      "source": [
        "# Creating the architecture of the Neural Network\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        # Defining the first Critic neural network\n",
        "        # size after below block = input_size - 2\n",
        "        self.c1_convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=0),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # size after below block = input_size - 2 - 4\n",
        "        self.c1_convblock2 = conv2d_block(in_channels=16, out_channels=16)\n",
        "        # size after below block = (input_size - 2 - 4) / 2\n",
        "        self.c1_transitionblock = transition_block(in_channels=16, out_channels=10)\n",
        "        # size after below block = ((input_size - 2 - 4) / 2) - 4\n",
        "        self.c1_convblock3 = conv2d_block(in_channels=10, out_channels=16)\n",
        "        # size after below block = ((input_size - 2 - 4) / 2) - 4\n",
        "        self.c1_convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10)\n",
        "        )\n",
        "        conv_output_size_h = ((state_dim[0] + action_dim - 2 - 4) / 2) - 4\n",
        "        conv_output_size_w = ((state_dim[1] - 2 - 4) / 2) - 4\n",
        "        linear_input_size = conv_output_size_w * conv_output_size_h * 10\n",
        "        self.c1_fc = nn.Linear(linear_input_size, 1)\n",
        "\n",
        "        # Defining the second Critic neural network\n",
        "        # size after below block = input_size - 2\n",
        "        self.c2_convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=0),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # size after below block = input_size - 2 - 4\n",
        "        self.c2_convblock2 = conv2d_block(in_channels=16, out_channels=16)\n",
        "        # size after below block = (input_size - 2 - 4) / 2\n",
        "        self.c2_transitionblock = transition_block(in_channels=16, out_channels=10)\n",
        "        # size after below block = ((input_size - 2 - 4) / 2) - 4\n",
        "        self.c2_convblock3 = conv2d_block(in_channels=10, out_channels=16)\n",
        "        # size after below block = ((input_size - 2 - 4) / 2) - 4\n",
        "        self.c2_convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10)\n",
        "        )\n",
        "        conv_output_size_h = ((state_dim[0] + action_dim - 2 - 4) / 2) - 4\n",
        "        conv_output_size_w = ((state_dim[1] - 2 - 4) / 2) - 4\n",
        "        linear_input_size = conv_output_size_w * conv_output_size_h * 10\n",
        "        self.c2_fc = nn.Linear(linear_input_size, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "      xu = torch.cat([x, u], 2)\n",
        "      # Forward-Propagation on the first Critic Neural Network\n",
        "      x1 = self.c1_convblock1(xu)\n",
        "      x1 = self.c1_convblock2(x1)\n",
        "      x1 = self.c1_transitionblock(x1)\n",
        "      x1 = self.c1_convblock3(x1)\n",
        "      x1 = self.c1_convblock4(x1)\n",
        "      x1 = self.c1_fc(x1.view(x1.size(0), -1))\n",
        "      # Forward-Propagation on the second Critic Neural Network\n",
        "      x2 = self.c2_convblock1(xu)\n",
        "      x2 = self.c2_convblock2(x2)\n",
        "      x2 = self.c2_transitionblock(x2)\n",
        "      x2 = self.c2_convblock3(x2)\n",
        "      x2 = self.c2_convblock4(x2)\n",
        "      x2 = self.c2_fc(x2.view(x2.size(0), -1))\n",
        "      return x1, x2\n",
        "\n",
        "    def Q1(self, x, u):\n",
        "      xu = torch.cat([x, u], 2)\n",
        "      x1 = self.c1_convblock1(xu)\n",
        "      x1 = self.c1_convblock2(x1)\n",
        "      x1 = self.c1_transitionblock(x1)\n",
        "      x1 = self.c1_convblock3(x1)\n",
        "      x1 = self.c1_convblock4(x1)\n",
        "      x1 = self.c1_fc(x1.view(x1.size(0), -1))\n",
        "      return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPFqU04lYoUM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "206cfda1-0858-4da1-971d-e21a03bc3e2c"
      },
      "source": [
        "model = Critic((32,32),2).to(device)\n",
        "summary(model, input_size=[(1, 32, 32), (1,2,32)])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 30]             160\n",
            "              ReLU-2           [-1, 16, 32, 30]               0\n",
            "       BatchNorm2d-3           [-1, 16, 32, 30]              32\n",
            "            Conv2d-4           [-1, 16, 30, 28]           2,320\n",
            "              ReLU-5           [-1, 16, 30, 28]               0\n",
            "       BatchNorm2d-6           [-1, 16, 30, 28]              32\n",
            "            Conv2d-7           [-1, 16, 28, 26]           2,320\n",
            "              ReLU-8           [-1, 16, 28, 26]               0\n",
            "       BatchNorm2d-9           [-1, 16, 28, 26]              32\n",
            "        Dropout2d-10           [-1, 16, 28, 26]               0\n",
            "           Conv2d-11           [-1, 10, 28, 26]             170\n",
            "             ReLU-12           [-1, 10, 28, 26]               0\n",
            "        MaxPool2d-13           [-1, 10, 14, 13]               0\n",
            "        Dropout2d-14           [-1, 10, 14, 13]               0\n",
            "      BatchNorm2d-15           [-1, 10, 14, 13]              20\n",
            "           Conv2d-16           [-1, 16, 12, 11]           1,456\n",
            "             ReLU-17           [-1, 16, 12, 11]               0\n",
            "      BatchNorm2d-18           [-1, 16, 12, 11]              32\n",
            "           Conv2d-19            [-1, 16, 10, 9]           2,320\n",
            "             ReLU-20            [-1, 16, 10, 9]               0\n",
            "      BatchNorm2d-21            [-1, 16, 10, 9]              32\n",
            "        Dropout2d-22            [-1, 16, 10, 9]               0\n",
            "           Conv2d-23            [-1, 10, 10, 9]             170\n",
            "             ReLU-24            [-1, 10, 10, 9]               0\n",
            "      BatchNorm2d-25            [-1, 10, 10, 9]              20\n",
            "           Linear-26                    [-1, 1]             901\n",
            "           Conv2d-27           [-1, 16, 32, 30]             160\n",
            "             ReLU-28           [-1, 16, 32, 30]               0\n",
            "      BatchNorm2d-29           [-1, 16, 32, 30]              32\n",
            "           Conv2d-30           [-1, 16, 30, 28]           2,320\n",
            "             ReLU-31           [-1, 16, 30, 28]               0\n",
            "      BatchNorm2d-32           [-1, 16, 30, 28]              32\n",
            "           Conv2d-33           [-1, 16, 28, 26]           2,320\n",
            "             ReLU-34           [-1, 16, 28, 26]               0\n",
            "      BatchNorm2d-35           [-1, 16, 28, 26]              32\n",
            "        Dropout2d-36           [-1, 16, 28, 26]               0\n",
            "           Conv2d-37           [-1, 10, 28, 26]             170\n",
            "             ReLU-38           [-1, 10, 28, 26]               0\n",
            "        MaxPool2d-39           [-1, 10, 14, 13]               0\n",
            "        Dropout2d-40           [-1, 10, 14, 13]               0\n",
            "      BatchNorm2d-41           [-1, 10, 14, 13]              20\n",
            "           Conv2d-42           [-1, 16, 12, 11]           1,456\n",
            "             ReLU-43           [-1, 16, 12, 11]               0\n",
            "      BatchNorm2d-44           [-1, 16, 12, 11]              32\n",
            "           Conv2d-45            [-1, 16, 10, 9]           2,320\n",
            "             ReLU-46            [-1, 16, 10, 9]               0\n",
            "      BatchNorm2d-47            [-1, 16, 10, 9]              32\n",
            "        Dropout2d-48            [-1, 16, 10, 9]               0\n",
            "           Conv2d-49            [-1, 10, 10, 9]             170\n",
            "             ReLU-50            [-1, 10, 10, 9]               0\n",
            "      BatchNorm2d-51            [-1, 10, 10, 9]              20\n",
            "           Linear-52                    [-1, 1]             901\n",
            "================================================================\n",
            "Total params: 20,034\n",
            "Trainable params: 20,034\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.25\n",
            "Forward/backward pass size (MB): 2.56\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 2.89\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NzIDuONodenW"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.max_action = torch.Tensor(max_action).to(device)\n",
        "    self.actor = Actor(state_dim, action_dim, self.max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, self.max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    \n",
        "  def select_action(self, state):\n",
        "    #state = torch.Tensor(state.unsqueeze(0)).to(device)\n",
        "    state = state.unsqueeze(0)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def expand_action(self, action, state_dim):\n",
        "    vel = torch.stack([action[:,0]]*state_dim)\n",
        "    vel = vel.T # (bsize,s_size)\n",
        "    vel = vel.unsqueeze(1).unsqueeze(1) # (bsize, 1, 1, s_size)\n",
        "\n",
        "    ang = torch.stack([action[:,1]]*state_dim)\n",
        "    ang = ang.T # (bsize,s_size)\n",
        "    ang = ang.unsqueeze(1).unsqueeze(1) # (bsize, 1, 1, s_size)\n",
        "\n",
        "    return torch.cat([vel,ang], 2)\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state).squeeze(1)\n",
        "\n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action[:, 0] = (next_action[:, 0] + noise[:, 0]).clamp(0, self.max_action[0])\n",
        "      next_action[:, 1] = (next_action[:, 1] + noise[:, 1]).clamp(-self.max_action[1], self.max_action[1])\n",
        "\n",
        "      # Step 6.1: Because we want to append action of dim=[batch_size,] to state of dim=[batch_size,1,32,32]\n",
        "      #           We expand action to this dim=[batch_size,1,1,32] and then concat it with state\n",
        "      next_action = self.expand_action(next_action, batch_states.shape[-1])\n",
        "      action = self.expand_action(action, batch_states.shape[-1])\n",
        "\n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        q1_action = self.actor(state)\n",
        "        q1_action = q1_action.squeeze(1)\n",
        "        q1_action = self.expand_action(q1_action, state.shape[-1])\n",
        "        actor_loss = -self.critic.Q1(state, q1_action).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gGuKmH_ijf7U"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HFj6wbAo97lk",
        "colab": {}
      },
      "source": [
        "env_name = \"AutoDrivingCarModels\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e4 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hjwf2HCol3XP"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1fyH8N5z-o3o",
        "outputId": "87661d64-9c9d-4329-d683-0e72771e463f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"T3D\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: T3D_AutoDrivingCarModels_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kop-C96Aml8O"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Src07lvY-zXb",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjXU4F5qV69_",
        "colab_type": "text"
      },
      "source": [
        "# Download Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFxh7LT1V7Oe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -O MASK1.png https://drive.google.com/uc\\?export\\=view\\&id\\=1eHsgsEdvdUMZjSR5XUhA2kFgdPK3qSN3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxN_v2aAppds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.markers as mmarkers\n",
        "\n",
        "def center_crop_img(img, x, y, crop_size):\n",
        "  max_x, max_y = img.size\n",
        "  pad_left, pad_right, pad_bottom, pad_top = 0, 0, 0, 0\n",
        "  start_x = x - int(crop_size/2)\n",
        "  start_y = y - int(crop_size/2)\n",
        "  end_x = x + int(crop_size/2)\n",
        "  end_y = y + int(crop_size/2)\n",
        "  if start_x < 0:\n",
        "      pad_left = -start_x\n",
        "      start_x = 0\n",
        "  if end_x >= max_x:\n",
        "      pad_right = end_x - max_x\n",
        "  if start_y < 0:\n",
        "      pad_top = -start_y\n",
        "      start_y = 0\n",
        "  if end_y >= max_y:\n",
        "      pad_bottom = end_y - max_y\n",
        "  padding = (int(pad_left), int(pad_top), int(pad_right), int(pad_bottom))\n",
        "  new_img = ImageOps.expand(img, padding, fill=255)\n",
        "  crop_img = new_img.crop((start_x, start_y, start_x+crop_size, start_y+crop_size))\n",
        "\n",
        "  return crop_img\n",
        "\n",
        "def rotate_img(img, angle):\n",
        "  im1 = img.convert('RGBA')\n",
        "  rot = im1.rotate(angle)\n",
        "  # a white image same size as rotated image\n",
        "  fff = Image.new('RGBA', rot.size, (255,)*4)\n",
        "  # create a composite image using the alpha layer of rot as a mask\n",
        "  return Image.composite(rot, fff, rot).convert('L')\n",
        "\n",
        "def show_img(ax, img, title=\"\"):\n",
        "  np_img = np.asarray(img)/255\n",
        "  np_img = np_img.astype(int)\n",
        "  ax.imshow(np_img, cmap='gray', vmin=0, vmax=1)\n",
        "  if title != \"\":\n",
        "    ax.set_title(title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qEAzOd47mv1Z"
      },
      "source": [
        "## We create the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFCJs2XJKzIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.markers as mmarkers\n",
        "\n",
        "from PIL import Image as PILImage\n",
        "from kivy.vector import Vector\n",
        "\n",
        "class CarEnv(object):\n",
        "    def __init__(self):\n",
        "        img = PILImage.open(\"MASK1.png\").convert('L')\n",
        "        self.sand = np.asarray(img)/255\n",
        "        self.sand = self.sand.astype(int)\n",
        "        self.max_y, self.max_x = self.sand.shape\n",
        "        self.pos = Vector(int(self.max_x/2), int(self.max_y/2))\n",
        "        self.angle = Vector(0,10).angle(self.pos)\n",
        "        self.rel_pos = Vector(0,0)\n",
        "        self.max_angle = 10\n",
        "        self.min_velocity = 0.5\n",
        "        self.max_velocity = 10\n",
        "        self.max_action = [self.max_velocity, self.max_angle]\n",
        "        self.crop_size = 100\n",
        "        self.goal_iter = 0\n",
        "        self.goals = [Vector(1420, 38), Vector(9, 575)]\n",
        "        self.last_distance = 0\n",
        "        self.state_dim = (32, 32)\n",
        "        self.action_dim = (2,)\n",
        "        self._max_episode_steps = 2000\n",
        "      \n",
        "    def seed(self, seed):\n",
        "        pass\n",
        "    \n",
        "    def reset(self):\n",
        "        on_sand = True\n",
        "        while on_sand:\n",
        "          self.pos.x = np.random.randint(low=0, high=self.max_x-5)\n",
        "          self.pos.y = np.random.randint(low=0, high=self.max_y-5)\n",
        "          if self.sand[int(self.pos.y),int(self.pos.x)] <= 0:\n",
        "            on_sand = False\n",
        "        self.angle = Vector(0,10).angle(self.pos)\n",
        "        return self.get_state()\n",
        "    \n",
        "    def random_action(self):\n",
        "        vel = np.random.randint(low=self.min_velocity, high=self.max_velocity)\n",
        "        ang = np.random.randint(low=-self.max_angle, high=self.max_angle)\n",
        "        return (vel, ang)\n",
        "    \n",
        "    def step(self, action):\n",
        "        vel, ang = action\n",
        "        self.angle += ang\n",
        "        reward = 0\n",
        "        done = False\n",
        "        current_goal = self.goals[self.goal_iter]\n",
        "        distance = self.pos.distance(current_goal)\n",
        "        if self.sand[int(self.pos.y),int(self.pos.x)] > 0:\n",
        "            self.pos += Vector(vel, 0).rotate(self.angle)\n",
        "            reward = -1\n",
        "        else: # otherwise\n",
        "            self.pos += Vector(vel, 0).rotate(self.angle)\n",
        "            reward = -0.1\n",
        "            if distance < self.last_distance:\n",
        "                reward = 1\n",
        "        if self.pos.x < 5:\n",
        "            self.pos.x = 5\n",
        "            reward = -0.5\n",
        "            done = True\n",
        "        if self.pos.x > self.max_x - 5:\n",
        "            self.pos.x = self.max_x - 5\n",
        "            reward = -0.5\n",
        "        if self.pos.y < 5:\n",
        "            self.pos.y = 5\n",
        "            reward = -0.5\n",
        "            done = True\n",
        "        if self.pos.y > self.max_y - 5:\n",
        "            self.pos.y = self.max_y - 5\n",
        "            reward = -0.5\n",
        "            done = True\n",
        "        if distance < 25:\n",
        "            self.goal_iter = (self.goal_iter + 1) % len(self.goals)\n",
        "            goal = self.goals[self.goal_iter]\n",
        "            reward = 1\n",
        "            done = True\n",
        "        self.last_distance = distance\n",
        "        return self.get_state(), reward, done\n",
        "    \n",
        "    def render(self):\n",
        "        # Create figure and axes\n",
        "        fig, ax = plt.subplots(1, 5, figsize=(30, 6))\n",
        "\n",
        "        # Display the image\n",
        "        ax[0].imshow(self.sand, cmap='gray', vmin=0, vmax=1)\n",
        "        # Create a Rectangle patch\n",
        "        rect = patches.Rectangle(\n",
        "            (self.pos.x - int(self.crop_size/2), self.pos.y - int(self.crop_size/2)),\n",
        "            self.crop_size, self.crop_size,\n",
        "            linewidth=1, edgecolor='r', facecolor='none'\n",
        "        )\n",
        "        # Add the patch to the Axes\n",
        "        ax[0].add_patch(rect)\n",
        "        ax[0].set_title(\"x=%d,y=%d,angle=%d\" % (self.pos.x, self.pos.y, self.angle))\n",
        "        \n",
        "        marker = mmarkers.MarkerStyle(marker=\"$ \\\\rightarrow$\")\n",
        "        marker._transform = marker.get_transform().rotate_deg(self.angle)\n",
        "        ax[0].scatter(self.pos.x, self.pos.y, s=50, c='red', marker=marker)\n",
        "        self.get_state(ax).cpu().numpy()\n",
        "        plt.show()\n",
        "        \n",
        "    def get_state(self, ax=None):\n",
        "        resize = T.Compose([T.ToPILImage(),\n",
        "                            T.Resize(self.state_dim[0], interpolation=Image.CUBIC),\n",
        "                            T.ToTensor()])\n",
        "        \n",
        "        img = Image.open(\"MASK1.png\").convert('L')\n",
        "\n",
        "        crop_img = center_crop_img(img, self.pos.x, self.pos.y, self.crop_size*3)\n",
        "        if ax is not None:\n",
        "          show_img(ax[1], crop_img, \"large crop\")\n",
        "\n",
        "        r_img = rotate_img(crop_img, -self.angle)\n",
        "        if ax is not None:\n",
        "          show_img(ax[2], r_img, \"rotated crop\")\n",
        "\n",
        "        r_img_x, r_img_y = r_img.size\n",
        "        crop_img = center_crop_img(r_img, int(r_img_x/2), int(r_img_y/2), self.crop_size)\n",
        "        if ax is not None:\n",
        "          show_img(ax[3], crop_img, \"final crop\")\n",
        "\n",
        "        np_img = np.asarray(crop_img)/255\n",
        "        np_img = np_img.astype(int)\n",
        "        screen = np.ascontiguousarray(np_img, dtype=np.float32) \n",
        "        screen = torch.from_numpy(screen)\n",
        "        screen = resize(screen)\n",
        "        if ax is not None:\n",
        "            np_img = screen.squeeze(0).numpy()\n",
        "            np_img = np_img.astype(int)\n",
        "            ax[4].imshow(np_img, cmap='gray', vmin=0, vmax=1)\n",
        "            marker = mmarkers.MarkerStyle(marker=\"$ \\\\rightarrow$\")\n",
        "            ax[4].scatter(self.state_dim[0]/2, self.state_dim[1]/2, s=100, c='red', marker=marker)\n",
        "            ax[4].set_title(\"final resized img\")\n",
        "        return screen.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIZ7-z_ARu7Y",
        "colab_type": "text"
      },
      "source": [
        "# Visualizing Environment\n",
        "\n",
        "Here we crop large portion of image from postion x,y. Then rotate it and then crop it again to required size. Then resize it to required final size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "VsSMlufaRYw3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "87d65fc8-9359-4826-9e16-f8fd2798472f"
      },
      "source": [
        "env = CarEnv()\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x432 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvmcYjdZR-FW",
        "colab_type": "text"
      },
      "source": [
        "# Small animation to understand env.step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbXaXY1rRYw5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "aa6f10f2-66da-4122-a516-e4cab5dbc791"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "plt.figure()\n",
        "env = CarEnv()\n",
        "env.reset()\n",
        "for i in range(20):\n",
        "    img = env.get_state().cpu().squeeze(0).numpy()\n",
        "    plt.imshow(img, cmap='gray', animated=True, vmin=0, vmax=1)\n",
        "    plt.show()\n",
        "    sleep(0.2)\n",
        "    clear_output(wait=True)\n",
        "    vel = np.random.randint(low=0.5, high=5)\n",
        "    angle = np.random.randint(low=-10, high=+10)\n",
        "    env.step((vel, angle))\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5YdPG4HXnNsh"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z3RufYec_ADj",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.state_dim\n",
        "action_dim = env.action_dim[0]\n",
        "max_action = env.max_action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wBtZC0LgjYs9"
      },
      "source": [
        "# Understanding Environment\n",
        "\n",
        "* Action Space:\n",
        "  - Action Space is of 2 dims. With first value (velocity) ranging from 0.5 to 10. And second value (angle) ranging from -10 to +10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HWEgDAQxnbem"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wTVvG7F8_EWg",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZI60VN2Unklh"
      },
      "source": [
        "[link text](https://)## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sd-ZsdXR_LgV",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ka-ZRtQvjBex"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwS4ffh4VKUN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        },
        "outputId": "30f82b31-d457-4f56-d059-120d471dffb8"
      },
      "source": [
        "obs = env.reset()\n",
        "action = policy.select_action(obs)\n",
        "new_obs, reward, done,  = env.step(action)\n",
        "print(obs.shape, new_obs.shape, action, reward, done)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(torch.Size([1, 32, 32]), torch.Size([1, 32, 32]), array([1.8616898, 1.95614  ], dtype=float32), -0.1, False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qabqiYdp9wDM",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    episode_timesteps = 0\n",
        "    while not done:\n",
        "      action = policy.select_action(obs)\n",
        "      obs, reward, done = env.step(action)\n",
        "      episode_timesteps += 1\n",
        "      avg_reward += reward\n",
        "      if episode_timesteps + 1 == env._max_episode_steps:\n",
        "        done = True\n",
        "\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QYOpCyiDnw7s"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dhC_5XJ__Orp",
        "colab": {}
      },
      "source": [
        "evaluations = [] # evaluate_policy(policy, eval_episodes=1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xm-4b3p6rglE"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MTL9uMd0ru03",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "31n5eb03p-Fm"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1vN5EvxK_QhT",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q9gsjvtPqLgT"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPn31gvhKSAp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "413eeb89-7f08-49b8-f11d-a954cf0a38c5"
      },
      "source": [
        "import time\n",
        "\n",
        "max_timesteps = 500000\n",
        "start_time = time.time()\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Episode Len: {} Reward: {} Time Taken: {} secs\".format(\n",
        "              total_timesteps, episode_num, episode_timesteps,\n",
        "              episode_reward, time.time() - start_time))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "      start_time = time.time()\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "      start_time = time.time()\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.random_action()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(obs)\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_dim)).clip(-env.max_angle, env.max_angle)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done = env.step(action)\n",
        "\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  #done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  if episode_timesteps + 1 == env._max_episode_steps: done = True\n",
        "  done_bool = float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs.cpu(), new_obs.cpu(), action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 210 Episode Num: 1 Episode Len: 210 Reward: -153.3 Time Taken: 3.30522799492 secs\n",
            "Total Timesteps: 387 Episode Num: 2 Episode Len: 177 Reward: -119.3 Time Taken: 2.74998497963 secs\n",
            "Total Timesteps: 660 Episode Num: 3 Episode Len: 273 Reward: -180.4 Time Taken: 4.24683499336 secs\n",
            "Total Timesteps: 719 Episode Num: 4 Episode Len: 59 Reward: 27.8 Time Taken: 0.939138889313 secs\n",
            "Total Timesteps: 740 Episode Num: 5 Episode Len: 21 Reward: -7.6 Time Taken: 0.344402074814 secs\n",
            "Total Timesteps: 857 Episode Num: 6 Episode Len: 117 Reward: -93.8 Time Taken: 1.83670687675 secs\n",
            "Total Timesteps: 1377 Episode Num: 7 Episode Len: 520 Reward: -388.7 Time Taken: 8.06953501701 secs\n",
            "Total Timesteps: 1379 Episode Num: 8 Episode Len: 2 Reward: 0.5 Time Taken: 0.0470719337463 secs\n",
            "Total Timesteps: 1430 Episode Num: 9 Episode Len: 51 Reward: -33.6 Time Taken: 0.810391902924 secs\n",
            "Total Timesteps: 1517 Episode Num: 10 Episode Len: 87 Reward: -76.7 Time Taken: 1.37070083618 secs\n",
            "Total Timesteps: 1543 Episode Num: 11 Episode Len: 26 Reward: -23.5 Time Taken: 0.421019077301 secs\n",
            "Total Timesteps: 1795 Episode Num: 12 Episode Len: 252 Reward: -161.1 Time Taken: 3.93288016319 secs\n",
            "Total Timesteps: 1945 Episode Num: 13 Episode Len: 150 Reward: -103.2 Time Taken: 2.39067196846 secs\n",
            "Total Timesteps: 2107 Episode Num: 14 Episode Len: 162 Reward: -82.3 Time Taken: 2.56941485405 secs\n",
            "Total Timesteps: 2488 Episode Num: 15 Episode Len: 381 Reward: -266.4 Time Taken: 5.91760706902 secs\n",
            "Total Timesteps: 2744 Episode Num: 16 Episode Len: 256 Reward: -139.3 Time Taken: 3.9809179306 secs\n",
            "Total Timesteps: 2848 Episode Num: 17 Episode Len: 104 Reward: -87.0 Time Taken: 1.61975193024 secs\n",
            "Total Timesteps: 3306 Episode Num: 18 Episode Len: 458 Reward: -266.9 Time Taken: 7.10705900192 secs\n",
            "Total Timesteps: 3316 Episode Num: 19 Episode Len: 10 Reward: -5.0 Time Taken: 0.172660112381 secs\n",
            "Total Timesteps: 3695 Episode Num: 20 Episode Len: 379 Reward: -261.3 Time Taken: 5.90337705612 secs\n",
            "Total Timesteps: 4252 Episode Num: 21 Episode Len: 557 Reward: -291.6 Time Taken: 8.72871780396 secs\n",
            "Total Timesteps: 4465 Episode Num: 22 Episode Len: 213 Reward: -150.8 Time Taken: 3.30461621284 secs\n",
            "Total Timesteps: 4612 Episode Num: 23 Episode Len: 147 Reward: -108.8 Time Taken: 2.3113079071 secs\n",
            "Total Timesteps: 5233 Episode Num: 24 Episode Len: 621 Reward: -323.9 Time Taken: 9.66292715073 secs\n",
            "Total Timesteps: 5928 Episode Num: 25 Episode Len: 695 Reward: -487.0 Time Taken: 10.8593950272 secs\n",
            "Total Timesteps: 6100 Episode Num: 26 Episode Len: 172 Reward: -64.0 Time Taken: 2.67681789398 secs\n",
            "Total Timesteps: 6214 Episode Num: 27 Episode Len: 114 Reward: 0.7 Time Taken: 1.77361893654 secs\n",
            "Total Timesteps: 6232 Episode Num: 28 Episode Len: 18 Reward: -1.1 Time Taken: 0.299522161484 secs\n",
            "Total Timesteps: 6302 Episode Num: 29 Episode Len: 70 Reward: -32.9 Time Taken: 1.10510802269 secs\n",
            "Total Timesteps: 6303 Episode Num: 30 Episode Len: 1 Reward: -0.5 Time Taken: 0.032054901123 secs\n",
            "Total Timesteps: 6508 Episode Num: 31 Episode Len: 205 Reward: -113.4 Time Taken: 3.21069097519 secs\n",
            "Total Timesteps: 6658 Episode Num: 32 Episode Len: 150 Reward: -133.0 Time Taken: 2.34928703308 secs\n",
            "Total Timesteps: 6684 Episode Num: 33 Episode Len: 26 Reward: -19.0 Time Taken: 0.418757915497 secs\n",
            "Total Timesteps: 6806 Episode Num: 34 Episode Len: 122 Reward: -84.9 Time Taken: 1.91256594658 secs\n",
            "Total Timesteps: 6988 Episode Num: 35 Episode Len: 182 Reward: -153.9 Time Taken: 2.88042902946 secs\n",
            "Total Timesteps: 7045 Episode Num: 36 Episode Len: 57 Reward: -34.7 Time Taken: 0.908524036407 secs\n",
            "Total Timesteps: 7351 Episode Num: 37 Episode Len: 306 Reward: -164.5 Time Taken: 4.76335406303 secs\n",
            "Total Timesteps: 7403 Episode Num: 38 Episode Len: 52 Reward: -45.0 Time Taken: 0.834787845612 secs\n",
            "Total Timesteps: 7613 Episode Num: 39 Episode Len: 210 Reward: -129.5 Time Taken: 3.28935289383 secs\n",
            "Total Timesteps: 7779 Episode Num: 40 Episode Len: 166 Reward: -104.8 Time Taken: 2.59463596344 secs\n",
            "Total Timesteps: 7983 Episode Num: 41 Episode Len: 204 Reward: -135.7 Time Taken: 3.1660091877 secs\n",
            "Total Timesteps: 8045 Episode Num: 42 Episode Len: 62 Reward: -51.4 Time Taken: 0.990561008453 secs\n",
            "Total Timesteps: 8198 Episode Num: 43 Episode Len: 153 Reward: -99.1 Time Taken: 2.38804888725 secs\n",
            "Total Timesteps: 8650 Episode Num: 44 Episode Len: 452 Reward: -346.0 Time Taken: 7.05100607872 secs\n",
            "Total Timesteps: 8761 Episode Num: 45 Episode Len: 111 Reward: -69.5 Time Taken: 1.73690009117 secs\n",
            "Total Timesteps: 8778 Episode Num: 46 Episode Len: 17 Reward: -5.5 Time Taken: 0.282383918762 secs\n",
            "Total Timesteps: 8885 Episode Num: 47 Episode Len: 107 Reward: -52.0 Time Taken: 1.67146897316 secs\n",
            "Total Timesteps: 8899 Episode Num: 48 Episode Len: 14 Reward: -0.7 Time Taken: 0.234320878983 secs\n",
            "Total Timesteps: 8946 Episode Num: 49 Episode Len: 47 Reward: -26.5 Time Taken: 0.741989135742 secs\n",
            "Total Timesteps: 8947 Episode Num: 50 Episode Len: 1 Reward: -0.5 Time Taken: 0.0318660736084 secs\n",
            "Total Timesteps: 9100 Episode Num: 51 Episode Len: 153 Reward: -80.4 Time Taken: 2.38035607338 secs\n",
            "Total Timesteps: 9179 Episode Num: 52 Episode Len: 79 Reward: -68.6 Time Taken: 1.27980589867 secs\n",
            "Total Timesteps: 9439 Episode Num: 53 Episode Len: 260 Reward: -145.4 Time Taken: 4.07297301292 secs\n",
            "Total Timesteps: 10381 Episode Num: 54 Episode Len: 942 Reward: -492.4 Time Taken: 15.3922629356 secs\n",
            "Total Timesteps: 10707 Episode Num: 55 Episode Len: 326 Reward: -139.5 Time Taken: 5.68475818634 secs\n",
            "Total Timesteps: 12707 Episode Num: 56 Episode Len: 2000 Reward: -1764.3 Time Taken: 35.0193519592 secs\n",
            "Total Timesteps: 12799 Episode Num: 57 Episode Len: 92 Reward: -28.8 Time Taken: 1.62563896179 secs\n",
            "Total Timesteps: 14799 Episode Num: 58 Episode Len: 2000 Reward: -1893.0 Time Taken: 34.7906699181 secs\n",
            "Total Timesteps: 16799 Episode Num: 59 Episode Len: 2000 Reward: -1581.2 Time Taken: 34.768643856 secs\n",
            "Total Timesteps: 16849 Episode Num: 60 Episode Len: 50 Reward: -2.7 Time Taken: 0.894969940186 secs\n",
            "Total Timesteps: 18849 Episode Num: 61 Episode Len: 2000 Reward: -1835.7 Time Taken: 35.0181000233 secs\n",
            "Total Timesteps: 20849 Episode Num: 62 Episode Len: 2000 Reward: -1329.6 Time Taken: 34.8393959999 secs\n",
            "Total Timesteps: 22849 Episode Num: 63 Episode Len: 2000 Reward: -1831.2 Time Taken: 34.9446258545 secs\n",
            "Total Timesteps: 24849 Episode Num: 64 Episode Len: 2000 Reward: -343.9 Time Taken: 34.8063728809 secs\n",
            "Total Timesteps: 24850 Episode Num: 65 Episode Len: 1 Reward: 1 Time Taken: 0.034511089325 secs\n",
            "Total Timesteps: 26850 Episode Num: 66 Episode Len: 2000 Reward: -602.9 Time Taken: 34.8760159016 secs\n",
            "Total Timesteps: 27256 Episode Num: 67 Episode Len: 406 Reward: -234.8 Time Taken: 7.11343288422 secs\n",
            "Total Timesteps: 29256 Episode Num: 68 Episode Len: 2000 Reward: -734.8 Time Taken: 34.9231119156 secs\n",
            "Total Timesteps: 31256 Episode Num: 69 Episode Len: 2000 Reward: -1004.7 Time Taken: 34.9711060524 secs\n",
            "Total Timesteps: 33256 Episode Num: 70 Episode Len: 2000 Reward: 394.0 Time Taken: 34.9560220242 secs\n",
            "Total Timesteps: 33789 Episode Num: 71 Episode Len: 533 Reward: -211.3 Time Taken: 9.25232696533 secs\n",
            "Total Timesteps: 35789 Episode Num: 72 Episode Len: 2000 Reward: 208.4 Time Taken: 34.775468111 secs\n",
            "Total Timesteps: 37789 Episode Num: 73 Episode Len: 2000 Reward: -1885.5 Time Taken: 35.725041151 secs\n",
            "Total Timesteps: 39789 Episode Num: 74 Episode Len: 2000 Reward: 393.4 Time Taken: 35.1816401482 secs\n",
            "Total Timesteps: 41789 Episode Num: 75 Episode Len: 2000 Reward: -1391.3 Time Taken: 35.4287610054 secs\n",
            "Total Timesteps: 43789 Episode Num: 76 Episode Len: 2000 Reward: -1098.8 Time Taken: 35.3982651234 secs\n",
            "Total Timesteps: 45789 Episode Num: 77 Episode Len: 2000 Reward: 347.3 Time Taken: 35.4174749851 secs\n",
            "Total Timesteps: 47789 Episode Num: 78 Episode Len: 2000 Reward: 550.2 Time Taken: 35.1701221466 secs\n",
            "Total Timesteps: 49789 Episode Num: 79 Episode Len: 2000 Reward: 658.0 Time Taken: 35.0597171783 secs\n",
            "Total Timesteps: 51789 Episode Num: 80 Episode Len: 2000 Reward: 808.7 Time Taken: 35.1210489273 secs\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 612.220000\n",
            "---------------------------------------\n",
            "Total Timesteps: 53789 Episode Num: 81 Episode Len: 2000 Reward: -1973.6 Time Taken: 35.5224411488 secs\n",
            "Total Timesteps: 55789 Episode Num: 82 Episode Len: 2000 Reward: 806.9 Time Taken: 35.4615468979 secs\n",
            "Total Timesteps: 57789 Episode Num: 83 Episode Len: 2000 Reward: 38.2 Time Taken: 35.3574960232 secs\n",
            "Total Timesteps: 59789 Episode Num: 84 Episode Len: 2000 Reward: 938.8 Time Taken: 35.2638869286 secs\n",
            "Total Timesteps: 61789 Episode Num: 85 Episode Len: 2000 Reward: -457.2 Time Taken: 35.2076570988 secs\n",
            "Total Timesteps: 63789 Episode Num: 86 Episode Len: 2000 Reward: -1919.4 Time Taken: 35.0241310596 secs\n",
            "Total Timesteps: 65789 Episode Num: 87 Episode Len: 2000 Reward: -1743.5 Time Taken: 35.2995259762 secs\n",
            "Total Timesteps: 67789 Episode Num: 88 Episode Len: 2000 Reward: 550.9 Time Taken: 35.3267068863 secs\n",
            "Total Timesteps: 67794 Episode Num: 89 Episode Len: 5 Reward: 0.2 Time Taken: 0.108824014664 secs\n",
            "Total Timesteps: 69794 Episode Num: 90 Episode Len: 2000 Reward: -1999.1 Time Taken: 35.2075300217 secs\n",
            "Total Timesteps: 71794 Episode Num: 91 Episode Len: 2000 Reward: -369.3 Time Taken: 35.2417089939 secs\n",
            "Total Timesteps: 73794 Episode Num: 92 Episode Len: 2000 Reward: 953.9 Time Taken: 35.0754179955 secs\n",
            "Total Timesteps: 75794 Episode Num: 93 Episode Len: 2000 Reward: -1901.7 Time Taken: 35.0114109516 secs\n",
            "Total Timesteps: 77794 Episode Num: 94 Episode Len: 2000 Reward: -1723.2 Time Taken: 34.9818890095 secs\n",
            "Total Timesteps: 79794 Episode Num: 95 Episode Len: 2000 Reward: -413.9 Time Taken: 35.0345659256 secs\n",
            "Total Timesteps: 81794 Episode Num: 96 Episode Len: 2000 Reward: 945.1 Time Taken: 35.1678471565 secs\n",
            "Total Timesteps: 83794 Episode Num: 97 Episode Len: 2000 Reward: 872.6 Time Taken: 35.2047579288 secs\n",
            "Total Timesteps: 85794 Episode Num: 98 Episode Len: 2000 Reward: 973.7 Time Taken: 35.3520579338 secs\n",
            "Total Timesteps: 87794 Episode Num: 99 Episode Len: 2000 Reward: -1256.0 Time Taken: 35.1274299622 secs\n",
            "Total Timesteps: 89794 Episode Num: 100 Episode Len: 2000 Reward: 930.8 Time Taken: 35.2958798409 secs\n",
            "Total Timesteps: 91794 Episode Num: 101 Episode Len: 2000 Reward: -160.8 Time Taken: 35.7917220592 secs\n",
            "Total Timesteps: 93794 Episode Num: 102 Episode Len: 2000 Reward: -1996.2 Time Taken: 35.562125206 secs\n",
            "Total Timesteps: 95794 Episode Num: 103 Episode Len: 2000 Reward: -1958.8 Time Taken: 35.4953808784 secs\n",
            "Total Timesteps: 97794 Episode Num: 104 Episode Len: 2000 Reward: -1766.8 Time Taken: 35.568459034 secs\n",
            "Total Timesteps: 99794 Episode Num: 105 Episode Len: 2000 Reward: -1820.6 Time Taken: 35.143089056 secs\n",
            "Total Timesteps: 101794 Episode Num: 106 Episode Len: 2000 Reward: 740.1 Time Taken: 35.1965742111 secs\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -652.970000\n",
            "---------------------------------------\n",
            "Total Timesteps: 103794 Episode Num: 107 Episode Len: 2000 Reward: -298.2 Time Taken: 35.6166319847 secs\n",
            "Total Timesteps: 105794 Episode Num: 108 Episode Len: 2000 Reward: 519.3 Time Taken: 35.4916489124 secs\n",
            "Total Timesteps: 105795 Episode Num: 109 Episode Len: 1 Reward: 1 Time Taken: 0.0340251922607 secs\n",
            "Total Timesteps: 107795 Episode Num: 110 Episode Len: 2000 Reward: 769.3 Time Taken: 35.6359448433 secs\n",
            "Total Timesteps: 109795 Episode Num: 111 Episode Len: 2000 Reward: -1300.8 Time Taken: 35.5531051159 secs\n",
            "Total Timesteps: 111795 Episode Num: 112 Episode Len: 2000 Reward: -886.9 Time Taken: 35.7091419697 secs\n",
            "Total Timesteps: 113795 Episode Num: 113 Episode Len: 2000 Reward: -1950.2 Time Taken: 36.09988904 secs\n",
            "Total Timesteps: 115795 Episode Num: 114 Episode Len: 2000 Reward: 923.3 Time Taken: 38.6464960575 secs\n",
            "Total Timesteps: 117795 Episode Num: 115 Episode Len: 2000 Reward: -216.2 Time Taken: 37.7445240021 secs\n",
            "Total Timesteps: 119795 Episode Num: 116 Episode Len: 2000 Reward: -1050.8 Time Taken: 37.9416821003 secs\n",
            "Total Timesteps: 121795 Episode Num: 117 Episode Len: 2000 Reward: 259.1 Time Taken: 37.5056598186 secs\n",
            "Total Timesteps: 123795 Episode Num: 118 Episode Len: 2000 Reward: 217.9 Time Taken: 37.4348428249 secs\n",
            "Total Timesteps: 125795 Episode Num: 119 Episode Len: 2000 Reward: 694.5 Time Taken: 37.3076541424 secs\n",
            "Total Timesteps: 127795 Episode Num: 120 Episode Len: 2000 Reward: 918.7 Time Taken: 36.3414747715 secs\n",
            "Total Timesteps: 129795 Episode Num: 121 Episode Len: 2000 Reward: 846.1 Time Taken: 36.3280279636 secs\n",
            "Total Timesteps: 131795 Episode Num: 122 Episode Len: 2000 Reward: 735.1 Time Taken: 35.4809470177 secs\n",
            "Total Timesteps: 133795 Episode Num: 123 Episode Len: 2000 Reward: -1910.9 Time Taken: 37.539525032 secs\n",
            "Total Timesteps: 133852 Episode Num: 124 Episode Len: 57 Reward: -48.2 Time Taken: 1.07697200775 secs\n",
            "Total Timesteps: 135852 Episode Num: 125 Episode Len: 2000 Reward: 985.8 Time Taken: 36.8260469437 secs\n",
            "Total Timesteps: 137852 Episode Num: 126 Episode Len: 2000 Reward: 868.1 Time Taken: 36.5504720211 secs\n",
            "Total Timesteps: 139852 Episode Num: 127 Episode Len: 2000 Reward: -921.4 Time Taken: 36.525113821 secs\n",
            "Total Timesteps: 141852 Episode Num: 128 Episode Len: 2000 Reward: -1971.9 Time Taken: 36.5587308407 secs\n",
            "Total Timesteps: 141855 Episode Num: 129 Episode Len: 3 Reward: 0.4 Time Taken: 0.0709178447723 secs\n",
            "Total Timesteps: 143855 Episode Num: 130 Episode Len: 2000 Reward: 481.9 Time Taken: 36.7898390293 secs\n",
            "Total Timesteps: 145855 Episode Num: 131 Episode Len: 2000 Reward: -1847.1 Time Taken: 36.7560110092 secs\n",
            "Total Timesteps: 147855 Episode Num: 132 Episode Len: 2000 Reward: 177.6 Time Taken: 36.9907929897 secs\n",
            "Total Timesteps: 148509 Episode Num: 133 Episode Len: 654 Reward: 288.4 Time Taken: 12.0392351151 secs\n",
            "Total Timesteps: 150509 Episode Num: 134 Episode Len: 2000 Reward: -1833.5 Time Taken: 36.9359281063 secs\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -440.710000\n",
            "---------------------------------------\n",
            "Total Timesteps: 152509 Episode Num: 135 Episode Len: 2000 Reward: -887.6 Time Taken: 37.343380928 secs\n",
            "Total Timesteps: 154509 Episode Num: 136 Episode Len: 2000 Reward: 644.1 Time Taken: 37.4404788017 secs\n",
            "Total Timesteps: 156509 Episode Num: 137 Episode Len: 2000 Reward: 1027.6 Time Taken: 37.3972899914 secs\n",
            "Total Timesteps: 158509 Episode Num: 138 Episode Len: 2000 Reward: -461.0 Time Taken: 37.1138470173 secs\n",
            "Total Timesteps: 160509 Episode Num: 139 Episode Len: 2000 Reward: -954.7 Time Taken: 36.7741680145 secs\n",
            "Total Timesteps: 162509 Episode Num: 140 Episode Len: 2000 Reward: -1598.1 Time Taken: 36.9330940247 secs\n",
            "Total Timesteps: 164509 Episode Num: 141 Episode Len: 2000 Reward: 865.8 Time Taken: 36.8630678654 secs\n",
            "Total Timesteps: 166509 Episode Num: 142 Episode Len: 2000 Reward: -1128.9 Time Taken: 36.9909369946 secs\n",
            "Total Timesteps: 168509 Episode Num: 143 Episode Len: 2000 Reward: 123.3 Time Taken: 36.7387721539 secs\n",
            "Total Timesteps: 168720 Episode Num: 144 Episode Len: 211 Reward: 11.3 Time Taken: 3.87605190277 secs\n",
            "Total Timesteps: 170720 Episode Num: 145 Episode Len: 2000 Reward: 745.4 Time Taken: 37.108052969 secs\n",
            "Total Timesteps: 172720 Episode Num: 146 Episode Len: 2000 Reward: 687.3 Time Taken: 36.8557460308 secs\n",
            "Total Timesteps: 174720 Episode Num: 147 Episode Len: 2000 Reward: 653.3 Time Taken: 36.7462441921 secs\n",
            "Total Timesteps: 175060 Episode Num: 148 Episode Len: 340 Reward: -60.0 Time Taken: 6.30604481697 secs\n",
            "Total Timesteps: 177060 Episode Num: 149 Episode Len: 2000 Reward: 358.1 Time Taken: 36.6127049923 secs\n",
            "Total Timesteps: 179060 Episode Num: 150 Episode Len: 2000 Reward: -106.6 Time Taken: 36.8859288692 secs\n",
            "Total Timesteps: 179105 Episode Num: 151 Episode Len: 45 Reward: 4.4 Time Taken: 0.841869831085 secs\n",
            "Total Timesteps: 179106 Episode Num: 152 Episode Len: 1 Reward: -0.5 Time Taken: 0.0366349220276 secs\n",
            "Total Timesteps: 181106 Episode Num: 153 Episode Len: 2000 Reward: 749.1 Time Taken: 37.0876019001 secs\n",
            "Total Timesteps: 183106 Episode Num: 154 Episode Len: 2000 Reward: -1279.7 Time Taken: 37.049549818 secs\n",
            "Total Timesteps: 185106 Episode Num: 155 Episode Len: 2000 Reward: -14.8 Time Taken: 37.240101099 secs\n",
            "Total Timesteps: 187106 Episode Num: 156 Episode Len: 2000 Reward: 158.6 Time Taken: 38.1755928993 secs\n",
            "Total Timesteps: 189106 Episode Num: 157 Episode Len: 2000 Reward: -1015.6 Time Taken: 38.3531610966 secs\n",
            "Total Timesteps: 191106 Episode Num: 158 Episode Len: 2000 Reward: 682.5 Time Taken: 38.0835080147 secs\n",
            "Total Timesteps: 193106 Episode Num: 159 Episode Len: 2000 Reward: 51.6 Time Taken: 36.7691411972 secs\n",
            "Total Timesteps: 194545 Episode Num: 160 Episode Len: 1439 Reward: 18.4 Time Taken: 26.2380409241 secs\n",
            "Total Timesteps: 196545 Episode Num: 161 Episode Len: 2000 Reward: 605.1 Time Taken: 36.8556940556 secs\n",
            "Total Timesteps: 198545 Episode Num: 162 Episode Len: 2000 Reward: 121.0 Time Taken: 37.320941925 secs\n",
            "Total Timesteps: 200545 Episode Num: 163 Episode Len: 2000 Reward: -1026.8 Time Taken: 36.719094038 secs\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 83.510000\n",
            "---------------------------------------\n",
            "Total Timesteps: 202545 Episode Num: 164 Episode Len: 2000 Reward: 688.0 Time Taken: 37.0927789211 secs\n",
            "Total Timesteps: 204545 Episode Num: 165 Episode Len: 2000 Reward: -1706.6 Time Taken: 36.5081040859 secs\n",
            "Total Timesteps: 206545 Episode Num: 166 Episode Len: 2000 Reward: -539.2 Time Taken: 36.6338469982 secs\n",
            "Total Timesteps: 208545 Episode Num: 167 Episode Len: 2000 Reward: 534.1 Time Taken: 37.4611971378 secs\n",
            "Total Timesteps: 210545 Episode Num: 168 Episode Len: 2000 Reward: 315.8 Time Taken: 36.9915010929 secs\n",
            "Total Timesteps: 212545 Episode Num: 169 Episode Len: 2000 Reward: 462.1 Time Taken: 36.7675509453 secs\n",
            "Total Timesteps: 214545 Episode Num: 170 Episode Len: 2000 Reward: 10.0 Time Taken: 37.0895938873 secs\n",
            "Total Timesteps: 216545 Episode Num: 171 Episode Len: 2000 Reward: 819.1 Time Taken: 36.9047970772 secs\n",
            "Total Timesteps: 218545 Episode Num: 172 Episode Len: 2000 Reward: 723.8 Time Taken: 36.8765609264 secs\n",
            "Total Timesteps: 220545 Episode Num: 173 Episode Len: 2000 Reward: -1988 Time Taken: 37.1873059273 secs\n",
            "Total Timesteps: 222545 Episode Num: 174 Episode Len: 2000 Reward: 800.8 Time Taken: 37.0647311211 secs\n",
            "Total Timesteps: 224545 Episode Num: 175 Episode Len: 2000 Reward: 316.7 Time Taken: 37.0955388546 secs\n",
            "Total Timesteps: 226545 Episode Num: 176 Episode Len: 2000 Reward: -1789.3 Time Taken: 36.7796909809 secs\n",
            "Total Timesteps: 228545 Episode Num: 177 Episode Len: 2000 Reward: 847.2 Time Taken: 36.8447990417 secs\n",
            "Total Timesteps: 230545 Episode Num: 178 Episode Len: 2000 Reward: -1067.8 Time Taken: 37.1654961109 secs\n",
            "Total Timesteps: 232545 Episode Num: 179 Episode Len: 2000 Reward: 527.3 Time Taken: 37.0833880901 secs\n",
            "Total Timesteps: 232548 Episode Num: 180 Episode Len: 3 Reward: 0.4 Time Taken: 0.0707759857178 secs\n",
            "Total Timesteps: 234548 Episode Num: 181 Episode Len: 2000 Reward: -1923.1 Time Taken: 36.6251699924 secs\n",
            "Total Timesteps: 236548 Episode Num: 182 Episode Len: 2000 Reward: -774.3 Time Taken: 38.3431460857 secs\n",
            "Total Timesteps: 238548 Episode Num: 183 Episode Len: 2000 Reward: 18.9 Time Taken: 36.8621811867 secs\n",
            "Total Timesteps: 239299 Episode Num: 184 Episode Len: 751 Reward: -306.0 Time Taken: 13.9973731041 secs\n",
            "Total Timesteps: 241299 Episode Num: 185 Episode Len: 2000 Reward: -1246.6 Time Taken: 37.055907011 secs\n",
            "Total Timesteps: 243299 Episode Num: 186 Episode Len: 2000 Reward: 306.3 Time Taken: 36.8704738617 secs\n",
            "Total Timesteps: 243605 Episode Num: 187 Episode Len: 306 Reward: 73.1 Time Taken: 5.61578917503 secs\n",
            "Total Timesteps: 244671 Episode Num: 188 Episode Len: 1066 Reward: 309.0 Time Taken: 19.7333498001 secs\n",
            "Total Timesteps: 246671 Episode Num: 189 Episode Len: 2000 Reward: -1555.4 Time Taken: 36.9087259769 secs\n",
            "Total Timesteps: 248671 Episode Num: 190 Episode Len: 2000 Reward: 765.4 Time Taken: 36.6169760227 secs\n",
            "Total Timesteps: 250671 Episode Num: 191 Episode Len: 2000 Reward: 219.4 Time Taken: 36.2813630104 secs\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 364.930000\n",
            "---------------------------------------\n",
            "Total Timesteps: 252671 Episode Num: 192 Episode Len: 2000 Reward: -311.7 Time Taken: 37.2996020317 secs\n",
            "Total Timesteps: 254671 Episode Num: 193 Episode Len: 2000 Reward: 710.9 Time Taken: 36.7090902328 secs\n",
            "Total Timesteps: 256671 Episode Num: 194 Episode Len: 2000 Reward: 608.9 Time Taken: 36.6093668938 secs\n",
            "Total Timesteps: 258671 Episode Num: 195 Episode Len: 2000 Reward: 581.9 Time Taken: 36.7167708874 secs\n",
            "Total Timesteps: 260671 Episode Num: 196 Episode Len: 2000 Reward: 263.5 Time Taken: 36.3533761501 secs\n",
            "Total Timesteps: 262671 Episode Num: 197 Episode Len: 2000 Reward: 613.7 Time Taken: 36.235574007 secs\n",
            "Total Timesteps: 262750 Episode Num: 198 Episode Len: 79 Reward: 14.6 Time Taken: 1.41826701164 secs\n",
            "Total Timesteps: 264750 Episode Num: 199 Episode Len: 2000 Reward: 890.4 Time Taken: 36.1647589207 secs\n",
            "Total Timesteps: 264930 Episode Num: 200 Episode Len: 180 Reward: -19.7 Time Taken: 3.22574806213 secs\n",
            "Total Timesteps: 266930 Episode Num: 201 Episode Len: 2000 Reward: 907.8 Time Taken: 36.0498638153 secs\n",
            "Total Timesteps: 268930 Episode Num: 202 Episode Len: 2000 Reward: 288.3 Time Taken: 35.9484181404 secs\n",
            "Total Timesteps: 270930 Episode Num: 203 Episode Len: 2000 Reward: 832.8 Time Taken: 35.6723639965 secs\n",
            "Total Timesteps: 272930 Episode Num: 204 Episode Len: 2000 Reward: 243.1 Time Taken: 35.8417301178 secs\n",
            "Total Timesteps: 274930 Episode Num: 205 Episode Len: 2000 Reward: 105.7 Time Taken: 35.3847229481 secs\n",
            "Total Timesteps: 275036 Episode Num: 206 Episode Len: 106 Reward: -30.7 Time Taken: 1.9192609787 secs\n",
            "Total Timesteps: 277036 Episode Num: 207 Episode Len: 2000 Reward: 353.2 Time Taken: 36.0701560974 secs\n",
            "Total Timesteps: 279036 Episode Num: 208 Episode Len: 2000 Reward: 188.8 Time Taken: 35.3201830387 secs\n",
            "Total Timesteps: 281036 Episode Num: 209 Episode Len: 2000 Reward: 71.6 Time Taken: 35.3297350407 secs\n",
            "Total Timesteps: 283036 Episode Num: 210 Episode Len: 2000 Reward: 236.7 Time Taken: 35.5965690613 secs\n",
            "Total Timesteps: 283071 Episode Num: 211 Episode Len: 35 Reward: -32.5 Time Taken: 0.651660919189 secs\n",
            "Total Timesteps: 285071 Episode Num: 212 Episode Len: 2000 Reward: 59.6 Time Taken: 36.2679359913 secs\n",
            "Total Timesteps: 285089 Episode Num: 213 Episode Len: 18 Reward: -1.5 Time Taken: 0.346873998642 secs\n",
            "Total Timesteps: 285535 Episode Num: 214 Episode Len: 446 Reward: 62.7 Time Taken: 8.10383987427 secs\n",
            "Total Timesteps: 285819 Episode Num: 215 Episode Len: 284 Reward: -5.6 Time Taken: 5.08186078072 secs\n",
            "Total Timesteps: 287819 Episode Num: 216 Episode Len: 2000 Reward: 267.5 Time Taken: 36.2250480652 secs\n",
            "Total Timesteps: 289819 Episode Num: 217 Episode Len: 2000 Reward: -652.5 Time Taken: 36.3863749504 secs\n",
            "Total Timesteps: 291819 Episode Num: 218 Episode Len: 2000 Reward: -885.4 Time Taken: 36.449534893 secs\n",
            "Total Timesteps: 293819 Episode Num: 219 Episode Len: 2000 Reward: -908.7 Time Taken: 36.8136589527 secs\n",
            "Total Timesteps: 295819 Episode Num: 220 Episode Len: 2000 Reward: 15.8 Time Taken: 36.1712779999 secs\n",
            "Total Timesteps: 296200 Episode Num: 221 Episode Len: 381 Reward: -71.4 Time Taken: 6.93519496918 secs\n",
            "Total Timesteps: 298200 Episode Num: 222 Episode Len: 2000 Reward: 45.0 Time Taken: 37.0768921375 secs\n",
            "Total Timesteps: 300200 Episode Num: 223 Episode Len: 2000 Reward: 164.3 Time Taken: 36.7738080025 secs\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -40.980000\n",
            "---------------------------------------\n",
            "Total Timesteps: 302200 Episode Num: 224 Episode Len: 2000 Reward: 182.1 Time Taken: 36.6608090401 secs\n",
            "Total Timesteps: 304200 Episode Num: 225 Episode Len: 2000 Reward: -333.2 Time Taken: 36.8991110325 secs\n",
            "Total Timesteps: 306200 Episode Num: 226 Episode Len: 2000 Reward: 198.5 Time Taken: 36.9627029896 secs\n",
            "Total Timesteps: 308200 Episode Num: 227 Episode Len: 2000 Reward: -1614.8 Time Taken: 36.9021658897 secs\n",
            "Total Timesteps: 310200 Episode Num: 228 Episode Len: 2000 Reward: 450.8 Time Taken: 36.7650361061 secs\n",
            "Total Timesteps: 310217 Episode Num: 229 Episode Len: 17 Reward: -14.5 Time Taken: 0.338749170303 secs\n",
            "Total Timesteps: 312217 Episode Num: 230 Episode Len: 2000 Reward: -54.7 Time Taken: 36.9278388023 secs\n",
            "Total Timesteps: 314217 Episode Num: 231 Episode Len: 2000 Reward: -13.0 Time Taken: 38.8587651253 secs\n",
            "Total Timesteps: 316217 Episode Num: 232 Episode Len: 2000 Reward: -1207.7 Time Taken: 38.358497858 secs\n",
            "Total Timesteps: 318217 Episode Num: 233 Episode Len: 2000 Reward: 206.5 Time Taken: 37.1223070621 secs\n",
            "Total Timesteps: 320217 Episode Num: 234 Episode Len: 2000 Reward: 223.3 Time Taken: 37.9716119766 secs\n",
            "Total Timesteps: 320219 Episode Num: 235 Episode Len: 2 Reward: 0.5 Time Taken: 0.0537588596344 secs\n",
            "Total Timesteps: 322219 Episode Num: 236 Episode Len: 2000 Reward: 816.4 Time Taken: 37.3834021091 secs\n",
            "Total Timesteps: 324219 Episode Num: 237 Episode Len: 2000 Reward: 894.1 Time Taken: 36.9123790264 secs\n",
            "Total Timesteps: 326219 Episode Num: 238 Episode Len: 2000 Reward: 148.9 Time Taken: 37.0709409714 secs\n",
            "Total Timesteps: 326220 Episode Num: 239 Episode Len: 1 Reward: -0.5 Time Taken: 0.0348789691925 secs\n",
            "Total Timesteps: 328220 Episode Num: 240 Episode Len: 2000 Reward: 234.6 Time Taken: 36.3015449047 secs\n",
            "Total Timesteps: 330220 Episode Num: 241 Episode Len: 2000 Reward: 54.7 Time Taken: 36.2316100597 secs\n",
            "Total Timesteps: 332220 Episode Num: 242 Episode Len: 2000 Reward: 563.8 Time Taken: 36.1462619305 secs\n",
            "Total Timesteps: 334220 Episode Num: 243 Episode Len: 2000 Reward: 509.9 Time Taken: 36.3550992012 secs\n",
            "Total Timesteps: 336220 Episode Num: 244 Episode Len: 2000 Reward: 113.6 Time Taken: 36.1793620586 secs\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}