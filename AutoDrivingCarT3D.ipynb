{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AutoDrivingCarT3D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.16"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WXu1r8qvSzWf"
      },
      "source": [
        "# Twin-Delayed DDPG\n",
        "\n",
        "Complete credit goes to this [awesome Deep Reinforcement Learning 2.0 Course on Udemy](https://www.udemy.com/course/deep-reinforcement-learning/) for the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIKSm5DiTOIC",
        "colab_type": "text"
      },
      "source": [
        "# Understanding Implementation\n",
        "* Because we can't run Kivy on Colab and because we don't have GPUs on our desktop, following is the strategy to train\n",
        "  * Load the sand image and move the coordinates (Vector) in env.Step() for simulating car movement.\n",
        "  * State is captured by cropping a portion of sand image from car's position. And then rotating it in the direction of the car in such a way that car's orientation is horizontal i.e 0 degrees from x-axis. This state is passed to Actor network\n",
        "  * Action is 1 dimensional, with its value being amount of angle the car should rotate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xjm2onHdT-Av"
      },
      "source": [
        "## Importing the libraries\n",
        "* Although Kivy is not used in Colab, we are installing it to use its `Vector` module for simulating Kivy environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1oeheFtR9ZW",
        "colab_type": "code",
        "outputId": "432598f8-fd0f-4109-d20e-4955e60b9a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "source": [
        "!pip install kivy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kivy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/4d/3f8a720f561dc1eabe036c0d87c6ce9d02823275391265538e606f45e37a/Kivy-1.11.1.tar.gz (23.6MB)\n",
            "\u001b[K     |████████████████████████████████| 23.6MB 117kB/s \n",
            "\u001b[?25hCollecting Kivy-Garden>=0.1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/68/decaee596ff8168a39432eb3949fc7c0be952ebb9467806823bffc165d48/kivy-garden-0.1.4.tar.gz\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python2.7/dist-packages (from kivy) (0.14)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python2.7/dist-packages (from kivy) (2.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python2.7/dist-packages (from Kivy-Garden>=0.1.4->kivy) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests->Kivy-Garden>=0.1.4->kivy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests->Kivy-Garden>=0.1.4->kivy) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests->Kivy-Garden>=0.1.4->kivy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests->Kivy-Garden>=0.1.4->kivy) (2.8)\n",
            "Building wheels for collected packages: kivy, Kivy-Garden\n",
            "  Building wheel for kivy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kivy: filename=Kivy-1.11.1-cp27-cp27mu-linux_x86_64.whl size=14862615 sha256=d0040ff253cbaf6a0bb95771d4adeb897b0d19fd1a09f3afdad8a7497467718c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/50/0e/8175f4c54872da1cd9977eec1d91211a8237b093b17f425dc7\n",
            "  Building wheel for Kivy-Garden (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Kivy-Garden: filename=Kivy_Garden-0.1.4-cp27-none-any.whl size=4531 sha256=ba83efd51768ce3ff39cefd878c43106e25e4bf3298b61cfc4abf7987cafa6ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/09/36/4bec048252175b6aa7ba75441cbeed8f31a0bea37abedcfed0\n",
            "Successfully built kivy Kivy-Garden\n",
            "Installing collected packages: Kivy-Garden, kivy\n",
            "Successfully installed Kivy-Garden-0.1.4 kivy-1.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ikr2p0Js8iB4",
        "colab": {}
      },
      "source": [
        "%matplotlib notebook\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n",
        "from PIL import Image as PILImage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y2nGdtlKVydr"
      },
      "source": [
        "# Define Replay Memory\n",
        "* This is a fixed size array storing multiple experiences.\n",
        "* An experience (aka transition) is defined by the following:\n",
        "  * s: current state in which the agent is\n",
        "  * a: action the agent takes to go to next state\n",
        "  * s': new state agent reaches after taking an action (a)\n",
        "  * r: reward an agent receive for going from state (s) to state (s') by taking action (a)\n",
        "* Initially, agent plays with the environment randomly and fills in replay memory.\n",
        "* Then during training, a batch of experiences is sampled randomly to train the agent.\n",
        "* Also this memory is simultaneously filled as and when agent explores the environment.\n",
        "* If memory is full, then first entry is removed and new entry is added.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u5rW0IDB8nTO",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state.cpu(), copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyOeU_rR0ccK",
        "colab_type": "text"
      },
      "source": [
        "* Fetch device information. This is very useful to speed up network training/inference by using GPU when available. If GPU is not available then, CPU is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppZ7G3DACBPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jb7TTaHxWbQD"
      },
      "source": [
        "## Network Architecture\n",
        "* We build one neural network for the Actor model and one neural network for the Actor target\n",
        "* We use MobileNet for this implementation as we don't need very complex network and also the network should have good speed\n",
        "* Below also hosts common functions used to build Actor and Critic networks\n",
        "* conv_bn: does simple conv2d with kernel size 3x3 and padding 1\n",
        "* conv_dw: does depthwise convolution with padding 1\n",
        "* state_model: common state network used for both Actor and Critic networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IWcFnjUhk_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_bn(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "def conv_dw(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
        "        nn.BatchNorm2d(inp),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "def state_model():\n",
        "    return nn.Sequential(\n",
        "            # CHW: 1, 32, 32\n",
        "            conv_bn(  1,  32, 1), # 32, 32, 32\n",
        "            conv_dw( 32,  32, 1), # 32, 32, 32\n",
        "            conv_dw( 32,  32, 2), # 32, 16, 16\n",
        "            conv_dw( 32,  16, 1), # 16, 16, 16\n",
        "            conv_dw( 16,  16, 2), # 16,  8,  8\n",
        "            nn.AvgPool2d(8),\n",
        "        )\n",
        "\n",
        "\n",
        "# Creating the architecture of the Neural Network\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.model = state_model()\n",
        "        self.fc1 = nn.Linear(16, 100)\n",
        "        self.fc2 = nn.Linear(100, action_dim)\n",
        "        self.max_action = max_action\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = x.view(-1, 16)\n",
        "        x = self.fc2(self.fc1(x))\n",
        "        x = self.max_action * torch.tanh(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4VLNPzfW9wP",
        "colab_type": "code",
        "outputId": "d70a5234-f9cf-427a-f3bf-3b0597efc00a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        }
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = Actor((1, 32,32),1,10).to(device)\n",
        "summary(model, input_size=(1, 32, 32))"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             288\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]             288\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "            Conv2d-7           [-1, 32, 32, 32]           1,024\n",
            "       BatchNorm2d-8           [-1, 32, 32, 32]              64\n",
            "              ReLU-9           [-1, 32, 32, 32]               0\n",
            "           Conv2d-10           [-1, 32, 16, 16]             288\n",
            "      BatchNorm2d-11           [-1, 32, 16, 16]              64\n",
            "             ReLU-12           [-1, 32, 16, 16]               0\n",
            "           Conv2d-13           [-1, 32, 16, 16]           1,024\n",
            "      BatchNorm2d-14           [-1, 32, 16, 16]              64\n",
            "             ReLU-15           [-1, 32, 16, 16]               0\n",
            "           Conv2d-16           [-1, 32, 16, 16]             288\n",
            "      BatchNorm2d-17           [-1, 32, 16, 16]              64\n",
            "             ReLU-18           [-1, 32, 16, 16]               0\n",
            "           Conv2d-19           [-1, 16, 16, 16]             512\n",
            "      BatchNorm2d-20           [-1, 16, 16, 16]              32\n",
            "             ReLU-21           [-1, 16, 16, 16]               0\n",
            "           Conv2d-22             [-1, 16, 8, 8]             144\n",
            "      BatchNorm2d-23             [-1, 16, 8, 8]              32\n",
            "             ReLU-24             [-1, 16, 8, 8]               0\n",
            "           Conv2d-25             [-1, 16, 8, 8]             256\n",
            "      BatchNorm2d-26             [-1, 16, 8, 8]              32\n",
            "             ReLU-27             [-1, 16, 8, 8]               0\n",
            "        AvgPool2d-28             [-1, 16, 1, 1]               0\n",
            "           Linear-29                  [-1, 100]           1,700\n",
            "           Linear-30                    [-1, 1]             101\n",
            "================================================================\n",
            "Total params: 6,393\n",
            "Trainable params: 6,393\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 2.95\n",
            "Params size (MB): 0.02\n",
            "Estimated Total Size (MB): 2.98\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HRDDce8FXef7"
      },
      "source": [
        "* We build two neural networks for the two Critic models and two neural networks for the two Critic targets\n",
        "* Uses common functions defined above\n",
        "* State and Actor inputs are concatenated before FC (fully connected) layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5svIwtleSxLW",
        "colab": {}
      },
      "source": [
        "# Creating the architecture of the Neural Network\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Defining the first Critic neural network\n",
        "        self.c1_model = state_model()\n",
        "        self.c1_fc1 = nn.Linear(16 + action_dim, 100)\n",
        "        self.c1_fc2 = nn.Linear(100, 1)\n",
        "\n",
        "        # Defining the second Critic neural network\n",
        "        self.c2_model = state_model()\n",
        "        self.c2_fc1 = nn.Linear(16 + action_dim, 100)\n",
        "        self.c2_fc2 = nn.Linear(100, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "      #xu = torch.cat([x, u], 2)\n",
        "      # Forwad-Propagation on the first Critic Neural Network\n",
        "      x1 = self.c1_model(x)\n",
        "      x1 = x1.view(-1, 16)\n",
        "      x1 = torch.cat([x1, u], 1)\n",
        "      x1 = self.c1_fc2(self.c1_fc1(x1))\n",
        "\n",
        "      # Forward-Propagation on the second Critic Neural Network\n",
        "      x2 = self.c2_model(x)\n",
        "      x2 = x2.view(-1, 16)\n",
        "      x2 = torch.cat([x2, u], 1)\n",
        "      x2 = self.c2_fc2(self.c2_fc1(x2))\n",
        "\n",
        "      return x1, x2\n",
        "\n",
        "    def Q1(self, x, u):\n",
        "      x1 = self.c1_model(x)\n",
        "      x1 = x1.view(-1, 16)\n",
        "      x1 = torch.cat([x1, u], 1)\n",
        "      x1 = self.c1_fc2(self.c1_fc1(x1))\n",
        "      return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQkGTKaYFt5n",
        "colab_type": "text"
      },
      "source": [
        "* Below torch summary fails to calculate parameters\n",
        "* This is a known torchsummary issue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPFqU04lYoUM",
        "colab_type": "code",
        "outputId": "6be768de-8d97-473a-90f1-6b0a1b5c004e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Critic(2,1).to(device)\n",
        "summary(model, input_size=[(1, 32, 32), (1,)])"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             288\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]             288\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "            Conv2d-7           [-1, 32, 32, 32]           1,024\n",
            "       BatchNorm2d-8           [-1, 32, 32, 32]              64\n",
            "              ReLU-9           [-1, 32, 32, 32]               0\n",
            "           Conv2d-10           [-1, 32, 16, 16]             288\n",
            "      BatchNorm2d-11           [-1, 32, 16, 16]              64\n",
            "             ReLU-12           [-1, 32, 16, 16]               0\n",
            "           Conv2d-13           [-1, 32, 16, 16]           1,024\n",
            "      BatchNorm2d-14           [-1, 32, 16, 16]              64\n",
            "             ReLU-15           [-1, 32, 16, 16]               0\n",
            "           Conv2d-16           [-1, 32, 16, 16]             288\n",
            "      BatchNorm2d-17           [-1, 32, 16, 16]              64\n",
            "             ReLU-18           [-1, 32, 16, 16]               0\n",
            "           Conv2d-19           [-1, 16, 16, 16]             512\n",
            "      BatchNorm2d-20           [-1, 16, 16, 16]              32\n",
            "             ReLU-21           [-1, 16, 16, 16]               0\n",
            "           Conv2d-22             [-1, 16, 8, 8]             144\n",
            "      BatchNorm2d-23             [-1, 16, 8, 8]              32\n",
            "             ReLU-24             [-1, 16, 8, 8]               0\n",
            "           Conv2d-25             [-1, 16, 8, 8]             256\n",
            "      BatchNorm2d-26             [-1, 16, 8, 8]              32\n",
            "             ReLU-27             [-1, 16, 8, 8]               0\n",
            "        AvgPool2d-28             [-1, 16, 1, 1]               0\n",
            "           Linear-29                  [-1, 100]           1,800\n",
            "           Linear-30                    [-1, 1]             101\n",
            "           Conv2d-31           [-1, 32, 32, 32]             288\n",
            "      BatchNorm2d-32           [-1, 32, 32, 32]              64\n",
            "             ReLU-33           [-1, 32, 32, 32]               0\n",
            "           Conv2d-34           [-1, 32, 32, 32]             288\n",
            "      BatchNorm2d-35           [-1, 32, 32, 32]              64\n",
            "             ReLU-36           [-1, 32, 32, 32]               0\n",
            "           Conv2d-37           [-1, 32, 32, 32]           1,024\n",
            "      BatchNorm2d-38           [-1, 32, 32, 32]              64\n",
            "             ReLU-39           [-1, 32, 32, 32]               0\n",
            "           Conv2d-40           [-1, 32, 16, 16]             288\n",
            "      BatchNorm2d-41           [-1, 32, 16, 16]              64\n",
            "             ReLU-42           [-1, 32, 16, 16]               0\n",
            "           Conv2d-43           [-1, 32, 16, 16]           1,024\n",
            "      BatchNorm2d-44           [-1, 32, 16, 16]              64\n",
            "             ReLU-45           [-1, 32, 16, 16]               0\n",
            "           Conv2d-46           [-1, 32, 16, 16]             288\n",
            "      BatchNorm2d-47           [-1, 32, 16, 16]              64\n",
            "             ReLU-48           [-1, 32, 16, 16]               0\n",
            "           Conv2d-49           [-1, 16, 16, 16]             512\n",
            "      BatchNorm2d-50           [-1, 16, 16, 16]              32\n",
            "             ReLU-51           [-1, 16, 16, 16]               0\n",
            "           Conv2d-52             [-1, 16, 8, 8]             144\n",
            "      BatchNorm2d-53             [-1, 16, 8, 8]              32\n",
            "             ReLU-54             [-1, 16, 8, 8]               0\n",
            "           Conv2d-55             [-1, 16, 8, 8]             256\n",
            "      BatchNorm2d-56             [-1, 16, 8, 8]              32\n",
            "             ReLU-57             [-1, 16, 8, 8]               0\n",
            "        AvgPool2d-58             [-1, 16, 1, 1]               0\n",
            "           Linear-59                  [-1, 100]           1,800\n",
            "           Linear-60                    [-1, 1]             101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-141-cc92e98592ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torchsummary/torchsummary.pyc\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# assume 4 bytes/number (float on cuda).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mtotal_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mtotal_output_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# x2 for gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mtotal_params_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2770\u001b[0m     \"\"\"\n\u001b[1;32m   2771\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2772\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'tuple'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NzIDuONodenW"
      },
      "source": [
        "## Define Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.max_action = torch.Tensor(max_action).to(device)\n",
        "    self.actor = Actor(state_dim, action_dim, self.max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, self.max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    \n",
        "  def select_action(self, state):\n",
        "    state = state.unsqueeze(0).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)#.squeeze(1)\n",
        "\n",
        "      # We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action[0], self.max_action[0])\n",
        "\n",
        "      # The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gGuKmH_ijf7U"
      },
      "source": [
        "# Set the Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HFj6wbAo97lk",
        "colab": {}
      },
      "source": [
        "env_name = \"AutoDrivingCarModels\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 2e4 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hjwf2HCol3XP"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1fyH8N5z-o3o",
        "outputId": "9e755f86-070b-4110-dd76-a4d65f1ce4a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"T3D\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: T3D_AutoDrivingCarModels_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kop-C96Aml8O"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Src07lvY-zXb",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjXU4F5qV69_",
        "colab_type": "text"
      },
      "source": [
        "# Download Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFxh7LT1V7Oe",
        "colab_type": "code",
        "outputId": "ee1a8a19-ab5a-4386-b647-47215de9a885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        }
      },
      "source": [
        "!wget -O MASK1.png https://drive.google.com/uc\\?export\\=view\\&id\\=1eHsgsEdvdUMZjSR5XUhA2kFgdPK3qSN3"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-19 06:18:25--  https://drive.google.com/uc?export=view&id=1eHsgsEdvdUMZjSR5XUhA2kFgdPK3qSN3\n",
            "Resolving drive.google.com (drive.google.com)... 216.58.193.206, 2607:f8b0:4007:80e::200e\n",
            "Connecting to drive.google.com (drive.google.com)|216.58.193.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-60-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9990b9083fvdaa33irr6c7lc6l9tu6tm/1587277050000/09305796575597883680/*/1eHsgsEdvdUMZjSR5XUhA2kFgdPK3qSN3?e=view [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-04-19 06:18:25--  https://doc-0c-60-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9990b9083fvdaa33irr6c7lc6l9tu6tm/1587277050000/09305796575597883680/*/1eHsgsEdvdUMZjSR5XUhA2kFgdPK3qSN3?e=view\n",
            "Resolving doc-0c-60-docs.googleusercontent.com (doc-0c-60-docs.googleusercontent.com)... 216.58.217.193, 2607:f8b0:4007:808::2001\n",
            "Connecting to doc-0c-60-docs.googleusercontent.com (doc-0c-60-docs.googleusercontent.com)|216.58.217.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 103090 (101K) [image/png]\n",
            "Saving to: ‘MASK1.png’\n",
            "\n",
            "MASK1.png           100%[===================>] 100.67K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2020-04-19 06:18:25 (1.22 MB/s) - ‘MASK1.png’ saved [103090/103090]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR0P41ogGZcN",
        "colab_type": "text"
      },
      "source": [
        "# Define helper functions\n",
        "* Below are helper functions to crop the image and perform rotation on it\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxN_v2aAppds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.markers as mmarkers\n",
        "\n",
        "def center_crop_img(img, x, y, crop_size):\n",
        "  max_x, max_y = img.size\n",
        "  pad_left, pad_right, pad_bottom, pad_top = 0, 0, 0, 0\n",
        "  start_x = x - int(crop_size/2)\n",
        "  start_y = y - int(crop_size/2)\n",
        "  end_x = x + int(crop_size/2)\n",
        "  end_y = y + int(crop_size/2)\n",
        "  if start_x < 0:\n",
        "      pad_left = -start_x\n",
        "      start_x = 0\n",
        "  if end_x >= max_x:\n",
        "      pad_right = end_x - max_x\n",
        "  if start_y < 0:\n",
        "      pad_top = -start_y\n",
        "      start_y = 0\n",
        "  if end_y >= max_y:\n",
        "      pad_bottom = end_y - max_y\n",
        "  padding = (int(pad_left), int(pad_top), int(pad_right), int(pad_bottom))\n",
        "  new_img = ImageOps.expand(img, padding, fill=255)\n",
        "  crop_img = new_img.crop((start_x, start_y, start_x+crop_size, start_y+crop_size))\n",
        "\n",
        "  return crop_img\n",
        "\n",
        "def rotate_img(img, angle):\n",
        "  im1 = img.convert('RGBA')\n",
        "  rot = im1.rotate(angle)\n",
        "  # a white image same size as rotated image\n",
        "  fff = Image.new('RGBA', rot.size, (255,)*4)\n",
        "  # create a composite image using the alpha layer of rot as a mask\n",
        "  return Image.composite(rot, fff, rot).convert('L')\n",
        "\n",
        "def show_img(ax, img, title=\"\"):\n",
        "  np_img = np.asarray(img)/255\n",
        "  np_img = np_img.astype(int)\n",
        "  ax.imshow(np_img, cmap='gray', vmin=0, vmax=1)\n",
        "  if title != \"\":\n",
        "    ax.set_title(title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qEAzOd47mv1Z"
      },
      "source": [
        "## Create the environment\n",
        "* We simulate Kivy environment here. As Kivy doesn't do much apart from Graphics\n",
        "* We maintain x,y position and car's angle. This is rotated based on action\n",
        "* Action here is one-dimensional, which is the amount of degrees the car should rotate\n",
        "* If x,y position is on sand, we set a small velocity, else a slightly high velocity\n",
        "* Our state here corresponds to the cropped portion of current postion as center. This image is rotated to be in the direction of car. This was our network understands car's orientation\n",
        "* Cropping here is done differently. If we directly crop and rotate the image, we may loose information from the edges. Hence we do the following:\n",
        "  * Crop a larger portion of image\n",
        "  * Rotate it to make the cropped image in the direction of car's orientation\n",
        "  * Then crop it to required size\n",
        "* `reset` function picks a random coordinates on road and a random angle and returns state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFCJs2XJKzIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.markers as mmarkers\n",
        "\n",
        "from PIL import Image as PILImage\n",
        "from kivy.vector import Vector\n",
        "from collections import Counter\n",
        "\n",
        "class CarEnv(object):\n",
        "    def __init__(self):\n",
        "        self.filename = \"MASK1.png\"\n",
        "        img = PILImage.open(self.filename).convert('L')\n",
        "        self.sand = np.asarray(img)/255\n",
        "        self.sand = self.sand.astype(int)\n",
        "        self.max_y, self.max_x = self.sand.shape\n",
        "        self.pos = Vector(int(self.max_x/2), int(self.max_y/2))\n",
        "        self.angle = Vector(0,10).angle(self.pos)\n",
        "        self.velocity = Vector(6, 0)\n",
        "        self.wall_padding = 20\n",
        "        self.rel_pos = Vector(0,0)\n",
        "        self.max_angle = 10\n",
        "        self.max_action = [self.max_angle]\n",
        "        self.crop_size = 100\n",
        "        self.goal_iter = 0\n",
        "        self.goals = [Vector(1890, 150), Vector(140, 380)]\n",
        "        self.last_distance = 0\n",
        "        self.state_dim = (32, 32)\n",
        "        self.action_dim = (1,)\n",
        "        self._max_episode_steps = 4000\n",
        "        # track rewards distribution\n",
        "        self.rewards_distribution = Counter()\n",
        "      \n",
        "    def seed(self, seed):\n",
        "        pass\n",
        "    \n",
        "    def reset(self):\n",
        "        self.angle = np.random.randint(low=0, high=360)\n",
        "        onsand = True\n",
        "        while onsand:\n",
        "          self.pos.x = np.random.randint(low=self.wall_padding, high=self.max_x-self.wall_padding)\n",
        "          self.pos.y = np.random.randint(low=self.wall_padding, high=self.max_y-self.wall_padding)\n",
        "          if self.sand[int(self.pos.y),int(self.pos.x)] <= 0:\n",
        "              onsand = False\n",
        "        self.velocity = Vector(2, 0).rotate(self.angle)\n",
        "        return self.get_state()\n",
        "    \n",
        "    def random_action(self):\n",
        "        rotation = np.random.randint(low=-self.max_angle, high=self.max_angle)\n",
        "        return (rotation,)\n",
        "    \n",
        "    def step(self, action):\n",
        "        rotation = action[0]\n",
        "        self.angle += rotation\n",
        "        self.pos = Vector(*self.velocity) + self.pos\n",
        "        self.pos.x = int(self.pos.x)\n",
        "        self.pos.y = int(self.pos.y)\n",
        "        reward = 0\n",
        "        done = False\n",
        "        current_goal = self.goals[self.goal_iter]\n",
        "\n",
        "        distance = self.pos.distance(current_goal)\n",
        "        if self.sand[int(self.pos.y),int(self.pos.x)] > 0:\n",
        "            self.velocity = Vector(0.5, 0).rotate(self.angle)\n",
        "            reward = -1\n",
        "            tag = \"sand (-1)\"\n",
        "        else: # otherwise\n",
        "            self.velocity = Vector(2, 0).rotate(self.angle)\n",
        "            reward = -0.1\n",
        "            tag = \"road (-0.1)\"\n",
        "            \n",
        "            if distance < self.last_distance:\n",
        "              reward = 1\n",
        "              tag = \"road (+1)\"\n",
        "\n",
        "        if self.pos.x < self.wall_padding:\n",
        "            self.pos.x = self.wall_padding\n",
        "            reward = -5\n",
        "            tag = \"wall (-5)\"\n",
        "            done = True\n",
        "        if self.pos.x > self.max_x - self.wall_padding:\n",
        "            self.pos.x = self.max_x - self.wall_padding\n",
        "            reward = -5\n",
        "            tag = \"wall (-5)\"\n",
        "            done = True\n",
        "        if self.pos.y < self.wall_padding:\n",
        "            self.pos.y = self.wall_padding\n",
        "            reward = -5\n",
        "            tag = \"wall (-5)\"\n",
        "            done = True\n",
        "        if self.pos.y > self.max_y - self.wall_padding:\n",
        "            self.pos.y = self.max_y - self.wall_padding\n",
        "            reward = -5\n",
        "            tag = \"wall (-5)\"\n",
        "            done = True\n",
        "\n",
        "        if distance < 25:\n",
        "            self.goal_iter = (self.goal_iter + 1) % len(self.goals)\n",
        "            goal = self.goals[self.goal_iter]\n",
        "            reward = 10\n",
        "            tag = \"goal (+10)\"\n",
        "            done = True\n",
        "\n",
        "        self.last_distance = distance\n",
        "        self.rewards_distribution[tag] += 1\n",
        "        return self.get_state(), reward, done\n",
        "    \n",
        "    def render(self):\n",
        "        # Create figure and axes\n",
        "        fig, ax = plt.subplots(1, 5, figsize=(30, 6))\n",
        "\n",
        "        # Display the image\n",
        "        ax[0].imshow(self.sand, cmap='gray', vmin=0, vmax=1)\n",
        "        # Create a Rectangle patch\n",
        "        rect = patches.Rectangle(\n",
        "            (self.pos.x - int(self.crop_size/2), self.pos.y - int(self.crop_size/2)),\n",
        "            self.crop_size, self.crop_size,\n",
        "            linewidth=1, edgecolor='r', facecolor='none'\n",
        "        )\n",
        "        # Add the patch to the Axes\n",
        "        ax[0].add_patch(rect)\n",
        "        ax[0].set_title(\"x=%d,y=%d,angle=%d\" % (self.pos.x, self.pos.y, self.angle))\n",
        "        \n",
        "        marker = mmarkers.MarkerStyle(marker=\"$ \\\\rightarrow$\")\n",
        "        marker._transform = marker.get_transform().rotate_deg(self.angle)\n",
        "        ax[0].scatter(self.pos.x, self.pos.y, s=50, c='red', marker=marker)\n",
        "        self.get_state(ax).cpu().numpy()\n",
        "        plt.show()\n",
        "        \n",
        "    def get_state(self, ax=None):\n",
        "        resize = T.Compose([T.ToPILImage(),\n",
        "                            T.Resize(self.state_dim[0], interpolation=Image.CUBIC),\n",
        "                            T.ToTensor()])\n",
        "        \n",
        "        img = Image.open(self.filename).convert('L')\n",
        "\n",
        "        # If we directly crop and rotate the image, we may loose information\n",
        "        # from the edges. Hence we do the following:\n",
        "        #   * Crop a larger portion of image\n",
        "        #   * Rotate it to make the cropped image in the direction\n",
        "        #     of car's orientation\n",
        "        #   * Then crop it to required size\n",
        "        crop_img = center_crop_img(img, self.pos.x, self.pos.y, self.crop_size*3)\n",
        "        if ax is not None:\n",
        "          show_img(ax[1], crop_img, \"large crop\")\n",
        "\n",
        "        r_img = rotate_img(crop_img, -self.angle)\n",
        "        if ax is not None:\n",
        "          show_img(ax[2], r_img, \"rotated crop\")\n",
        "\n",
        "        r_img_x, r_img_y = r_img.size\n",
        "        crop_img = center_crop_img(r_img, int(r_img_x/2), int(r_img_y/2), self.crop_size)\n",
        "        if ax is not None:\n",
        "          show_img(ax[3], crop_img, \"final crop\")\n",
        "\n",
        "        np_img = np.asarray(crop_img)/255\n",
        "        np_img = np_img.astype(int)\n",
        "        screen = np.ascontiguousarray(np_img, dtype=np.float32) \n",
        "        screen = torch.from_numpy(screen)\n",
        "        screen = resize(screen)\n",
        "        if ax is not None:\n",
        "            np_img = screen.squeeze(0).numpy()\n",
        "            np_img = np_img.astype(int)\n",
        "            ax[4].imshow(np_img, cmap='gray', vmin=0, vmax=1)\n",
        "            marker = mmarkers.MarkerStyle(marker=\"$ \\\\rightarrow$\")\n",
        "            ax[4].scatter(self.state_dim[0]/2, self.state_dim[1]/2, s=100, c='red', marker=marker)\n",
        "            ax[4].set_title(\"final resized img\")\n",
        "        return screen.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIZ7-z_ARu7Y",
        "colab_type": "text"
      },
      "source": [
        "# Visualizing Environment\n",
        "* Here we crop large portion of image from postion x,y. Then rotate it and then crop it again to required size. Then resize it to required final size\n",
        "* It is visualized below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "VsSMlufaRYw3",
        "colab_type": "code",
        "outputId": "1445f96a-57c8-4ef8-92b2-326131f0265e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "env = CarEnv()\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABrUAAAFQCAYAAAARGPVvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAIABJREFUeJzs3X38ZWVd7//3GxBvALmRaYJhYEgnFSvR+YZ6NCXRArSG+hVhpmh6RgtLy1S0PJonS/udVLTCRlFHI5WDGmSYGUodKsj5KscENEYEZ5CbAZE7bxD9nD/WtWHNnr2/+27dr9fz8diP73fvvfbe17r7rGtdn3VdyxEhAAAAAAAAAAAAoMn2qLsAAAAAAAAAAAAAwCQktQAAAAAAAAAAANB4JLUAAAAAAAAAAADQeCS1AAAAAAAAAAAA0HgktQAAAAAAAAAAANB4JLUAAAAAAAAAAADQeCS1AABoEdvX2H5a3eUAAJTPdth+WN3lAICy2H647cts32H7t22/0/ZrC/jedSmG7lVEOQGgCm2PibYPt32n7T0L/t6x7SBFLSO0Cwd3AAAAAJjA9vsk7YiIP5hy+udJemFEPKnMcgFAy71S0mci4ui6CwIADdDqmBgRX5O0b8W/+eIqfw/NQE8tAAB6oM6rVIu+SgsAitblK/m7PG8AOuEISZfXXYhxiKEAKlZ7TCTuoQ1IagEA0FK2j7H977a/aft6239ue+/c+2H7NNtXSboqvfYztr9s+zbbf2n7n22/MPeZX7d9pe1bbX/S9hEr/P6TbP9b+v3tqVeCbL/P9pm2L7B9l6Sftr2/7ffb3mn7Wtt/YHuPNP3zbP9rKv9ttr9k+7iSFhsASLp3GJNX2f6CpLts72X7kbYvSnHtcts/n6bdJOnZkl6ZhlT5u/T66ba/koaIucL2L6TXHynpnZKekKb/Znr9/rb/l+2v2b4xDZfywFyZXpHi+ddt//qE8h9k+71p2ltt/216/VjbO9K83SDpven1/257m+1v2D7f9qG574o0xM3Vtm+2/f8PYjQAlMX2pyX9tKQ/T7HyR1M98o/S+4N49nLbN6X4+Pzc559h+/O2b0910dfP8NtrbX801U1vsf3n6fVBvfSttm+R9Hrbe6S667WpHO+3vX+afjCk16YUj6+3/XtFLicA/VBzTBxVLz7U9kdSnPyq7d/OTX+M7a3pt260/Zb0+r3DHNoe1IMHj+/YviZNt0euHn2L7XNsH5T7/uekmHuL7d+fUPZRy+iVuWV0ku0Tbf9Xqge/JvfZB9rekurSV6bP7Zh2uaE+nKgAANBe35f0O5IOlvQEScdJ+s2haU6S9DhJR9k+WNK5kl4t6SGSvizpvw0mtL1R0msk/aKkVZL+j6QPjvphZ8muT0h6R5r2aEmX5Sb5VUlvlLSfpIvTdPtL+hFJT5H0XEnPz03/OElfSfPyOkkfzVdqAaAkz5L0DEkHSLKkv5P0j5J+SNJvSTrb9sMjYrOksyX9aUTsGxE/lz7/FUk/pSy+/aGkv7Z9SERcKenFkv49TX9Amv5Nkn5UWcx8mKQ1kv6HJNk+XtLvSXq6pPWSJt0/8QOSHiTpUam8b82998OSDlJ2te8m20+V9CeSTpZ0iKRrJX1o6Pt+QdKSpMdK2ihpxaQaACwqIp6qrL75khQr/2vEZD+sLMaukfQCSX9h+8D03l3K6pQHKIvlv2H7pEm/62wUgY8ri4Xr0nfnY+LjJF0tabWy+uzz0uOnldVl95X050Nf+9PKYvfPSHqVuQcugBnVFRNz8vXiHyirF//f9FvHSXqZ7Z9N054h6YyIeLCkh0o6Z8T8DOrB+0o6UNKluq994beUtVU8RdKhkm6V9BeSZPsoSWdKek567yGSDpthPn5Y0gN0Xz37XZJ+TdIGZfX219o+Mk37OmXHgR9RVgf/tRl+BzUiqQUAQEtFxHJEXBIR90TENZL+SlmlMO9PIuIbEfFtSSdKujwiPhoR90h6u6QbctO+OE1/ZXr/jyUd7dG9tX5V0j9FxAcj4nsRcUtE5JNa50XEv0bEDyR9T9Ipkl4dEXeksv6ZskrqwE2S3pa+68PKEm7PmGvBAMD03h4R21OMfLyyhso3RcTdEfFpZY2ezxr34Yj43xHx9Yj4QYpdV0k6ZtS0ti1pk6TfSXH5DmVx9pQ0ycmS3hsRX4yIuyS9ftzv2j5E0gmSXhwRt6bY+c+5SX4g6XUR8d00b8+W9J6I+FxEfFfZxQ1PsL0u95k3p3J9TdLbVppvAKjQ9yS9IcW5CyTdKenhkhQRF0XEf6YY/AVljaXDdeFRjlHWUPqKiLgrIr4TERfn3v96RLwj1bEHMfQtEXF1RNypLIae4l2H6PrD9F3/qayHLDEUQBnKiIkD+XrxT0paFRFvSPXiq5Ulhwb11u9JepjtgyPizoi4ZNJ3S7pD0qDX1Ysl/X5E7Eh109dL+qUUV39J0scj4l/Se69VVred1vckvTEivqfsgoWDlSXg7oiIyyVdIenRadqTJf1xqk/vSOVEC5DUAgCgpdJwBB+3fYPt25U1jh48NNn23P+H5p9HREjKd60/QtIZzobd+qakbyjrubBmxM+vVdZDYZz87x4s6X7KroYduHboe69L5cm/f6gAoFy7xciUjB8YjlW7sP1c25fl4uaPafc4PLBKWc+q5dz0/5Bev/f3h357nLWSvhERt455f2dEfCf3/ND896VG2Vu067wN/zYxGEAT3JIuthr4lrILEGT7cbY/k4bGuk1ZI+m4GJy3VtK1Q9+bt33o+S4xNP2/l7KeXKM+QwwFUJYyYuJAPo4dIenQQZ011Vtfo/vi3guUjT7wJduftf3McV9q+0WSjpX0q7l69hGSPpb77iuVjUSzWru3W9ylrN46rVsi4vvp/2+nvzfm3v+20jIb/i3tHv/RUCS1AABorzMlfUnS+tTt/zXKklB5+UTR9cp120+9BvLd+LdLelFEHJB7PDAi/m3Eb29XNszAOPnfvVnZ1VL5Hl+HS7ou93xNKk/+/a+v8P0AUIR8rPq6pLXe9V5S+ViVn3YwDOu7JL1E0kPSEINf1H1xeJfplcXCb0t6VC7G7p+GZJGyGL126LfH2S7pINsHjHl/+Le/rlwMtr2PsqFc8nF4+LeJwQCa7m8knS9pbUTsr+xehsN14VG2Szp8qKdV3ooxVFmMvEe7NpISQwHUbd6YOJCPfdslfXWobWC/iDhRkiLiqoh4lrIhsN8s6dxUv9yF7Z+S9D8lbYyI24e+/4Sh739ARFynoTqx7Qcpq7eWYZc2Eu0ay9FgJLUAAGiv/STdLulO24+Q9BsTpv97ST+ebpS6l6TTlI03PfBOSa+2/ShJsr2/7V8e811nS3qa7ZPTTWAfYvvoUROmq6TOkfRG2/ulhuDflfTXucl+SNJv275f+s1HSrpgwvwAQJEuVXa16ytTLDpW0s/pvvus3KhsvP2BfZSd/O+UJGc36v6x3Ps3SjrM9t6SlK5MfZekt9r+ofSZNbl7E5wj6Xm2j0on768bV9CIuF7ZfQ3/0vaBqbxPXmHePijp+baPtn1/ZT17L03DwQ68In3XWkkvlfThFb4PAJpgP2W9Vr9j+xhlw2NP4z+UNWS+yfY+th9g+4krTP9BSb9j+0jb+yqLoR8e6i3xWtsPSvXo54sYCqB688bEUf5D0h22X2X7gbb3tP1jtn9Skmz/mu1VqX77zfSZXYYITHXKcyQ9N3a/P9g7lbUPHJGmXZXu8S1l9wF/pu0npXr0G1ReDuMcZW0gB9peo+xiNbQASS0AANrr95RVVO9Q1lC64slzRNws6Zcl/amy7vtHSdoq6bvp/Y8pu8rqQ2k4wy8qu2fLqO/6mrJ7dL1c2TCFl+m+calH+S1lN669WtLFyq4ie0/u/UuV3Vz7ZmU35P6liJhliAEAWEhE3K0siXWCslj0l8pOwr+UJjlL0lFpmJS/jYgrlN0f8N+VJbB+XNK/5r7y05Iul3SD7ZvTa6+StE3SJSnO/pPuuw/CJ5Tdy+rTaZpPTyjyc5T1gv2SsvsSvmyFefsnZfcj+IiyhtyH6r57IgycJ2lZWTz/+zS/ANBkvynpDbbvkPQ/lDVOTpQuuPo5SQ+T9DVlw3H/ygofeY+kD0j6F0lflfQdZXXbvH9WFrsvlPS/IuIfp58NACjEXDFxlBQnnynpaGVx72ZJ75a0f5rkeEmX275T0hmSTkn34so7TtlwgufavjM9Lk/vnaGsV9k/pvJeIulx6bcvV3YB7t8oq7feql1vm1CkN6Tv/qqyevm5Su0jaDbvevsKAADQF2mIrR2Snh0Rn6mxHM+T9MKIeFJdZQCAPrMdyoay3VZ3WQCgTWyvU9YYer8V7tEFAGgB27+hLEH3lLrLgpXRUwsAgB6x/bO2D0jDTw3uwXVJzcUCAAAAAACojO1DbD/R9h62H65sJJqP1V0uTDbuppgAAKCbnqCsG//ekq6QdNKIYQIAAAAAAAC6bG9JfyXpSGX3BvuQsiHI0XAMPwgALWX7eGXjEO8p6d0R8aaaiwQAnUKcBQAAAACgWRh+EABayPaekv5C2c3sj5L0LNtH1VsqAOgO4iwAlM/28ba/bHub7dPrLg8AAACaj6QWALTTMZK2RcTVEXG3si7SG2suEwB0CXEWAErExQMAAACYB/fUAoB2WiNpe+75DkmPGzfxwQcfHOvWrSu7TLVZXl6uuwi9tWHDhrqLULuit78uLtPl5eWbI2JV3eWY0UxxVup+rAXQXNdcc41uvvlm112OGd178YAk2R5cPHDFuA8QZwHUqaV12pkQZwHUado4S1ILADrK9iZJmyTp8MMP19atW2suUXHstrXZdFeXtqt5Fb09dnGZ2r627jKUpcuxFkB7LC0t1V2Eecx88cC6deuIswBq09Y67Sz3iSXOAqjTtHGW4QcBoJ2uk7Q29/yw9Nq9ImJzRCxFxNKqVe2+mMz2Lg80Q0TUXYROYhtvjIlxVupWrAWAJrK9yfZW21t37txZd3EAoFUY6hVAF5HUAoB2+qyk9baPtL23pFMknV9zmRYynLgiidVcEUFCC33QuTgLAA3DxQMAUD7uEwugc0hqAUALRcQ9kl4i6ZOSrpR0TkRcXm+pZkPiqn1IZo3GMummLsRZAGg4Lh4AgPKNGup1TU1lAYBCcE8tAGipiLhA0gV1l2NaJK7aiYQN+qxtcRYA2iQi7rE9uHhgT0nv4eIBAKje8D1iAaDpSGoBAApD4qp9SFo1k23WDQCg87h4AABKN9X9uCVtlqSlpSVOQgA0HsMPAgDmwr2v2mkwhCBDCaIKxAYAAACgVgz1CqBz6KkFAJgKDdPtROKqGhHBPjKE5QEAAADUi6FeAXQRSS0AwC5oiG4XklZoGmIIAAAA0BwM9Qqga0hqAQBohG4ZElloIuIIAAAAAAAoG0ktAOgxGqHbgSQWmo5YAgAAAAAAqkBSCwB6hIbn5iJxhWG2G79dEFMAAAAAAECVSGoBQA8sLy/T+FyjpicmUIyI6M1+1pf5BAAAAAAAzUJSCwCAgpC8Qh+Q0AIAAAAAAHUhqQUAwJxIYqFsTRqCkGQWAAAAAACoG0ktAACm0JTEAlAHEloAAAAAAKAJSGoBAJCQuMKiunZfrS7NCwAAAAAAaD+SWgCA3iKJhTaoawhCEloAAAAAAKBpSGoBAHqDJBbaqsrEFsksAAAAAADQVCS1AACdQuIKXVV2YotkFgAAAAAAaDqSWgCAViOJhaZp43212lZeAAAAAADQTyS1AACtQQILfVdGby0SWgAAAAAAoC1IagEAGoXEFbCyohJbJLMAAAAAAEDbkNQCANSKJBYwu0USWySzAAAAAABAW5HUAgBUjkQWUA8SWgAAAAAAoM1IagEACkfSCn0XEaUnkGbprUUyCwAAAAAAdAFJLQDAwkhiAfWYlNgimQUAAABgWsvLyxPPITj/B1A3kloAgJlRiQUmq6K31kpIaAEAAAAAgK4hqQUAGInEFdAOg+TVYJ8lmQUAAAAAALpqj7oLAABoHhJaQDGq3pdIaAEAAAAAgC6jpxYA4F4ks4D2amJCi95jAAAAAACgSCS1AKCnSGAB1ehjYof4AgAAAAAAysDwgwDQMxFBgzOA0hBfALRJny44AAAAALqApBYA9MCGDRtIZgE168P+14d5BNAdg4SWbZJbAAAAQEuQ1AIAAKhIV5M+JM0BdAGJreZbXl4mCQkAANBz3FMLAHpgeXm57iIA6CASWQC6ZpAsIb4BADBaURcWcKwFMC96agEAAFSoKydvXZkPABiF3kAAAABAM9FTCwAAoGIR0drGUpJZALpg2hhsm7jXUOPWIesLAACg20hqAQAA1KBtiS0aCQF0xayxNz89sRAAAACoF0ktAOgJrjQGmqcNiS3iBgDch/pU8006rrL+AAAA2o17agEAANSEhBYAtA/32wIAAADqQ1ILAHqEBhigGdrQIEpCC0AXFRl7mx7HAQAAgC5i+EEAAICStanhk2QWgK4qIxYzHGH7jNsOWI8AAADtQFILAACgJCSzAAAAAAAAikNSCwAazvY1ku6Q9H1J90TEku2DJH1Y0jpJ10g6OSJunfL7aLwGStamZNZAn2ND0XEWQPO0MS4DANBlVR6b+3qeA3QV99QCgHb46Yg4OiKW0vPTJV0YEeslXZieA6jB4P5Y+Udbtb38CyLOApgZjWTdMep43pXjO4B+s32N7f+0fZntrXWXBwAWRVILANppo6Qt6f8tkk6qsSxAL3W5gaur8zUj4izQEcQ0AAB2u4ALAFqLpBYANF9I+kfby7Y3pddWR8T16f8bJK2up2hA/3Q5mZXXh3nMIc4CmBm9tAAAAIDqcU8tAGi+J0XEdbZ/SNKnbH8p/2ZEhO3dWlVSw+ym4dcBTK9niZ3dDOa/Bw23c8VZaddYe/jhh5dfUgCN0IO4iBHG1QsG20P+fbYRAA0yuIArJP1VRGyuu0AAsAh6agFAw0XEdenvTZI+JukYSTfaPkSS0t+bRnxuc0QsjRpeoO8N9cAkfemNNa2uL49542z6zL2xdtWqVVUVGcCUyopdXY6JAIDOeVJEPFbSCZJOs/3k/Ju2N9neyv22ALQFSS0AaDDb+9jeb/C/pJ+R9EVJ50s6NU12qqTz6ikh0B3cCH6yLi4b4iyAedALBwDQFmMu4Mq/P/aCWABoIoYfBIBmWy3pY6kheS9JfxMR/2D7s5LOsf0CSddKOrnGMgKt08XkTFVsd60xlzgLdBSxHlUZta1NGqoQAKqQLtraIyLuyF3A9YaaiwUACyGpBQANFhFXS3r0iNdvkXTcIt/dwYZpYCo0ci6uS/GjzDgLoJu6Ev8AAL0w8gKueosEAIshqQUAADqPRFbxBsuUxl0AfULMAwC0ybgLuPqmyvNB6gpA+binFgD0GA396Kr8/bHYzsvFsgbQRMSj5rC91vZnbF9h+3LbL02vH2T7U7avSn8PrLusVeCYCQAAsBiSWgAAoBNoJGoG1gMAYMg9kl4eEUdJeryk02wfJel0SRdGxHpJF6bnAAAAwIpIagEAgNbqewIlIho9vEWf1w2A+pQVd5ocb5ssIq6PiM+l/++QdKWkNZI2StqSJtsi6aR6SggAAIA24Z5aAACgNUiQZIYbViOCZQMASVkx0TaJrQXZXifpMZIulbQ6Iq5Pb90gaXVNxarNrNsp2x8AAABJLQAA0HAka3Y1rkGLxBYAZOip1Uy295X0EUkvi4jb8+spIsL2yAVse5OkTdWUEgAAAE3H8IMA0HM0gqOpGLpuV9MMNUiDK4C+47jRTLbvpyyhdXZEfDS9fKPtQ9L7h0i6adRnI2JzRCxFxFI1pQUAAECTkdQCAACNkL8/Fo2S9yWxZr1vVtPvswUAbURcnZ+zg/pZkq6MiLfk3jpf0qnp/1MlnVd12dpmuK5E3QkAAPQRSS2gILbfZ/uP6i4HALQJDTG7mieJtdJ3AUCfcCxprCdKeo6kp9q+LD1OlPQmSU+3fZWkp6XnAAAAwIpIanWU7T+1vd327bavtf2aofefavtz6f2r0zjlg/ds+/dtfy29/yHbDy65vC+xvdX2d22/b+i9dbbD9p25x2unnde2sn1/22elebojnfydkHt/b9vn2r4mLZ9jhz7/CttfTJ/9qu1XVD4TaA0agVAlElm7K6t3FYktAH3BMaW5IuLiiHBE/EREHJ0eF0TELRFxXESsj4inRcQ36i4rAACLmtSrtuoH0EUktbrrLEmPiIgHS/pvkp5t+xele8cz/5ikv5K0v6RfkfQW249On32usivpnijpUEkPlPSOksv7dUl/JOk9K0xzQETsmx7/M/f62Hltub0kbZf0FGXr6Q8knWN7XW6aiyX9mqQbRnzeytblgZKOl/QS26eUWF4AoFI9heFhBctOPDEcIQAshhiKtqChEwAA9AFJrQay/VDb37D92PT8UNs7h3virCQivhwRd+Ve+oGkh6X/D5L0YEkfiMxnJV0p6aj0/s9JOisitkfEnZLeLOlXbD9oRFn/wvafDb12vu3fmbasqbwfjYi/lXTLLJ9Ln11pXofL+1Dbn7Z9i+2bbZ9t+4Dc+9fY/j3bX7B9m+0P235A7v1X2r7e9tdtvzD1kBr3W89Mvau+afvfbP/EjPN1V0S8PiKuiYgfRMTHJX1V0ob0/t0R8baIuFjS90d8/k8j4nMRcU9EfFnZGPVPnKUMALASGkdmU3dyiUZZAF3F8QcAAADoD5JaDRQRX5H0Kkl/nRJJ75W0JSIusv2XKUky6vGF/PfYPt32nZJ2SNpH0t+k779R0gclPd/2nrafIOkIZb1+7v340P/3l7R+RHG3SHqW7T3Sbx6sbDz0v0nPP75CeT8+46K51vYO2+9NvzNxXkewpD9R1gPtkZLWSnr90DQnK+vZdKSkn5D0vPQbx0v63TR/D5N07LiC2n6Msl5nL5L0EGW94s63ff/0/szLxfZqST8q6fJxv7tCeSzpp+b5LPqDBqF+YxiD4ozqidWUhFLTygMATUe8RJeNq9dR1wMAAE1GUquhIuJdkrZJulTSIZJ+P73+mxFxwJjHTwx9x5sk7SfpsZI+IOm23NsflPQ/JH1X0v+R9PsRsT299w+SXujsXlb7K0uwSdJuPbUi4j/S9x6XXjpF0kUpcaaIeOYK5X3mlIvjZkk/qSzxtiHN09kzzGt+um0R8amI+G5E7JT0FmXD++W9PSK+nsZ0/ztJR6fXT5b03oi4PCK+pd2TYXmbJP1VRFwaEd+PiC3KlvXjUzlmWi7Ohow8W1ly80sr/O44r1e2v793js8C6BgSVOVoW7KobeUFAAAAAAAgqdVs75L0Y5LeERHfnecL0vCCn5f0bUl/KEm2HyHpQ8rut7S3pEdJeqXtZ6SPvUdZ0usiZT17PpNe3zHmZ7You6+T0t8PzFPWFebhzojYmobRu1HSSyT9jO39hqbbbV6H2V5t+0O2r7N9u6S/lnTw0GT5+1N9S9K+6f9Dld3jaiD//7AjJL083wNLWa+wQ1ee25Fl3kPZMr1b2bzP+vmXKFvXz5h3OwLQbiSwytOFnk9tLjsAlIn4CAAAADQPSa2Gsr2vpLdJOkvS620flF5/p+07xzxWGlpuL0kPTf//mKT/iohPpns1fVnS30s6QZLSa6+LiHURcZiyxNZ16THKX0vaaPvRyob0+9vcfHxihfJ+Ys7FMzi7HLf95ud12B+nz/94RDxYWRJu2hbe6yUdlnu+doVpt0t641APrAdFxAel6ZdLGjbwLEmrJf1/EfG9Kcs6+PyvSzpd0nERMS4pCdyLhEf7MExgtZo4pGARujY/APqD4xxQjFF1x3mGp6b+CQAAykZSq7nOkLQ1Il6oLOH0TkmKiBdHxL5jHo+Ssp49tl9k+0BnjpF0mqQL03d/XtJ6209N7z9U0jMlfSF9/iDbD03vHaVsiL43RMQP0vuvt33RoKApWfJZZb2JPhIR3869d8IK5T1hMJ3tvWw/QNKekva0/QDbe6X3Hmf74Wm+HiLp7cqGOLxtinmV7WtsPy893U/SnZJus71G0itmWCfnKLsP2SOd3evstStM+y5JL05lt+19bD9j0Lts2uUi6UxlicKfyy/X3LzdPy03Sdo7LTen956tLIn39Ii4eob5BNBgNBhUr4tJrHH6MI8AuqWsuEU8BAAAAJqJpFYD2d4o6XhJv5Fe+l1Jj01Jimn9gqSvSLpDWU+qd6SHIuIrkn5dWXLodkn/LOkjkt6dPnuwpAsk3SXpE5LeExGbc9+9VtK/Dv3eFkk/rvmHHvwDZcMGnq6s99S302uS9CPK7vN1h6QvKrs31bOmmVfbe0t6iKRL0rR/qOy+W7cpSxZ+dNoCRsQnlC2zzyi739ngO3cb0i8itkr675L+XNKtafrnTftbqexHSHqRsnt63ZDryZXfDr6sbFmtkfTJ9P8R6b0/Ujbvn8199p2zlAFA/Uhi1aNPiaxhfZxnAO3FsREAAADoF9NwgVnZvkzZcHa35F57srKE0hHRoI3K9pMknRYRz5o48ezf/UhlSbb7R8Q9RX8/UCTbM+2XDdqNO4/GuGZgm19ZAdvpckQsFVGWJltaWoqtW7fWXQygV8o4jrbxmLC0tKStW7d2vlIxa50WzdPG/QsYsN35Oi1xFosgxmNR08bZvaooDLolIo7OP7d9P0kvlfTuJiW0JCkiLpZ0cVHfZ/sXlPVie5CkN0v6OxJaAGZBEqs5GnbIarTBsmL7BdAkJLQAAACA/ill+EHbx9v+su1ttk8v4zfQDKm30jclHSLpbTUXpwovknSTsuEOv6/7hogEgJEYPrCZaLScD8sNQFNwTAUAAAD6qfCeWrb3lPQXkp4uaYey+/mcHxFXFP1bqF9EXClpn7rLUZWIOL7uMgBVsE3j9QxoWGs2tuVijVqe7AMAAGAa89YZqM8BAICBMnpqHSNpW0RcHRF3S/qQpI0l/A4AALWg91XzRcS9D5SPZV0M4gkAAAAAACsrI6m1RtL23PMd6TUAAFqJJFZ7kFypF8t/cfk4Q7wBqkX8AgAAAJqv8OEHp2V7k6RNkrTPPvtseMQjHlFXUUq3vLxc2ndv2LBh7s8Ol2vW75o0X4uUreum3SbqXIb7mc3WAAAgAElEQVSDMvZhPS4vL98cEavqLgfqQ8NxO9H42FzD64Z9bLLhZZRPbLGtA7sqI6awnwHNNut+zz4NAEB3lZHUuk7S2tzzw9Jru4iIzZI2S9LS0lJs3bq1hKJUr6xGmzIqZMNlnXYdTDOPVCDHm2UbGSSWil6es5ah6+vT9rV1lwHVooG93boek9A/k2LS4H22fYBjOAAAANB3ZSS1Pitpve0jlSWzTpH0qyX8TmP06UrBaeeVq4p3t9Kyi4gV35+1MavobbIJ63PSPNVdPjQfjWDtxj4OkNwCAAAAAKDwpFZE3GP7JZI+KWlPSe+JiMuL/p26FdU42qRGiZUSF/PMbxMSIXWZZXkNltHg7zTJrTqUsT6LnJ9pv6uv22TXkbDqFvZT9Mm8dawB9hf0Ccd7ANMaFS84ZgJAuZpYVyP2d1Mp99SKiAskXVDGd9dt3p2ziTvQqN5Bw4mLWXrHjJq2L4mtIreLaZJbdZlnfTZtPppWHsyOddhtfThmAEUajonsQ+iqMo//7DcAAABAe5SS1OqyScPE5adru1mHexu3bLo4VE4RJ9WTlkcVya1JZZg1UUmyoVuakpRmu+quJmxfQNfQiwuYDfsJAAAA0C4ktQrS1pOhcb21Jn1mlu/Lf28bl1PRDeqzLoNpE6lF/d40v92EJMOiywXNwXrspjbGe6BqZce/Ll5YBADAvPpygTIAAF1HUgszmaaCNymxNe331KXuJNZK31H0uOBNTCbM0gtsnp5mmE3RyWjWSXc1Oa4DfdfWC4uAsrFfAAAAAO1DUgtT9XgpundRkxpXimxkL3ue2pzAqmN9M0xi/VjW3dOU2A20XdXxcVD3alIdDJhWWfsL+wMAAADQPiS15tCVYc/KnodJ94Sq4ySyib2wylD39lnFcqERon51b2coD/sWUK664ufgdzmGAhn2AwDD6hr6HwAATI+kVs/MW0FbpPFjpeRWmY0qfUhgNSGpUNVy6Uoyua1Y9t3VxNgGoHzDcZ1YgKaiDgIAAAAgj6RWQZp6xWuRJ4H575pnXsclJYpYdmWc7DZpfVbVq27WcjRpGU2LhpHZscy6o437LNA1TY2pi9bzgLZhOwcAAADaiaTWnJrca2Tecg2f2E26J9a4z036jUUTW11OYDUleTWtpiZzZ9HkfRmYRdv3RaAP2nK8GZSTuIKuYtsGUIZRx3niDQDUq2nnYBwXirFH3QXAYmzv9phFRNz7GPXeLGWY5TdX+p5JjyLk57uuYFLWvOU1YT4BlIf9G0CZyqqfANNi+wMAYDq232P7JttfzL12kO1P2b4q/T2wzjICQFFIahWorJOuohM8szSCztJYWkRiqwzDiZ2qG3/LTM4NVHlfqyaZdTnSMIKmGxWvVnoAaI82H4OquBAHGFbmBV8AUJVpL97lWIsCvE/S8UOvnS7pwohYL+nC9BwAWo+k1gLKOCGqomfSIp+fpO7EVl2NvlVURCc1bPfhBL3L8zjLVVXOvN32NttfsP3Y+kqORZCkAqpTZ5ztYsMUjW4oW1nnWgAAdFFE/Iukbwy9vFHSlvT/FkknVVooACgJSa0aVdF7p+iG0mm+s6qTxSoTWFVfQdWmHhk0DhTmfZr+qqoTJK1Pj02SzqyojFhAm/ZroKPep5ribNcvQMnXh6gXoChl1bEBAOiR1RFxffr/Bkmr6ywMABSFpFbBhk++6uzBU7aVfmfa+Zt1qK2y5q+ubv9tHFKs6eVrqxmvqtoo6f2RuUTSAbYPqaakmKSN+zXQB3XH2Xxiq6tJLhJaKArbEgDMNnQhMElkFc+RlU/bm2xvtb214mIBwFxIapWgip5XTWooXTSxVYeqK4BNXXdovHFXVa2RtD033Y70GmrAfg20Wm1xtsvxggY2NFGX9zkAAMa4cXBhVvp706iJImJzRCxFxFKlpQOAOZHUmkNVyZA2NZQ2vXxSfeutzmVDg1J3rHRV1ThcbVW8Ju3fAIo1T5yVdo21O3funOd3OxtLqIdgHgw7CABAYc6XdGr6/1RJ59VYFgAoDEmtFdR5/6Q2nng1rcxV3a+s7eutCDRalWbcVVXXSVqbm+6w9NouuNpqPuzfQK8sFGelXWPtqlWr5i5Il4ckZHgkTKusbYRtD0DXjTvWcgzuD9sflPTvkh5ue4ftF0h6k6Sn275K0tPScwBoPZJaSVVjEk/dSLpunWQX91i3rpT5aYoi11/V9/MqUlVla/Iy6JhxV1WdL+m5zjxe0m254bMwo7bs3wBK0ag4O+q+W10zqKPRuIYqdXV/AgBgICKeFRGHRMT9IuKwiDgrIm6JiOMiYn1EPC0ihu8vC6Bis9wvcdFHl+1VdwGqVNXKLOSk6dprpSJPvjq0IZe5HjnhRV3SVVXHSjrY9g5Jr1N2FdU56QqrayWdnCa/QNKJkrZJ+pak51de4JZiHwf6q41xNiJk+96/XTGc2CI2QyLRCQAAAGA6nUxqtSp51VLTLuOmnZwOr7NRXfP7vF4XUdaym7QNzfK7TW4UjIhnjXnruBHThqTTyi1Re7EPAxilrXF2eFjCLie5JGJ4n5W1XbNNdcuGDRu0devut4vtUkwEFjVuf5h1PyF+AgCaqtVJLZJX1Wn7ScK4ddi1RqEqlbXsWB+YBfEZQN/kk1xdPGbSe6u/urg9AwAAACheK5JaJK+q0/aTyfxVzMg0ufcZ6wmzaOp2DAB16XK9h+RWv3RxGwYAAABQjsYktao8keHkuP0njqzD8Zp25XZZZWEb6B7WKQDMZ9Lwym2WH26R4wRmxTbTH+PW9agEeZdiJFCmSfsKMRYAUJdGJLWWl5dL+V4OsBnbCrW78r7oupylIYRGk/mVuY2xTrqHdQoA5WjaBS6LGsxLvnGa+lp3dGlbBQAAAFC+RiS1FsUJ7a7q7Bkz6rfrWD+zNOZ0reGnTF0cCpR1Xw3iNABUq+u9t9AdZdXFqXsAAAAA3dSapBYnJePZ1lMk3U/SPxXwfSxrMBwoZsV6BIDm69qFPMPzwrGonbq0TaJ5RsUFhiIEijFu/+F4DAAoWyOTWhwAJ/j2t/WTD3qQPifpIEnnSPplSTdJWj3jV7Gs+2HW4RfLwLbWHaxLAGifQewe/O1iQy5DEmKA7QAAgPls2LBBW7duXfh7uljXBNqmafthkXX0Ria1sLv8RvhCSZ+V9L+VJbMk6RpJvzjhO/p+clfkfbWafmVwXVdiN205YHGsUwDopq712hrI33cLzcewgwAAAABmRVKrwcad5P1s+jtIaL1M0jsk/SA3zW4ncx1stCjSpIadLjb6LILGgm5hfQJAP/Xlvlsc55qpS9sb2mtcfGD7BOa3yP7DMRsAMA2SWg2xW8+fFab9idz/n5R0xuAzHPx3segVyH09kRlebmxX3cL6BACMQ+8ttB3rGAAAAOi+PeouQNfZnuoxi1MlnSVph6Sf3bhREcEJ3IIWbcDpWgPQYJtiu+qODRs2sD4BABPl6wBdO27MW/dGe7BuAQAAgO6jp1aBqjiJ6lrjQhP05eSXm6cDAIBZDXpvda0XV773FnWk7mA9NpvtPSVtlXRdRDzT9pGSPiTpIZKWJT0nIu6us4wDDEsI1GOWfYyYDwD9NbGnlu21tj9j+wrbl9t+aXr9INufsn1V+ntget223257m+0v2H5s2TNRh0V7W82iq1fL1mHR9cS6AABgevSK6YZBvaeLdaDB9sm2Wq0ylnXXts2OeqmkK3PP3yzprRHxMEm3SnpBLaUCAABAq0wz/OA9kl4eEUdJeryk02wfJel0SRdGxHpJF6bnknSCpPXpsUnSmYWXumJVJrCGcXJWrzYPwdO28gIA2q+u+hKq1cZ60bTYbsvHMu4n24dJeoakd6fnlvRUSeemSbZIOqme0gEAAKBNJia1IuL6iPhc+v8OZVdWrZG0UVnFU9q1ArpR0vsjc4mkA2wfUnjJS1RWg8xwgmTUA8WaZpnmp2F9AACwsqLuEYr262pdie0aKMXbJL1S0g/S84dI+mZE3JOe71DWztBonM8DzTHuWM0xHAC6b6Z7atleJ+kxki6VtDoirk9v3SBpdfp/jaTtuY8NKqfXqyHKPrBRmW2fItYZ92MAAHQRDQJYyXDdp4vbS/7+W1hMF7cPTGb7mZJuiohl28fO8flNykaB0eGHH15w6QAAZSiq3kTdAeiOIvfnqZNatveV9BFJL4uI2/OFiIiwPVO0yldMi1R1sOPktr+6dsN0AEB/cPxCWUYlubpSZxo1D4N545xgsrLvQYxGe6Kkn7d9oqQHSHqwpDOUjeqyV+qtdZik60Z9OCI2S9osSUtLS41e2Ytsi12Ik0Bdxu0/s+xX+f13qM1z/oIBAEoxzT21ZPt+yhJaZ0fER9PLNw6GFUx/b0qvXydpbe7jIyunEbE5IpYiYmnewqffrqxrcZXDC1wjKSTJLuZxxBGllbVt+nyy0Od5H4cKKoCuY5hA1GlwnO3q8XawP7GfAeNFxKsj4rCIWCfpFEmfjohnS/qMpF9Kk50q6byaiggAAIAWmZjUSjdwPUvSlRHxltxb5yureEq7VkDPl/RcZx4v6bbcMIULqfJkse7xsY+U5PRQxOKPa66ptPxohkW32641ynRpXgBgHBrW0VR9u+cM+2A1+rRNddCrJP2u7W3K7rF1Vs3lAQAAQAtMM/zgEyU9R9J/2r4svfYaSW+SdI7tF0i6VtLJ6b0LJJ0oaZukb0l6/jwFq+IEsIknQF0ZnqVJqlymbR1+pq/bXBvXFQAM9DV2o/36Vt8dN2xhn5S5vtta/+6riLhI0kXp/6slHVNneZpk3Hbcp3gJ1KmIIQwBANWYmNSKiIuVOgyNcNyI6UPSaYsUqqwDBic7/UCFY7I+LqM+zjOA7iCGoWtG3XurT4bnl/OU+bHsAAAAgH6ZpqdW43FFU3/VvY7beKVx28q7qL7NL4BuIHahb8bdnL0vutzbqOwh4wEAAAD0S2uTWpzA9FNRJ8VdbjgYKLIBoa3La6Vl0Mb5AdAdfWy0B6Y1OEb3bT/Jzy/1FAADs8SDvsVNAADQT61IanFS1z9UxmdX1DJrY++zYZPKT0wBUKW2x1SgLn3uvcV9uCZjeQAAAAD91LikFicnu2trL5lZFd1YUde9GspeX0XOx7hytjmxRe8sAE3Q1hgKNFVfe2/lDea9TfWZPq8vAACwuKbVe6jbAM2wR90FkKQNGzZIKj9QtSXwNC1gV2HRdRMRuz2qUnYCa/hRhKqXURUmLZ+uzS+Aeo2Kz0XHagC762IdZlbEHOp1wDijzounfQAAALRFY3pqUYnCtLq6rZTRINGXq5oZbhBA2boeR4G2aXOv8jIML4su1326PG8AAAAAJmtMUqsMnOy2x7h1VXYvqDpOisvcJqedn64MaUkyC0AZqDsA7dDne25NMqjrdaXOl9fFeQIAAAAwvU4ntdAuXTw5bUICqyhNa0BgqEEAi6ABHOgWEly7GyyHrvXianv5gaYijgIAgLYgqYXeqLLnXtG/s+jJ+zzz3uSejiS0AMyqqfEMQPHywy83uT5Tl8Hy4H7GAAAAANqIpBYwoy71vmobklkApkFDKgDpvroBia3RqkpuFalNZQUAAABQDpJaLdG0od/6ggRWc5DQApBHAzWAWeR7b2FXZdzXtozlTH0PqM4s+1t+f2cIQwAAUIXeJbXakhziitJq5LeHviWw2rIvSCS0ANAwAqAYJLems8h9uMpatm2quwIAAAAoT++SWui3UcnCIk+8m3yiXUSitOrGBJJZAGh4bqfhGM16RNPQm2A2+fuT1VUHo+4HAADq1pQ2MaDv9qi7AEDRbK/4KEpE7PZAcUhoAf1UVsxG8UYdBzkeoo3YhqcziMmj4jTxGuivcbFz1tcBLMb2e2zfZPuLuddeb/s625elx4l1lhEAitL5pBYVpfaZlJSqKmmV19fGjrrmk4QW0C8ksdqjb8dB9Bfb+fSqSGyxLgAAmOh9ko4f8fpbI+Lo9Lig4jIBQCkYfhCla1sjJSfN9Zm0rbBugG5p2/Ghj4i76DuGKZxNGcuIOAQAwGQR8S+219VdDgCoQud7anVJ1SfSTeklVaa+9sAaaNI6m9Q7q2/rBuiaLhwzumilIQSJu8Cu2EcAYD7TDEXIUIVAaV5i+wtpeMID6y4MABSBpBbu1dXGRhrr7tPUeWa4QaCbunhMabu+HwfLtry8XHcRUDH2IwAA0GBnSnqopKMlXS/pz0ZNZHuT7a22t+7cubPK8gHAXHqZ1GpL41rZJ8ltSmJNupKcxFV1itxWJm17fVp/s97U1farbW+z/WXbP1tPqYHdteW40hccE+9DnEVZRu1nfd/fAABA/SLixoj4fkT8QNK7JB0zZrrNEbEUEUurVq2qtpAAMIdeJrX6qq4k1ryJJxrhmqGs5c9wg7t5n6a8qavtoySdIulR6TN/aXvPykoK5LTpAomu4sKOqb1PxFlUYDixxb44P5Yd0D+zDkVIvQcYz/Yhuae/IOmL46YFgDbZq+4CoDxFNS7OUjmkQRPToHfW7ma8qetGSR+KiO9K+qrtbcquuPr3kooH7IJYX6++xslFEWdRh+HElm1FBHEUAAAUyvYHJR0r6WDbOyS9TtKxto+WFJKukfSi2goIAAUiqdURRZ4Y19FYxsl9fQaNK1X91jg00o71EtvPlbRV0ssj4lZJayRdkptmR3oNKAwxuRrEvkYoNM5WeVxF+0zqvUXsBYDJVhrtZVrEW3RNRDxrxMtnVV4QFKbKcwpiItqmF8MPdq1hYXiop0UCD0MV9U9d65iE1lymuqnrONzsFfNgGMFyccxtnIXirLRrrM29VlwJ0SvEhV2xLwEAAHTDakk/Unch0Bm9SGq11aiTuEVP7GhMQxFm3Q5JaM1nhZu6XidpbW7Sw9Jrw5/nZq+YCvfEKg/H3WZbNM6m77g31pZbWvTFSvfI62Mc4d6NAAAA7bafpMsk/V9lVxMCiyKp1RBF9r7KozENdVppW2abnGyFm7qeL+kU2/e3faSk9ZL+o+ryoZ3KOt701bjGZ2JcO5QZZ9m3ULQ+J7by2LcAFIH6GgAUz8pOnB4+9Fgn6VuSHiTpX0RiC4vjnlo1KetkrO4KGffGap5p1sek+3/Ms17pnTWbWW7qGhGX2z5H0hWS7pF0WkR8v45yox2Iy8UhfrUXcRZdMohFgzpc3+L8YH6JyQAAAM3xZEmfljTuxOn7ynptXSTpgIrKhG7qbVKrjpt4F3myyQkcxmlCowYJrdnNelPXiHijpDeWVyJ0QRPiQdsRs7qDOIsuyvfeyie4+pLoIrkFAADQHLdK+oakPUe890BJ95d0t6RrqywUOqm3Sa0qlHEiyQkbhjWpwYJkFlCfJsWCNiE2oUw0uKNKw8MT5ntzdR37GoBFFBU78vG2TzEYAAa+IGnUHd33l3S1pL0lbZP0lCoLhU4iqTVB1RWQ4coUFaDyjap4Nl0R20XR80pCC6gex4jZEY9QNbY51K1PvbdIbgEAADTPnZKWJR0q6UmSvllvcdABJLVUf6Ng10+66hjqcVw5ppmmCWUdVtQ2WuS85ZcVCS2genUfu9qCGIS6NbVugX4Z7r2V18XjCfsdAABAc3xf0s8oG5aQGxWjCL1Jao26MrGuEzhOsKYz78loF07Mm9gTa559iG0dKEYX4lqZiDVoOhrY0WRd7cFFry0AVRsVb/KvdTHWAsAsSGihKJ1MajWtolBW75i+a9p6XkQTk1iLaFJZgLbqUowrA3EGAIozagj0riS7OH8CAABYWVvrSl2oq2I+e0w7oe09bX/e9sfT8yNtX2p7m+0P2947vX7/9Hxben9dOUXfpWy7PJogIu59LPo9bVPGvZpGPdqsiHnJb2NN2k6aVBagbboS48rSxJgHAF00PFxh2+Mux1YAAACgO6ZOakl6qaQrc8/fLOmtEfEwSbdKekF6/QWSbk2vvzVNV4iykxvDSYJFHphdVWP8T1pXRf9mUdts07exppYLaLIuJeuLxrEVXcM+jrbKJ7banuTimAugTtO0H1H3BQBgsqmSWrYPk/QMSe9Ozy3pqZLOTZNskXRS+n9jeq70/nGe46yhqp45VBi6qY6EY9FJ1yY15k6al7rLB7QJDWora0LMA8rAdo2u6EovLo7FAAAAQDtNe0+tt0l6paT90vOHSPpmRNyTnu+QtCb9v0bSdkmKiHts35amv3ncly8vL5fWIyePk5ZuKmKIx1m2jTITrE1EMgtYHMefyYgn6LpBHGBbR5eMS2y16bjHvgkAAAC0y8Sklu1nSropIpZtH1vUD9veJGlTUd83zUkINzuuTxFlrmOey9xe2rAOSWgB0+nCsaVKxA8A6LY2nnfly8txCkCVxsWc/OvjYtSoxHzb4i8AALOapqfWEyX9vO0TJT1A0oMlnSHpANt7pd5ah0m6Lk1/naS1knbY3kvS/pJuGf7SiNgsabMk2Z7prKFvJxltPCksSl3ruujl3bZtlmQWsKu+xuCiET8AoD/afA5D7y0AAACguSbeUysiXh0Rh0XEOkmnSPp0RDxb0mck/VKa7FRJ56X/z0/Pld7/dCxwNsCN4tupqPVU5olwWfe1afs2S0ILfTXuvnhtbZBrirbGQgDA4toe+6kDAAAAAM0z7T21RnmVpA/Z/iNJn5d0Vnr9LEkfsL1N0jeUJcKmUtfwcm0/2eqSoq/o7PvwgbMioYWuoBGqHsQJYDLqnuibNt9vS9q11xb7L4C6TDNE4aRp2xZ/AQAYZ6akVkRcJOmi9P/Vko4ZMc13JP3yLN+7YcMGbd26dZaPAPcqu2LWhxNXklloG07ImoMYAQCYRVuHJWxjmQEAALqsyvYI6oLNskhPrVZq60nUsD5dJZif1yrXXZ+W7zh9WQZoni7E6S4jNgCLGdRthus49ARBX9RRty8K99sCAAAA6tW7pBbaqYphBNt4Ur0oElqoSh/3ry4hHgDFG8TFfHzMv8Z+hz5ocz2c5BaAtsnHq3zcHfc6AABNRVKrJbrSw6xus5x0dr1BiYQWykCc6hZiAVCf4XhKLy50WZsbVNkvAQAAgGqR1ELnzHtS2afEIQktFKUv+0yfEAOAZhrVs4v9FV3Uxt5b9NoCAAAAqkNSS1xd10ajElCsw8lIZmEebWpUwuzY94H2yjekU59F1+S367bURdgPAbTFuFhVRAxrS8wGALQXSa0W6+pJ07gT10nz2tXlURQSWpgVJyPdxT4PdMu4XlzUjdB2g+23Tb23uPAOAAAAKFcvk1ptutovr63lRv1IaGEaxJduY18H+mU40UUMQBe08XyIfRAAAAAo1h51FwD9McsJ6DQnfVWcGLbtpHmUcfMQEZxc98jy8rJsr/hA+w3261EPAP1G/EdXtPX41vd90PYBts+1/SXbV9p+gu2DbH/K9lXp74F1lxPA4toYowEA7UJSC8jpUqVrpZPkLs0n0HecNAJYVB8b2NEdXTj+9WS/O0PSP0TEIyQ9WtKVkk6XdGFErJd0YXoOAADQOCtdRFzHo+9IaqE1enKyVwiGGwS6i4oMgLKR4ELbdOF4mN/nurbv2d5f0pMlnSVJEXF3RHxT0kZJW9JkWySdVE8JAZRl3kZazncAACshqZW09cShyeWm8lEPElpA93BSB6Au9OJCW3TlWJnfzzq0zx0paaek99r+vO13295H0uqIuD5Nc4Ok1bWVEAAAAK1BUguNNc8JaRknfm06meT+WUC3sO8CaBoSXGiDLhw/8722OtCDay9Jj5V0ZkQ8RtJdGhpqMLIVNnKl2d5ke6vtrTt37iy9sAAAAGi23ia12nqS09ZyF6XFJ3Kl4v5ZQHd05UpzAN033IuLZBeapkvH0pYntnZI2hERl6bn5ypLct1o+xBJSn9vGvXhiNgcEUsRsbRq1apKCgygHqPOg8adH3HeBAD91dukFrqhjMpL2ypEDDcIdEtLG6sAAGikfINnV+rGbUsgR8QNkrbbfnh66ThJV0g6X9Kp6bVTJZ1XQ/EAAADQMnvVXQBgJRHRqhO2qpHQAgAATTNcPxnU5/J/gaoNJ7a6cI7Rsv3ptySdbXtvSVdLer6yi2zPsf0CSddKOrnG8gEAAKAlSGoBLcVwgwAAoA2Gh01rWUM8OqorF8+1ZR4i4jJJSyPeOq7qsgBor3H1h3Gv52Nkfpq2xE4AwGgMP5jT1oNaW8s9r77N7ygktID65MduZwx3AJhPfvi0lt8rCC3WxaEJAQAAgK4jqYXGm/UEs4wGkSY1spDQAso1KmlFAgsAyjEusdWkuhe6j8QWAKDtbK+1/RnbV9i+3PZL0+sH2f6U7avS3wPrLiuAxU1quyry0US9Tmo1daVgNmWsx6ZuGyS0gNl05WANAH2ST2yR3ELVqA8AQHeMO89b6bUWHwPukfTyiDhK0uMlnWb7KEmnS7owItZLujA9B4BW63VSq61afIAtrGGijw0cJLSA0UhQAUC3DZJb9OJClahHAADaJCKuj4jPpf/vkHSlpDWSNkrakibbIumkekoIAMXZq+4CtMW0J8+c/KAMJLTQZ2znAICBfGKL4wPKNtjGSKQCANrE9jpJj5F0qaTVEXF9eusGSatrKhYAFIak1giLnLQMPstJdiYiCjkJLOp7FlFX4wkJLXQJ2+10aKwF+qkJ9Z02yS8rYibKRHILALppXP1h3OttOA7Y3lfSRyS9LCJuH6ovhe3dZs72JkmbJOnwww+vqqgAMDeGHxxS5PB43AegXF1fxivNGw03aIM+DQfY5XkDUI1BHOlDzCxDl+uEaA72SwBAk9m+n7KE1tkR8dH08o22D0nvHyLppuHPRcTmiFiKiKVVq1ZVV2AAmFOvklr5JEiVJ75V/A4n8bsqqndYHSZtm5xMo6n6ksACgKoQV2dHnRhla+o+uWHDhrqLAACokbNK0FmSroyIt+TeOl/Sqen/UyWdV3XZAKBonUxqjUpeFZXkWOkxqUxYTNNOHK0MmYUAACAASURBVIs2zXba9WWA5ps3BgIAxps2hhJ7p0OvLVSlKftgU8oBAKjVEyU9R9JTbV+WHidKepOkp9u+StLT0nMAaLXW31OryhPWSffLmjTWOvdIwSjTbMNsN91me62k9yu7YWtI2hwRZ9g+SNKHJa2TdI2kkyPi1nQF1hmSTpT0LUnPi4jPlVU+tj8AXdDkWLtIHZF7/YzHPbdQheFti30RAFCHiLhY0riD0HFVlgUAytaanlpl9b5atCyjrHTlbJG9xvpoUm+5sq9cLnKbm7ZnVl/Xdc/cI+nlEXGUpMdLOs32UZJOl3RhRKyXdGF6LkknSFqfHpsknVl0gegFAKCDGhdr8xatY4yrCxHHM/l6F0kHlKnqfY99HAAAAGWapT1+0ce0GpvUKjt5VVQyZFJyCxg27TbN9tMfEXH94Or/iLhD0pWS1kjaKGlLmmyLpJPS/xslvT8yl0g6YHDj1xl/l6EEG4xGV6BYdcXaGcpX2ncS1++TT2wRZ1GW/L7H/gcAAAAUqxFJreXl5dKSWPM02M7TsDuu3Cv12ELzFXESOuu2zclvv9leJ+kxki6VtDoirk9v3aBsyCwpa4TdnvvYjvTaikhclYtlCrRHmbG2qejBNRp1clShrLoX+zEAAAD6qPX31Booq0Kf/95pTnpH3RNh8Hz485Pu0YX2mbdhhG0AkmR7X0kfkfSyiLh96F4gYXumDcX2JmVDZunwww8vsqgA0Fplxtq2GJfY6muCZ3i+qZehLLOeW077XQAAAECfNKKn1qzqGipr2t9jOMJuG3d/NxJaWITt+ylrZD07Ij6aXr5xMNRV+ntTev06SWtzHz8svbaLiNgcEUsRsbRq1aryCg8ALVF2rC2v5NWgTpLpa3IP1aL3PAAAADCfqZJatg+wfa7tL9m+0vYTbB9k+1O2r0p/D0zT2vbbbW+z/QXbj523cE2+18uk8sxyMlzUiTMn4MUo+35uEiex2JWzDe0sSVdGxFtyb50v6dT0/6mSzsu9/twUbx8v6bbc0FkAgBGaHmubUo9bqf7dp3oL99xClWY93+3TvggAAAAMm7an1hmS/iEiHiHp0cpurH26pAsjYr2kC9NzSTpB0vr02CTpzElfvmHDhsYmr6YxS2KLe2w1Q9G9rSZp67aNyjxR0nMkPdX2ZelxoqQ3SXq67askPS09l6QLJF0taZukd0n6zRrKjApwbAAK1ehY2/T6QR8TWxLJLdRjpf2sb/sgAAAAMGziPbVs7y/pyZKeJ0kRcbeku21vlHRsmmyLpIskvUrSRknvj6y2fUnq5XVI13sRRMTIE95R981aaVpOUspRdWME6xGziIiLJY3bSI8bMX1IOq3UQmEu4+I7gPo1Pda2qR64Ujm7GgNnuVgNKALbFwAAADDaND21jpS0U9J7bX/e9rtt7yNpdS5RdYOk1en/NZK25z6/I73WebOceNBjqzx19b6iJxYAAOi7PtWH8vVM6vAAAAAAUI1pklp7SXqspDMj4jGS7tJ9Qw1KuvdK1pnOXG1vsr3V9tadO3fO8tFGq3Iowj40FkxSZgJrpaQVyx4AAGBlfbknF4ktAAAAAKjONEmtHZJ2RMSl6fm5ypJcN9o+RJLS35vS+9dJWpv7/GHptV1ExOaIWIqIpVWrVs1b/kaaJenR1ZP7MpSRwCJpBQAAUL58YqvLdS3uwQUAAAAA5ZqY1IqIGyRtt/3w9NJxkq6QdL6kU9Nrp0o6L/1/vqTnOvN4Sbd1/X5aixp1Ut/lk+FJ8zYqedXl5QEAwP9r7+5jJavrPI+/P3YDihpBZF3sRulVdgyasdEOi3FiXPABHSOaMC7GdViHXdxEx4d1MyNOsuqsJmtWZZzMDAkKAxpWZFtcjevMDqsk7vwhTjcgj8Pa4hMdFJQHdcziNH73j3MKiuu9t+/te6vOQ71fyU1XnTr39rfO79SvTp1P/X5HWkRjHsXl8askSZIkzcbWNa73+8DlSQ4H7gDeTBOIXZnkXOB7wOvbdb8MvArYB/yiXVesfgHwqvq1D75DumD4alZ6bn3gRb8lSVJfjOXYb72WC7b6cqy4UZM2XdS2lSRJkqTNtqZQq6puAHYt89Dpy6xbwFs3WNcoLBfmrJcfgA9uM7bztOm/5baXJEnz4nHHI5ZuiyGHXEuvuWU7S5IkSdKhW+tILc3BSuHMej4A9yUEm+WJh+We36ymcPTkg6Su9aVflzR7vt5XNtkuQw63JvwClSRJkiQdOkOtnllt1FGfA5ZZnWDYyHNd7XfXW68nmSSt1WaPHpW0ODzWOLix9bF9Pr6XJEmSpD4y1JqztYQjB/uwPu8Pv/M4cTDvD/JjmtJGkiRpkYxp1NaE4ZYkSZIkrY2h1owd6rdJ1/JhfVbXkZqHvn1gN+SSJEld6tux0RAYbkmSJEnS4jHU6rlDCcX69MF+qB/Il9vuTkEoSZJmweOLjZkOt8YyPaHhliRJkiQtz1BrAIbyLdSxfegey0kRSZLUb35xZnNMtuGYjuGmn4f7iCRJkiQZanXiUE9cTP9OFx/U/SAtSd3whLc0br6+N9+YR2+B+4wkSZKkxdWLUGvv3r0/T3J713UcxFOAH2/WH5vBB+tNrW85G6x55vVtgoPW2PEJkVFsw46tVN8z5l2IJEmavTGO3oJHT0/oFx8kSZIkLZJehFrA7VW1q+siVpNkT59rtL6N63uNfa8P+l9j3+uTJC0ew4j5GcqU3usxeS5eg0uSJEnSonhM1wVIkiRJi2pMActQVNVowx/3J0mSJEljZ6glSZIkdWSs4coQTE9NOCZJfm0ElyRJkiSNRV9CrYu6LmAN+l6j9W1c32vse33Q/xr7Xp+0Ik9MStLmmw62xhhuTf8rSZIkSWPQi1Crqnp/ornvNVrfxvW9xr7XB/2vse/1aTzGdmJUkhbF2EdvTY/ikiRJkqQh6kWoJUmSJEl9sHT01tgCrgnDLUmSJElD1HmoleSMJLcn2ZfkPR3VcHySa5LcmuSWJO9olz85ydVJvtX+e3S7PEn+tK35xiTPn1OdW5Jcn+RL7f0dSa5t6/hsksPb5Ue09/e1j58wp/qOSrI7yd8nuS3JC/u0DZO8q23fm5N8Jslju96GSS5JcneSm6eWrXubJTmnXf9bSc6ZcX3/tW3jG5N8PslRU4+d39Z3e5JXTC2f2et8uRqnHnt3kkrylPb+3LehJEkavrEFW5mamtBwS5IkSdKQdBpqJdkC/DnwSuAk4A1JTuqglAPAu6vqJOBU4K1tHe8BvlJVJwJfae/T1nti+3MecOGc6nwHcNvU/Q8DF1TVs4D7gHPb5ecC97XLL2jXm4ePA39dVc8GntfW2ottmGQb8HZgV1U9F9gCnE332/BS4Iwly9a1zZI8GXgf8C+AU4D3TYKwGdV3NfDcqvpN4P8C57d1nESzTZ/T/s5ftEHsrF/ny9VIkuOBlwPfn1rcxTaUJGlZYwtKxs5RW5IkSZLUva5Hap0C7KuqO6rql8AVwJnzLqKq7qqq69rbP6MJY7a1tVzWrnYZ8Nr29pnAp6rxdeCoJMfNssYk24HfBj7Z3g9wGrB7hfomde8GTs+MP6kmeRLwYuBigKr6ZVXdT4+2IbAVeFySrcCRwF10vA2r6mvAvUsWr3ebvQK4uqrurar7aEKnXwt5Nqu+qvqbqjrQ3v06sH2qviuq6sGq+g6wj+Y1PtPX+QrbEJow8g+A6bNPc9+GkiQtZ6zhyCIYa9tNj9oy5JIkSZLUV12HWtuAH0zdv7Nd1pl2mrmTgWuBp1bVXe1DPwSe2t7uou4/oTlB/6v2/jHA/VPhwnQND9fXPv5Au/4s7QDuAf4yzRSJn0zyeHqyDatqP/ARmlE7d9Fsk730axtOrHebdfk6+j3gr9rbvakvyZnA/qr65pKHelOjJEkarulRW2MLuZyaUJIkSVKfdR1q9UqSJwCfA95ZVT+dfqyaT6udfGJN8mrg7qra28X/v0ZbgecDF1bVycA/8Mi0eUDn2/BomlE6O4CnAY9nACNxutxmB5Pkj2im7ry861qmJTkSeC/wn7quRZIkjdt0sDW2cGtiEm4ZcEmSJEnqg65Drf3A8VP3t7fL5i7JYTSB1uVVdVW7+EeTKfHaf+9ul8+77hcBr0nyXZqp206juX7VUe1UektreLi+9vEnAT+ZYX3QjGy5s6qube/vpgm5+rINXwp8p6ruqap/BK6i2a592oYT691mc38dJfk3wKuBN9YjZ3D6Ut8zacLLb7avme3AdUn+aY9qlA6JJxSlcRhr+KFHwq3pnzGZDrgMuiRJkiR1oetQ6++AE5PsSHI4cDbwxXkX0V4r6WLgtqr62NRDXwTOaW+fA3xhavnvpnEq8MDUdHGbrqrOr6rtVXUCzTb6alW9EbgGOGuF+iZ1n9WuP9NP1FX1Q+AHSX6jXXQ6cCs92YY00w6emuTItr0n9fVmG05Z7zb7X8DLkxzdjkh7ebtsJpKcQTMV5muq6hdL6j47yRFJdgAnAt9gzq/zqrqpqv5JVZ3QvmbuBJ7f7qO92IaSpHF6wQtecNB1xhZy6ODGGnBNGG5JkiRJmqetB19ldqrqQJK30Zw83gJcUlW3dFDKi4A3ATcluaFd9l7gvwBXJjkX+B7w+vaxLwOvAvYBvwDePN9yH/aHwBVJPghcTxPM0f776ST7gHtpQoR5+H3g8ja4uINmuzyGHmzDqro2yW7gOpop864HLgL+Jx1uwySfAV4CPCXJncD7WOd+V1X3JvnPNOERwB9X1b0zrO984Ajg6vYExter6t9X1S1JrqQJCw8Ab62qh9q/M7PX+XI1VtXFK6w+922oxVVVnuSTFtDBXvtJRhtu6OAmbT/G9wf3bUmSJEnzED94SNL47dq1q/bs2dN1GQtnFictfd/WECXZW1W7uq5j1pb2tcv1Ab6GNTHGYGsp9/f52bVrF3v27Bn9TuUxraQu9fWYNsnxwKeAp9Jcl/2iqvp4kvcD/w64p131vVX15dX+lv2spC6ttZ/tdKSWJEmSNFaO2NRqlgY+7iuSJOkQHQDeXVXXJXkisDfJ1e1jF1TVRzqsTZI2naGWJEkD4vRO0rBMTzfna1cHM7YgdOlz8TUgSdLma68Vfld7+2dJbgO2dVuVJM3OY7ouQJIkSRo7T+brYCb7SFU96vaYTEKuMQV3Wpsk70pyS5Kbk3wmyWOT7EhybZJ9ST7bXptZkrQBSU4ATgaubRe9LcmNSS5JcnRnhUnSJjLUkiRpRsZ2MlKSND+LEGwZbi2GJNuAtwO7quq5wBbgbODDNNNiPQu4Dzi3uyolafiSPAH4HPDOqvopcCHwTGAnzUiuj67we+cl2ZNkzz333LPcKpLUK4ZakiRJktRj06O3xsZwa2FsBR6XZCtwJM3J1dOA3e3jlwGv7ag2SRq8JIfRBFqXV9VVAFX1o6p6qKp+BXwCOGW5362qi6pqV1XtOvbYY+dXtCQdIkMtSZIkSRqAsYdbGqeq2g98BPg+TZj1ALAXuL+qDrSr3YnXf5GkQ5LmTfRi4Laq+tjU8uOmVnsdcPO8a5OkWdjadQGSJEmSpLWbDrbGFAYtfS5jDfAWTXsNlzOBHcD9wH8HzljH758HnAfw9Kc/fRYlStLQvQh4E3BTkhvaZe8F3pBkJ1DAd4G3dFOeJG0uQy1JkgYmiSf6JEnArwc/Ywu5fL8bhZcC36mqewCSXEVzAvaoJFvb0Vrbgf3L/XJVXQRcBLBr1y53CElaoqr+FljuAODL865FkubB6QclSZohT8ZJkuZp8r4zlvefyTW3vPbWoH0fODXJke0UWacDtwLXAGe165wDfKGj+iRJkjQghlqSJEmSNCLTwdZYwq2JSbBlwDUcVXUtsBu4DriJ5jzERcAfAv8hyT7gGJrrwUiSJEmrMtSSpI4lOT7JNUluTXJLkne0y9+fZH+SG9qfV039zvlJ9iW5PckruqtekobBvlaLbMzBluHWMFTV+6rq2VX13Kp6U1U9WFV3VNUpVfWsqvqdqnqw6zolSZLUf15TS5K6dwB4d1Vdl+SJwN4kV7ePXVBVH5leOclJwNnAc4CnAf87yT+vqofmWrUkDYt9rRbaJNgyBJIkSZI0ZI7UkqSOVdVdVXVde/tnwG3AtlV+5UzgivYbrt8B9gGnzL5SSRou+1qpMdYpCQ3rJEmSpMVgqCVJPZLkBOBk4Np20duS3JjkkiRHt8u2AT+Y+rU7Wf3ErCRpin2t9Ei4NaaQaxJuGXJJkiRJ42WoJUk9keQJwOeAd1bVT4ELgWcCO4G7gI+u8++dl2RPkj333HPPptcrSUNkXystbxJsjSXggkdfe0uSJEnSOBhqSVIPJDmM5iTr5VV1FUBV/aiqHqqqXwGf4JFpr/YDx0/9+vZ22aNU1UVVtauqdh177LGzfQKSNAD2tdLqpoOtsYRb08GW4ZYkSZI0fIZaktSxNGdYLgZuq6qPTS0/bmq11wE3t7e/CJyd5IgkO4ATgW/Mq171gyfmpPWxr5XWbyzB1jRHb0mSJEnDtrXrAiRJvAh4E3BTkhvaZe8F3pBkJ1DAd4G3AFTVLUmuBG4FDgBvraqH5l611qyqPHkmdc++VjoEk2BrTO9jS4OtMYZ3kiRJ0lgZaklSx6rqb4HlzhR9eZXf+RDwoZkVJUkjY18rbczS4GdsIZfBliRJkjQMTj8oSZIkSVqXsYVAXnNLkiRJGgZHakmSJEmS1m2Mo7emn8PYgjtJkiRpDBypJUmSJEnasEkINJYwaHr01hgCO0mSJGkMDLUkSZIkSZtibMEWYLAlSZIk9YihliRJczCmk3uSJK1FVT38MwZjeR6SJEnSkHlNLUmSJEnSTE0HQkMc8WSgJUmSJPWDI7UkSZIkSXMzptFbkiRJkubLkVqSJA1UEk8KSpIGa+l7WF9HcPleK0mSJPWHI7UkSZoTT4pJkrSyyftkn94v+1SLJEmSJEMtSZIkSVJPTAdbBkqSJEmSljLUkiRJkiT1UpfBlqGaJEmS1D+GWpIkSZKk3upi1JaBliRJktRPhlqSJEmSpN7r4zW3JEmSJM2XoZYkSZIkaRDmcc0tQzNJkiSpvwy1JEmSJEmD1MXUhJIkSZK6Y6glSZIkSRqsJJv2twzIJEmSpH7b2nUBkiTp0CXxBJykzq0UKtg/aUjcXyVJi27v3r0H/bKI75eSuuZILUmS5sgPAJIWSZKHT4xs5mgaSZIkSdJiMtSSJEmStCEHC+yngy3DLfWRXzqRJEmShsFQS5IkSdKGVdWagwGDLfWJgZYkSZI0HIZakiRJkubOUVuSJEmSpPUy1JIkaeA8KSypT9YzYgseCbfsy9QFR2lJkiRJw7K16wIkSZIkjc8kLFhPWLV0XQMHHYxhqCRJkrRYHKklSdKceZJW0iLZSJ83CSwMLrSSjexfvh8Pz969ex3ZKUmStOAMtSRJkiTN1HqnJJw2HWx5IltLHeo+YaAlSZIkDZPTD0qSJEmai6racDCVxEBCgCP4JEmaSPJY4GvAETTne3dX1fuS7ACuAI4B9gJvqqpfbvD/2mi5a+Yxn6TlOFJLkiRJ0txsZNTWhGGGNsITZOMwGb1pfyBJADwInFZVzwN2AmckORX4MHBBVT0LuA84t8MaJWlTGGpJkjQCntCRNDQbDbc8ob3YnHZQkqRHVOPn7d3D2p8CTgN2t8svA17bQXmStKkMtSRJ6oAn1SSpsRn94XTAZci1GHwflSTp0ZJsSXIDcDdwNfBt4P6qOtCuciewrav6JGmzeE0tSZIkSZ2aBBSbFUhN/x3Dj3E6lH3FfWG8VtofbHNJi6SqHgJ2JjkK+Dzw7LX8XpLzgPNmWZskbSZDLUmSJEm9MH0CehYB19L/Q4vDdpckLYqquj/JNcALgaOSbG1Ha20H9i+z/kXARQBJfMOU1HtOPyhJkiSpdzZ6za2VTEIupykcrvW2nYHW4lo6NalTlUoaqyTHtiO0SPI44GXAbcA1wFntaucAX+imQknaPI7UkiRJktRbVbXpJ5+XBluT/8PwQ5IkDdRxwGVJttAMYriyqr6U5FbgiiQfBK4HLu6ySEnaDIZakiRJknpts6+5tdR0yGWwNS62pyRpEVTVjcDJyyy/Azhl/hVJ0uwYakmSNBKejJU0drMYtbWUo7fGw7bTwazUn6x0fT/3KUmar7Uc99k3S4vHa2pJkiRJGoxZXWtrKa+300+2iyRJkrTYDLUkSeqI3yiTpEM3rz40iUFKT6ynHXyPlSRJksbJ6QclSZIkDdI8piOcmExF6JSE3Zlne2uxrbSfrWW6QkmSJM2WI7UkSZIkDda8piOER05oT4/eMmSZn7VuawMGSZIkabwMtSSpY0kem+QbSb6Z5JYkH2iX70hybZJ9ST6b5PB2+RHt/X3t4yd0Wb8kDYF97fh1EWQYbPWPgZYkSZI0boZaktS9B4HTqup5wE7gjCSnAh8GLqiqZwH3Aee2658L3Ncuv6BdTwI8sSqtwr52Acxz1NZSjt6SFpfX3pMkSZofQy1J6lg1ft7ePaz9KeA0YHe7/DLgte3tM9v7tI+fHj9FS9Kq7GsXS9fh1uRfd5n5cpSWJEmSNH6GWpLUA0m2JLkBuBu4Gvg2cH9VHWhXuRPY1t7eBvwAoH38AeCY+VaszeIJOGl+7GsXTx/6WIOtzXGw7diHtpYkSZI0e1u7LkCSBFX1ELAzyVHA54Fnb/RvJjkPOK+9+/MkPwF+vNG/26GnYP1rMqMTqG7/bg29/mckOa+qLuqyiDn0tQ8muXmjf7NDQ9/Pelv/Ovrl3j6HNeqs/k167xvy9u9FPzsHPwa+R0/bapOPwXr5HGfA5zkei/Acn9F1AXMw6Wen9bptV+h7e13zCqx5Pqx5fg6l7jX1s4ZaktQjVXV/kmuAFwJHJdnajhDYDuxvV9sPHA/cmWQr8CTgJ8v8rYuAh09sJNlTVbtm/Rxmxfq7Zf3dGnr90DwHpvqkLs2qrx16O1l/94b+HKy/W33qZ2elqo6F4bfVWizCcwSf55gswnNcBJN+dtoQ29aa58Oa52OINcNs63b6QUnqWJJj21EDJHkc8DLgNuAa4Kx2tXOAL7S3v9jep338q+WcO5K0KvtaSZIkSZKGz5FaktS944DLkmyh+bLBlVX1pSS3Alck+SBwPXBxu/7FwKeT7APuBc7uomhJGhj7WkmSJEmSBs5QS5I6VlU3Aicvs/wO4JRllv8/4HcO4b8a+nQ01t8t6+/W0OuHjp/DnPraobeT9Xdv6M/B+rs19PrXYxGe6yI8R/B5jskiPMdFNcS2teb5sOb5GGLNMMO64ywqkiRJkiRJkiRJ6juvqSVJkiRJkiRJkqTeM9SSpJFLckaS25PsS/KerutZqyTfTXJTkhuS7GmXPTnJ1Um+1f57dNd1TiS5JMndSW6eWrZsvWn8adsmNyZ5fneVP1zrcvW/P8n+tg1uSPKqqcfOb+u/Pckruqn6EUmOT3JNkluT3JLkHe3yQbTBKvUPog2SPDbJN5J8s63/A+3yHUmubev8bJLD2+VHtPf3tY+f0GX9m2GIfa397HzZz9rPboT9bGOIfe1arPf1NWRJtiS5PsmX2vvL7sNDluSoJLuT/H2S25K8cKRt+a52f705yWfafmp07bnIhtrnZplj3L5Zz3FtX6z3WLYPhvj+eijHrF1b73HqZjDUkqQRS7IF+HPglcBJwBuSnNRtVevyL6tqZ1Xtau+/B/hKVZ0IfKW93xeXAmcsWbZSva8ETmx/zgMunFONq7mUX68f4IK2DXZW1ZcB2n3obOA57e/8RbuvdekA8O6qOgk4FXhrW+dQ2mCl+mEYbfAgcFpVPQ/YCZyR5FTgwzT1Pwu4Dzi3Xf9c4L52+QXteoM18L7WfnZ+LsV+tkv2swM38L72YNb7+hqydwC3Td1faR8eso8Df11VzwaeR/N8R9WWSbYBbwd2VdVzgS00feYY23MhjaDPXXqM2zeXsvbj2r64lDUey/bIEN9f13XM2hPrPU7dMEMtSRq3U4B9VXVHVf0SuAI4s+OaNuJM4LL29mXAazus5VGq6mvAvUsWr1TvmcCnqvF14Kgkx82n0uWtUP9KzgSuqKoHq+o7wD6afa0zVXVXVV3X3v4ZzcmDbQykDVapfyW9aoN2O/68vXtY+1PAacDudvnS7T9pl93A6Ukyp3JnYUx9rf3sjNjP2s9uhP0sMK6+9lEO4fU1SEm2A78NfLK9H1behwcpyZOAFwMXA1TVL6vqfkbWlq2twOOSbAWOBO5iZO254Ebb5/bBOo9re2Gdx7K9MMT310M4Zu3cIRynbpihliSN2zbgB1P376Tnb4ZTCvibJHuTnNcue2pV3dXe/iHw1G5KW7OV6h1Su7wtzbRRl0wNye91/WmmWDoZuJYBtsGS+mEgbZBmOqEbgLuBq4FvA/dX1YF2lekaH66/ffwB4Jj5Vrypetcea2Q/2w+DeI1Ps5/txoL3s9DDNpmFNb6+hupPgD8AftXeP4aV9+Gh2gHcA/xlmmkWP5nk8YysLatqP/AR4Ps0YdYDwF7G156LbMh97nLHuEMw1H5iueOo3hni++saj1l7YZ3HqRtmqCVJ6qvfqqrn00x38NYkL55+sKqK5mB1EIZWb+tC4Jk0w8fvAj7abTkHl+QJwOeAd1bVT6cfG0IbLFP/YNqgqh6qqp3Adppvdj6745J0cPaz3RvMa3zCfrY79rPjN/TX12qSvBq4u6r2dl3LjG0Fng9cWFUnA//Akmmtht6WAO3J1DNpQrynAY9n+WnJpC6seow7BAPqJwZxHDXE99ehHbPO+zjVUEuSxm0/cPzU/e3tst5rv/1HVd0NfJ7mTfFHk6mL2n/v7q7CNVmp3kG0S1X9qD0w+RXwCR6ZdqmX9Sc5jOag7/KquqpdPJg2WK7+obUBQDvFzjXAC2mmG9vaPjRd48P1t48/CfjJnEvdTL1tj9XYz3ZvFVwzjAAAAwJJREFUaK9x+9nu2wAWtp+FHrfJZljn62uIXgS8Jsl3aaYxO43m2lMr7cNDdSdwZ1VNvlW/mybkGlNbArwU+E5V3VNV/whcRdPGY2vPRTbYPneFY9whGFw/scpxVG8M8f11ncesvbLG49QNM9SSpHH7O+DEJDuSHE5z8d4vdlzTQSV5fJInTm4DLwdupqn9nHa1c4AvdFPhmq1U7xeB303jVOCBqaHvvbHk2ievo2kDaOo/O8kRSXYAJwLfmHd909prMlwM3FZVH5t6aBBtsFL9Q2mDJMcmOaq9/TjgZTRzf18DnNWutnT7T9rlLOCr7Tfkhmpwfa39bD8M5TUO9rPYz/bB4PratTqE19fgVNX5VbW9qk6gabuvVtUbWXkfHqSq+iHwgyS/0S46HbiVEbVl6/vAqUmObPffyfMcVXsuuEH2uasc4w7B4PqJVY6jemGI76+HcMzauUM4Tt34/zn841pJ0mqSvIpm/votwCVV9aGOSzqoJP+M5htV0Ezh8d+q6kNJjgGuBJ4OfA94fVX14kKlST4DvAR4CvAj4H3A/2CZetuDlD+jmaLjF8Cbq2pPF3VPrFD/S2iGthfwXeAtkxOSSf4I+D3gAM1w+L+ae9FTkvwW8H+Am3jkOg3vpZl7uvdtsEr9b2AAbZDkN2ku/LqF5ktTV1bVH7ev5SuAJwPXA/+6qh5M8ljg0zTzg98LnF1Vd3RT/eYYWl9rPzt/9rP2sxthP9sYWl+7Vut9fXVS5CZK8hLgP1bVq1fah7usb6OS7AQ+CRwO3AG8mfZ1y4jaMskHgH9F00deD/xbmuuljKo9F9kQ+9yVjnE7LGlZ6zmu7arGpdZ7LNsHQ3x/PZRj1q6t9zh1U/5PQy1JkiRJkiRJkiT1ndMPSpIkSZIkSZIkqfcMtSRJkiRJkiRJktR7hlqSJEmSJEmSJEnqPUMtSZIkSZIkSZIk9Z6hliRJkiRJkiRJknrPUEuSJEmSJEmSJEm9Z6glSZIkSZIkSZKk3jPUkiRJkiRJkiRJUu/9f4dTWem0ldh9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 2160x432 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvmcYjdZR-FW",
        "colab_type": "text"
      },
      "source": [
        "# Small animation to understand env step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbXaXY1rRYw5",
        "colab_type": "code",
        "outputId": "95270dda-9715-4bbf-cfe5-232760bcb420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "plt.figure()\n",
        "env = CarEnv()\n",
        "env.reset()\n",
        "for i in range(20):\n",
        "    img = env.get_state().cpu().squeeze(0).numpy()\n",
        "    plt.imshow(img, cmap='gray', animated=True, vmin=0, vmax=1)\n",
        "    plt.show()\n",
        "    sleep(0.2)\n",
        "    clear_output(wait=True)\n",
        "    vel = np.random.randint(low=0.5, high=5)\n",
        "    angle = np.random.randint(low=-10, high=+10)\n",
        "    env.step((angle,))\n"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5YdPG4HXnNsh"
      },
      "source": [
        "## Set seeds and get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z3RufYec_ADj",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.state_dim\n",
        "action_dim = env.action_dim[0]\n",
        "max_action = env.max_action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HWEgDAQxnbem"
      },
      "source": [
        "## Create the policy network\n",
        "* This sets up all the networks: Actor, Actor-Target, Critic1, Critic1-Target, Critic2, Critic2-Target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wTVvG7F8_EWg",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZI60VN2Unklh"
      },
      "source": [
        "## Initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sd-ZsdXR_LgV",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1L4Jra3KwED",
        "colab_type": "text"
      },
      "source": [
        "## Verify policy network before training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwS4ffh4VKUN",
        "colab_type": "code",
        "outputId": "914a694e-614a-44fc-ceba-4c52a809a7e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "obs = env.reset()\n",
        "action = policy.select_action(obs)\n",
        "new_obs, reward, done,  = env.step(action)\n",
        "print(obs.shape, new_obs.shape, action, reward, done)"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(torch.Size([1, 32, 32]), torch.Size([1, 32, 32]), array([-1.6164031], dtype=float32), 1, False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ka-ZRtQvjBex"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qabqiYdp9wDM",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    episode_timesteps = 0\n",
        "    while not done:\n",
        "      action = policy.select_action(obs)\n",
        "      obs, reward, done = env.step(action)\n",
        "      episode_timesteps += 1\n",
        "      avg_reward += reward\n",
        "      if episode_timesteps + 1 == env._max_episode_steps:\n",
        "        done = True\n",
        "\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QYOpCyiDnw7s"
      },
      "source": [
        "## Define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dhC_5XJ__Orp",
        "colab": {}
      },
      "source": [
        "evaluations = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "31n5eb03p-Fm"
      },
      "source": [
        "## Initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1vN5EvxK_QhT",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q9gsjvtPqLgT"
      },
      "source": [
        "## Training process\n",
        "* As part of training, we show following output:\n",
        "  * Total number of timesteps taken, Episode number, Length of current episod, Total reward accumulated during this episode and Time taken to finish this episode\n",
        "```\n",
        "Total Timesteps: 561 Episode Num: 1 Episode Len: 561 Reward: -449.7 Time Taken: 8.64677596092 secs\n",
        "```\n",
        "  * Distribution of rewards. This is not from replay memory, but from the actual steps taken during this run of episode. It shows how much and from where rewards were allocated. Like:\n",
        "    * `road (+1)` means it got +1 reward for being on road and for going towards final goal\n",
        "    * `road (-0.1)` means it got -0.1 reward for being on road but not going towards final goal (this is living penalty)\n",
        "    * `sand (-1)` means it got -1 reward for being on sand\n",
        "    * `wall (-5)` means it got -5 reward for hitting the wall\n",
        "    * Note: we assign a reward of -10 if we hit max episode steps\n",
        "    ```\n",
        "('Rewards Distribution: ', [('road (+1)', 33), ('road (-0.1)', 76), ('sand (-1)', 472), ('wall (-5)', 1)])\n",
        "    ```\n",
        "  * Amound of time taken by `policy.train` to finish \n",
        "```\n",
        "Training took 24.9262328148 secs\n",
        "```\n",
        "  * Path. Set of coordinates noted at every 100 steps, to verify that the network starts from a random point and covers lot of new points, rather than being stuck in a loop \n",
        "```\n",
        "('Path', [[783, 379], [784, 377], [757, 255], [647, 167], [598, 167], [545, 192], [561, 86]])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPn31gvhKSAp",
        "colab_type": "code",
        "outputId": "8074fdaa-f662-4d9f-868e-35e52fd56fd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "\n",
        "max_timesteps = 500000\n",
        "start_time = time.time()\n",
        "path = []\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Episode Len: {} Reward: {} Time Taken: {} secs\".format(\n",
        "              total_timesteps, episode_num, episode_timesteps,\n",
        "              episode_reward, time.time() - start_time))\n",
        "      print(\"Rewards Distribution: \", sorted(env.rewards_distribution.items()))\n",
        "      env.rewards_distribution = Counter()\n",
        "      start_time = time.time()\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "      print(\"Training took {} secs\".format(time.time() - start_time))\n",
        "      print(\"Path\", path)\n",
        "      print(\"\")\n",
        "      start_time = time.time()\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "      start_time = time.time()\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    path = [env.pos]\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.random_action()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(obs)\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_dim)).clip(-env.max_angle, env.max_angle)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done = env.step(action)\n",
        "  if episode_timesteps % 100 == 0:\n",
        "    path.append(env.pos)\n",
        "  #if fill_replay:\n",
        "  #  print(env.pos, action, reward, done)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  # done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  if episode_timesteps + 1 == env._max_episode_steps:\n",
        "    done = True\n",
        "    reward = -10\n",
        "    env.rewards_distribution[\"max-episodes (-10)\"] += 1\n",
        "  done_bool = float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs.cpu(), new_obs.cpu(), action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 561 Episode Num: 1 Episode Len: 561 Reward: -449.7 Time Taken: 8.64677596092 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 33), ('road (-0.1)', 76), ('sand (-1)', 472), ('wall (-5)', 1)])\n",
            "Training took 24.9262328148 secs\n",
            "('Path', [[783, 379], [784, 377], [757, 255], [647, 167], [598, 167], [545, 192], [561, 86]])\n",
            "\n",
            "Total Timesteps: 1774 Episode Num: 2 Episode Len: 1213 Reward: -908.8 Time Taken: 18.8063337803 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 101), ('road (-0.1)', 118), ('sand (-1)', 993), ('wall (-5)', 1)])\n",
            "Training took 53.7945930958 secs\n",
            "('Path', [[836, 598], [837, 598], [913, 573], [892, 473], [755, 379], [628, 295], [517, 293], [417, 277], [336, 294], [236, 246], [136, 232], [65, 242], [87, 152], [31, 41]])\n",
            "\n",
            "Total Timesteps: 2399 Episode Num: 3 Episode Len: 625 Reward: -479.4 Time Taken: 9.66949796677 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 19), ('road (-0.1)', 124), ('sand (-1)', 481), ('wall (-5)', 1)])\n",
            "Training took 27.7509810925 secs\n",
            "('Path', [[692, 298], [690, 297], [585, 183], [486, 85], [351, 91], [239, 119], [161, 132], [156, 43]])\n",
            "\n",
            "Total Timesteps: 2716 Episode Num: 4 Episode Len: 317 Reward: -243.4 Time Taken: 4.87486600876 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 19), ('road (-0.1)', 44), ('sand (-1)', 253), ('wall (-5)', 1)])\n",
            "Training took 14.0564718246 secs\n",
            "('Path', [[202, 366], [202, 367], [220, 312], [157, 208], [50, 150]])\n",
            "\n",
            "Total Timesteps: 2851 Episode Num: 5 Episode Len: 135 Reward: -124.4 Time Taken: 2.1001188755 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 1), ('road (-0.1)', 14), ('sand (-1)', 119), ('wall (-5)', 1)])\n",
            "Training took 5.97995591164 secs\n",
            "('Path', [[340, 170], [340, 168], [323, 56]])\n",
            "\n",
            "Total Timesteps: 3766 Episode Num: 6 Episode Len: 915 Reward: -767.3 Time Taken: 14.1049008369 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 7), ('road (-0.1)', 153), ('sand (-1)', 754), ('wall (-5)', 1)])\n",
            "Training took 40.6258499622 secs\n",
            "('Path', [[946, 566], [947, 564], [954, 457], [849, 385], [730, 398], [637, 429], [527, 386], [384, 328], [273, 253], [171, 143], [71, 33]])\n",
            "\n",
            "Total Timesteps: 3810 Episode Num: 7 Episode Len: 44 Reward: -14 Time Taken: 0.696173191071 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 17), ('sand (-1)', 26), ('wall (-5)', 1)])\n",
            "Training took 1.9458990097 secs\n",
            "('Path', [[804, 55], [806, 55]])\n",
            "\n",
            "Total Timesteps: 4577 Episode Num: 8 Episode Len: 767 Reward: -663.8 Time Taken: 11.8766319752 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 41), ('road (-0.1)', 28), ('sand (-1)', 697), ('wall (-5)', 1)])\n",
            "Training took 34.0043759346 secs\n",
            "('Path', [[654, 423], [652, 422], [547, 417], [472, 417], [471, 407], [478, 295], [381, 207], [319, 184], [344, 68]])\n",
            "\n",
            "Total Timesteps: 4844 Episode Num: 9 Episode Len: 267 Reward: -230.5 Time Taken: 4.1907889843 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 18), ('road (-0.1)', 5), ('sand (-1)', 243), ('wall (-5)', 1)])\n",
            "Training took 11.8805990219 secs\n",
            "('Path', [[1086, 30], [1085, 31], [1060, 38], [1051, 39]])\n",
            "\n",
            "Total Timesteps: 5283 Episode Num: 10 Episode Len: 439 Reward: -196.5 Time Taken: 6.87474417686 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 85), ('road (-0.1)', 85), ('sand (-1)', 268), ('wall (-5)', 1)])\n",
            "Training took 19.4680678844 secs\n",
            "('Path', [[873, 190], [871, 190], [718, 122], [604, 89], [591, 163], [604, 62]])\n",
            "\n",
            "Total Timesteps: 5888 Episode Num: 11 Episode Len: 605 Reward: -540.4 Time Taken: 9.39162707329 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 28), ('road (-0.1)', 14), ('sand (-1)', 562), ('wall (-5)', 1)])\n",
            "Training took 26.8381299973 secs\n",
            "('Path', [[349, 280], [347, 280], [245, 280], [184, 280], [184, 251], [194, 204], [212, 122], [122, 20]])\n",
            "\n",
            "Total Timesteps: 6178 Episode Num: 12 Episode Len: 290 Reward: -162.7 Time Taken: 4.54905200005 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 40), ('road (-0.1)', 57), ('sand (-1)', 192), ('wall (-5)', 1)])\n",
            "Training took 12.8686068058 secs\n",
            "('Path', [[556, 308], [557, 308], [593, 269], [565, 144]])\n",
            "\n",
            "Total Timesteps: 6584 Episode Num: 13 Episode Len: 406 Reward: -387.3 Time Taken: 6.32056093216 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 1), ('road (-0.1)', 23), ('sand (-1)', 381), ('wall (-5)', 1)])\n",
            "Training took 18.0190868378 secs\n",
            "('Path', [[1068, 174], [1066, 174], [1015, 174], [1015, 174], [1015, 142], [995, 28]])\n",
            "\n",
            "Total Timesteps: 7103 Episode Num: 14 Episode Len: 519 Reward: -476.5 Time Taken: 8.12261581421 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 12), ('road (-0.1)', 25), ('sand (-1)', 481), ('wall (-5)', 1)])\n",
            "Training took 23.0068838596 secs\n",
            "('Path', [[114, 278], [115, 279], [105, 294], [105, 294], [105, 259], [88, 148], [88, 37]])\n",
            "\n",
            "Total Timesteps: 7867 Episode Num: 15 Episode Len: 764 Reward: -691.9 Time Taken: 11.9629528522 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 25), ('road (-0.1)', 29), ('sand (-1)', 709), ('wall (-5)', 1)])\n",
            "Training took 33.8974270821 secs\n",
            "('Path', [[328, 360], [328, 358], [262, 283], [162, 283], [67, 303], [56, 303], [44, 303], [44, 246], [60, 133]])\n",
            "\n",
            "Total Timesteps: 8587 Episode Num: 16 Episode Len: 720 Reward: -470.6 Time Taken: 11.2349729538 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 88), ('road (-0.1)', 86), ('sand (-1)', 545), ('wall (-5)', 1)])\n",
            "Training took 31.9444360733 secs\n",
            "('Path', [[570, 309], [571, 310], [588, 410], [602, 373], [596, 262], [592, 171], [592, 168], [592, 149], [622, 38]])\n",
            "\n",
            "Total Timesteps: 8701 Episode Num: 17 Episode Len: 114 Reward: -12.2 Time Taken: 1.81585288048 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 52), ('road (-0.1)', 2), ('sand (-1)', 59), ('wall (-5)', 1)])\n",
            "Training took 5.04291796684 secs\n",
            "('Path', [[848, 100], [849, 101], [891, 37]])\n",
            "\n",
            "Total Timesteps: 10226 Episode Num: 18 Episode Len: 1525 Reward: -1200.0 Time Taken: 24.3114039898 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 61), ('road (-0.1)', 230), ('sand (-1)', 1233), ('wall (-5)', 1)])\n",
            "Training took 67.6059188843 secs\n",
            "('Path', [[1087, 319], [1085, 317], [971, 233], [861, 219], [772, 232], [752, 313], [649, 318], [609, 321], [638, 233], [528, 147], [430, 162], [335, 200], [238, 204], [152, 219], [137, 221], [79, 158], [28, 109]])\n",
            "\n",
            "Total Timesteps: 10574 Episode Num: 19 Episode Len: 348 Reward: -259.1 Time Taken: 6.06139183044 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 28), ('road (-0.1)', 41), ('sand (-1)', 278), ('wall (-5)', 1)])\n",
            "Training took 15.448210001 secs\n",
            "('Path', [[191, 381], [191, 379], [144, 336], [91, 277], [42, 213]])\n",
            "\n",
            "Total Timesteps: 10987 Episode Num: 20 Episode Len: 413 Reward: -335.4 Time Taken: 7.13236999512 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 21), ('road (-0.1)', 44), ('sand (-1)', 347), ('wall (-5)', 1)])\n",
            "Training took 18.3044011593 secs\n",
            "('Path', [[228, 543], [229, 541], [181, 496], [127, 441], [75, 379], [21, 326]])\n",
            "\n",
            "Total Timesteps: 11238 Episode Num: 21 Episode Len: 251 Reward: -230.6 Time Taken: 4.41972494125 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 5), ('road (-0.1)', 16), ('sand (-1)', 229), ('wall (-5)', 1)])\n",
            "Training took 11.1579301357 secs\n",
            "('Path', [[155, 422], [153, 423], [94, 356], [39, 305]])\n",
            "\n",
            "Total Timesteps: 11592 Episode Num: 22 Episode Len: 354 Reward: -274.0 Time Taken: 6.19506812096 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 24), ('road (-0.1)', 40), ('sand (-1)', 289), ('wall (-5)', 1)])\n",
            "Training took 15.6881539822 secs\n",
            "('Path', [[235, 186], [236, 185], [179, 147], [133, 99], [87, 45]])\n",
            "\n",
            "Total Timesteps: 12365 Episode Num: 23 Episode Len: 773 Reward: -512.6 Time Taken: 13.3767108917 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 80), ('road (-0.1)', 116), ('sand (-1)', 576), ('wall (-5)', 1)])\n",
            "Training took 34.2974059582 secs\n",
            "('Path', [[822, 407], [821, 408], [761, 363], [707, 316], [667, 267], [621, 215], [564, 156], [514, 102], [468, 60]])\n",
            "\n",
            "Total Timesteps: 12402 Episode Num: 24 Episode Len: 37 Reward: -9.0 Time Taken: 0.658946990967 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 7), ('road (-0.1)', 20), ('sand (-1)', 9), ('wall (-5)', 1)])\n",
            "Training took 1.64443802834 secs\n",
            "('Path', [[491, 46], [491, 44]])\n",
            "\n",
            "Total Timesteps: 12544 Episode Num: 25 Episode Len: 142 Reward: -146 Time Taken: 2.51625204086 secs\n",
            "('Rewards Distribution: ', [('sand (-1)', 141), ('wall (-5)', 1)])\n",
            "Training took 6.29236793518 secs\n",
            "('Path', [[90, 552], [90, 550], [42, 504]])\n",
            "\n",
            "Total Timesteps: 12620 Episode Num: 26 Episode Len: 76 Reward: -26.0 Time Taken: 1.41825914383 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 9), ('road (-0.1)', 40), ('sand (-1)', 26), ('wall (-5)', 1)])\n",
            "Training took 3.35710692406 secs\n",
            "('Path', [[634, 81], [633, 79]])\n",
            "\n",
            "Total Timesteps: 13140 Episode Num: 27 Episode Len: 520 Reward: -441.5 Time Taken: 9.10885286331 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 21), ('road (-0.1)', 45), ('sand (-1)', 453), ('wall (-5)', 1)])\n",
            "Training took 23.0616071224 secs\n",
            "('Path', [[1294, 318], [1292, 317], [1243, 258], [1192, 204], [1131, 147], [1080, 101], [1039, 45]])\n",
            "\n",
            "Total Timesteps: 13149 Episode Num: 28 Episode Len: 9 Reward: -4.7 Time Taken: 0.170606851578 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 1), ('road (-0.1)', 7), ('wall (-5)', 1)])\n",
            "Training took 0.393776893616 secs\n",
            "('Path', [[1308, 34], [1306, 33]])\n",
            "\n",
            "Total Timesteps: 13385 Episode Num: 29 Episode Len: 236 Reward: -154.2 Time Taken: 4.079611063 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 33), ('road (-0.1)', 22), ('sand (-1)', 180), ('wall (-5)', 1)])\n",
            "Training took 10.461523056 secs\n",
            "('Path', [[669, 132], [667, 132], [615, 73], [579, 25]])\n",
            "\n",
            "Total Timesteps: 14134 Episode Num: 30 Episode Len: 749 Reward: -583.6 Time Taken: 12.9753110409 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 46), ('road (-0.1)', 86), ('sand (-1)', 616), ('wall (-5)', 1)])\n",
            "Training took 33.2350649834 secs\n",
            "('Path', [[795, 430], [794, 431], [729, 369], [675, 321], [628, 268], [587, 209], [528, 156], [473, 105], [422, 49]])\n",
            "\n",
            "Total Timesteps: 14297 Episode Num: 31 Episode Len: 163 Reward: -162.1 Time Taken: 2.81508207321 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 2), ('road (-0.1)', 1), ('sand (-1)', 159), ('wall (-5)', 1)])\n",
            "Training took 7.26427292824 secs\n",
            "('Path', [[90, 129], [90, 127], [45, 79]])\n",
            "\n",
            "Total Timesteps: 14865 Episode Num: 32 Episode Len: 568 Reward: -470.4 Time Taken: 9.77796697617 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 31), ('road (-0.1)', 44), ('sand (-1)', 492), ('wall (-5)', 1)])\n",
            "Training took 25.1648671627 secs\n",
            "('Path', [[353, 297], [351, 297], [303, 251], [252, 205], [198, 155], [147, 101], [101, 50]])\n",
            "\n",
            "Total Timesteps: 14979 Episode Num: 33 Episode Len: 114 Reward: -70.7 Time Taken: 1.98136019707 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 16), ('road (-0.1)', 17), ('sand (-1)', 80), ('wall (-5)', 1)])\n",
            "Training took 5.04208302498 secs\n",
            "('Path', [[718, 77], [716, 75], [667, 32]])\n",
            "\n",
            "Total Timesteps: 15583 Episode Num: 34 Episode Len: 604 Reward: -447.6 Time Taken: 10.4271187782 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 46), ('road (-0.1)', 76), ('sand (-1)', 481), ('wall (-5)', 1)])\n",
            "Training took 26.8092031479 secs\n",
            "('Path', [[661, 338], [659, 338], [614, 281], [561, 235], [507, 185], [452, 128], [406, 68], [357, 22]])\n",
            "\n",
            "Total Timesteps: 15762 Episode Num: 35 Episode Len: 179 Reward: -94.1 Time Taken: 3.14510989189 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 26), ('road (-0.1)', 41), ('sand (-1)', 111), ('wall (-5)', 1)])\n",
            "Training took 7.9240398407 secs\n",
            "('Path', [[816, 105], [814, 103], [759, 70]])\n",
            "\n",
            "Total Timesteps: 15978 Episode Num: 36 Episode Len: 216 Reward: -173.8 Time Taken: 3.76435899734 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 15), ('road (-0.1)', 18), ('sand (-1)', 182), ('wall (-5)', 1)])\n",
            "Training took 9.59466600418 secs\n",
            "('Path', [[166, 124], [164, 122], [119, 79], [65, 32]])\n",
            "\n",
            "Total Timesteps: 17016 Episode Num: 37 Episode Len: 1038 Reward: -753.8 Time Taken: 17.9726190567 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 82), ('road (-0.1)', 138), ('sand (-1)', 817), ('wall (-5)', 1)])\n",
            "Training took 45.9830341339 secs\n",
            "('Path', [[722, 538], [720, 539], [683, 490], [628, 448], [571, 401], [517, 347], [469, 287], [423, 241], [370, 195], [313, 144], [262, 85], [211, 38]])\n",
            "\n",
            "Total Timesteps: 17899 Episode Num: 38 Episode Len: 883 Reward: -715.6 Time Taken: 15.2061719894 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 38), ('road (-0.1)', 106), ('sand (-1)', 738), ('wall (-5)', 1)])\n",
            "Training took 39.17867589 secs\n",
            "('Path', [[1043, 505], [1044, 504], [992, 441], [943, 377], [897, 328], [843, 290], [789, 232], [732, 187], [684, 134], [633, 88]])\n",
            "\n",
            "Total Timesteps: 18096 Episode Num: 39 Episode Len: 197 Reward: -81.8 Time Taken: 3.43335318565 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 38), ('road (-0.1)', 48), ('sand (-1)', 110), ('wall (-5)', 1)])\n",
            "Training took 8.77288603783 secs\n",
            "('Path', [[604, 120], [605, 119], [546, 77]])\n",
            "\n",
            "Total Timesteps: 18805 Episode Num: 40 Episode Len: 709 Reward: -577.9 Time Taken: 12.242702961 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 41), ('road (-0.1)', 59), ('sand (-1)', 608), ('wall (-5)', 1)])\n",
            "Training took 31.421202898 secs\n",
            "('Path', [[373, 440], [374, 441], [322, 381], [276, 330], [228, 284], [174, 237], [124, 173], [72, 118], [27, 72]])\n",
            "\n",
            "Total Timesteps: 19214 Episode Num: 41 Episode Len: 409 Reward: -321.6 Time Taken: 7.10282707214 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 25), ('road (-0.1)', 46), ('sand (-1)', 337), ('wall (-5)', 1)])\n",
            "Training took 18.1651930809 secs\n",
            "('Path', [[1078, 232], [1078, 233], [1032, 175], [986, 128], [924, 93], [871, 21]])\n",
            "\n",
            "Total Timesteps: 19629 Episode Num: 42 Episode Len: 415 Reward: -274.0 Time Taken: 7.28841686249 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 41), ('road (-0.1)', 70), ('sand (-1)', 303), ('wall (-5)', 1)])\n",
            "Training took 18.4006719589 secs\n",
            "('Path', [[1146, 253], [1147, 253], [1101, 190], [1047, 130], [1001, 84], [943, 38]])\n",
            "\n",
            "Total Timesteps: 19818 Episode Num: 43 Episode Len: 189 Reward: -173.2 Time Taken: 3.31226921082 secs\n",
            "('Rewards Distribution: ', [('road (-0.1)', 22), ('sand (-1)', 166), ('wall (-5)', 1)])\n",
            "Training took 8.40625810623 secs\n",
            "('Path', [[139, 320], [137, 318], [80, 275]])\n",
            "\n",
            "Total Timesteps: 20421 Episode Num: 44 Episode Len: 603 Reward: -529.6 Time Taken: 10.4697759151 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 18), ('road (-0.1)', 46), ('sand (-1)', 538), ('wall (-5)', 1)])\n",
            "Training took 26.7378270626 secs\n",
            "('Path', [[457, 322], [455, 320], [405, 273], [353, 234], [291, 168], [244, 115], [193, 67], [139, 21]])\n",
            "\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -312.450000\n",
            "---------------------------------------\n",
            "Total Timesteps: 20876 Episode Num: 45 Episode Len: 455 Reward: -324.6 Time Taken: 7.9187810421 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 371), ('road (-0.1)', 631), ('sand (-1)', 3702), ('wall (-5)', 11)])\n",
            "Training took 20.1190481186 secs\n",
            "('Path', [[812, 284], [813, 285], [767, 232], [722, 180], [679, 130], [612, 72]])\n",
            "\n",
            "Total Timesteps: 21460 Episode Num: 46 Episode Len: 584 Reward: -478.1 Time Taken: 10.1702408791 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 32), ('road (-0.1)', 51), ('sand (-1)', 500), ('wall (-5)', 1)])\n",
            "Training took 25.8680889606 secs\n",
            "('Path', [[359, 323], [359, 321], [300, 266], [246, 212], [201, 158], [155, 113], [101, 66]])\n",
            "\n",
            "Total Timesteps: 21490 Episode Num: 47 Episode Len: 30 Reward: -2.3 Time Taken: 0.558529853821 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 10), ('road (-0.1)', 13), ('sand (-1)', 6), ('wall (-5)', 1)])\n",
            "Training took 1.32982110977 secs\n",
            "('Path', [[1058, 29], [1056, 29]])\n",
            "\n",
            "Total Timesteps: 21622 Episode Num: 48 Episode Len: 132 Reward: -98.8 Time Taken: 2.33247494698 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 15), ('road (-0.1)', 8), ('sand (-1)', 108), ('wall (-5)', 1)])\n",
            "Training took 5.83860182762 secs\n",
            "('Path', [[313, 90], [312, 91], [278, 37]])\n",
            "\n",
            "Total Timesteps: 22151 Episode Num: 49 Episode Len: 529 Reward: -476.9 Time Taken: 9.17141795158 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 15), ('road (-0.1)', 29), ('sand (-1)', 484), ('wall (-5)', 1)])\n",
            "Training took 23.4970769882 secs\n",
            "('Path', [[602, 300], [602, 301], [554, 250], [509, 202], [458, 156], [404, 96], [348, 34]])\n",
            "\n",
            "Total Timesteps: 22939 Episode Num: 50 Episode Len: 788 Reward: -601.1 Time Taken: 13.7041239738 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 50), ('road (-0.1)', 101), ('sand (-1)', 636), ('wall (-5)', 1)])\n",
            "Training took 34.9280400276 secs\n",
            "('Path', [[828, 425], [826, 424], [776, 375], [721, 326], [661, 271], [603, 215], [549, 172], [501, 116], [434, 70]])\n",
            "\n",
            "Total Timesteps: 22953 Episode Num: 51 Episode Len: 14 Reward: 8 Time Taken: 0.265920877457 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 13), ('wall (-5)', 1)])\n",
            "Training took 0.608292818069 secs\n",
            "('Path', [[529, 26], [529, 27]])\n",
            "\n",
            "Total Timesteps: 23711 Episode Num: 52 Episode Len: 758 Reward: -556.1 Time Taken: 13.2002389431 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 53), ('road (-0.1)', 111), ('sand (-1)', 593), ('wall (-5)', 1)])\n",
            "Training took 33.6301469803 secs\n",
            "('Path', [[832, 413], [832, 411], [774, 360], [708, 309], [667, 248], [624, 203], [564, 159], [509, 97], [458, 44]])\n",
            "\n",
            "Total Timesteps: 24290 Episode Num: 53 Episode Len: 579 Reward: -428.7 Time Taken: 10.0794119835 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 47), ('road (-0.1)', 67), ('sand (-1)', 464), ('wall (-5)', 1)])\n",
            "Training took 25.7143158913 secs\n",
            "('Path', [[523, 317], [521, 315], [474, 273], [420, 224], [356, 171], [323, 117], [276, 70]])\n",
            "\n",
            "Total Timesteps: 24405 Episode Num: 54 Episode Len: 115 Reward: -118.1 Time Taken: 2.00563097 secs\n",
            "('Rewards Distribution: ', [('road (-0.1)', 1), ('sand (-1)', 113), ('wall (-5)', 1)])\n",
            "Training took 5.10504007339 secs\n",
            "('Path', [[80, 126], [79, 124], [26, 77]])\n",
            "\n",
            "Total Timesteps: 24519 Episode Num: 55 Episode Len: 114 Reward: -53.5 Time Taken: 2.06586503983 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 21), ('road (-0.1)', 25), ('sand (-1)', 67), ('wall (-5)', 1)])\n",
            "Training took 5.05741000175 secs\n",
            "('Path', [[944, 88], [943, 86], [893, 42]])\n",
            "\n",
            "Total Timesteps: 24627 Episode Num: 56 Episode Len: 108 Reward: -19.7 Time Taken: 1.98267292976 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 25), ('road (-0.1)', 47), ('sand (-1)', 35), ('wall (-5)', 1)])\n",
            "Training took 4.78305602074 secs\n",
            "('Path', [[1053, 77], [1052, 75], [987, 31]])\n",
            "\n",
            "Total Timesteps: 24628 Episode Num: 57 Episode Len: 1 Reward: -5 Time Taken: 0.0409021377563 secs\n",
            "('Rewards Distribution: ', [('wall (-5)', 1)])\n",
            "Training took 0.047022819519 secs\n",
            "('Path', [[20, 152], [20, 150]])\n",
            "\n",
            "Total Timesteps: 24724 Episode Num: 58 Episode Len: 96 Reward: -53.4 Time Taken: 1.75859999657 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 17), ('road (-0.1)', 14), ('sand (-1)', 64), ('wall (-5)', 1)])\n",
            "Training took 4.24771499634 secs\n",
            "('Path', [[917, 77], [917, 78]])\n",
            "\n",
            "Total Timesteps: 25552 Episode Num: 59 Episode Len: 828 Reward: -688.0 Time Taken: 14.6527421474 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 36), ('road (-0.1)', 80), ('sand (-1)', 711), ('wall (-5)', 1)])\n",
            "Training took 36.7241270542 secs\n",
            "('Path', [[716, 459], [715, 457], [643, 403], [588, 359], [538, 297], [493, 247], [443, 201], [391, 141], [334, 79], [288, 27]])\n",
            "\n",
            "Total Timesteps: 26501 Episode Num: 60 Episode Len: 949 Reward: -803.5 Time Taken: 16.7946810722 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 41), ('road (-0.1)', 75), ('sand (-1)', 832), ('wall (-5)', 1)])\n",
            "Training took 42.1004738808 secs\n",
            "('Path', [[1018, 533], [1019, 531], [963, 472], [912, 417], [864, 355], [816, 309], [747, 265], [699, 200], [653, 146], [602, 92], [548, 46]])\n",
            "\n",
            "Total Timesteps: 27749 Episode Num: 61 Episode Len: 1248 Reward: -833.5 Time Taken: 21.8885300159 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 126), ('road (-0.1)', 185), ('sand (-1)', 936), ('wall (-5)', 1)])\n",
            "Training took 55.3619289398 secs\n",
            "('Path', [[799, 619], [797, 619], [758, 577], [707, 534], [648, 486], [594, 434], [549, 382], [501, 336], [447, 288], [394, 234], [344, 178], [302, 134], [247, 99], [187, 40]])\n",
            "\n",
            "Total Timesteps: 28092 Episode Num: 62 Episode Len: 343 Reward: -190.9 Time Taken: 6.02796387672 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 47), ('road (-0.1)', 69), ('sand (-1)', 226), ('wall (-5)', 1)])\n",
            "Training took 15.2344639301 secs\n",
            "('Path', [[349, 196], [350, 197], [300, 142], [253, 89], [201, 43]])\n",
            "\n",
            "Total Timesteps: 28291 Episode Num: 63 Episode Len: 199 Reward: -130.2 Time Taken: 3.50170302391 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 22), ('road (-0.1)', 32), ('sand (-1)', 144), ('wall (-5)', 1)])\n",
            "Training took 8.83116698265 secs\n",
            "('Path', [[123, 286], [124, 285], [54, 226]])\n",
            "\n",
            "Total Timesteps: 28944 Episode Num: 64 Episode Len: 653 Reward: -544.0 Time Taken: 11.4963231087 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 34), ('road (-0.1)', 50), ('sand (-1)', 568), ('wall (-5)', 1)])\n",
            "Training took 28.9404730797 secs\n",
            "('Path', [[591, 373], [590, 371], [533, 321], [479, 261], [429, 207], [382, 152], [333, 106], [279, 57]])\n",
            "\n",
            "Total Timesteps: 29426 Episode Num: 65 Episode Len: 482 Reward: -397.3 Time Taken: 8.43390989304 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 25), ('road (-0.1)', 43), ('sand (-1)', 413), ('wall (-5)', 1)])\n",
            "Training took 21.4048631191 secs\n",
            "('Path', [[583, 287], [584, 287], [528, 221], [483, 166], [437, 120], [384, 64]])\n",
            "\n",
            "Total Timesteps: 29817 Episode Num: 66 Episode Len: 391 Reward: -372.8 Time Taken: 6.82675909996 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 3), ('road (-0.1)', 18), ('sand (-1)', 369), ('wall (-5)', 1)])\n",
            "Training took 17.3338630199 secs\n",
            "('Path', [[1063, 233], [1062, 231], [1011, 186], [957, 135], [907, 69]])\n",
            "\n",
            "Total Timesteps: 30773 Episode Num: 67 Episode Len: 956 Reward: -707.5 Time Taken: 16.6350109577 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 70), ('road (-0.1)', 125), ('sand (-1)', 760), ('wall (-5)', 1)])\n",
            "Training took 42.3944039345 secs\n",
            "('Path', [[831, 520], [831, 521], [781, 469], [735, 420], [681, 371], [617, 323], [565, 258], [519, 206], [471, 160], [417, 111], [365, 46]])\n",
            "\n",
            "Total Timesteps: 31176 Episode Num: 68 Episode Len: 403 Reward: -319.8 Time Taken: 7.08516597748 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 22), ('road (-0.1)', 48), ('sand (-1)', 332), ('wall (-5)', 1)])\n",
            "Training took 17.8980967999 secs\n",
            "('Path', [[1106, 230], [1105, 228], [1041, 185], [987, 133], [928, 73], [896, 21]])\n",
            "\n",
            "Total Timesteps: 31208 Episode Num: 69 Episode Len: 32 Reward: -15.3 Time Taken: 0.580872058868 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 9), ('road (-0.1)', 3), ('sand (-1)', 19), ('wall (-5)', 1)])\n",
            "Training took 1.40716099739 secs\n",
            "('Path', [[31, 416], [32, 417]])\n",
            "\n",
            "Total Timesteps: 31911 Episode Num: 70 Episode Len: 703 Reward: -554.1 Time Taken: 12.2900671959 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 49), ('road (-0.1)', 61), ('sand (-1)', 592), ('wall (-5)', 1)])\n",
            "Training took 31.1524209976 secs\n",
            "('Path', [[366, 363], [364, 364], [328, 319], [279, 273], [224, 225], [170, 165], [121, 112], [74, 66], [20, 20]])\n",
            "\n",
            "Total Timesteps: 32747 Episode Num: 71 Episode Len: 836 Reward: -646.3 Time Taken: 14.574657917 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 55), ('road (-0.1)', 93), ('sand (-1)', 687), ('wall (-5)', 1)])\n",
            "Training took 37.0899260044 secs\n",
            "('Path', [[710, 460], [710, 458], [644, 407], [590, 354], [537, 297], [491, 251], [437, 205], [387, 143], [332, 81], [294, 40]])\n",
            "\n",
            "Total Timesteps: 32971 Episode Num: 72 Episode Len: 224 Reward: -152.4 Time Taken: 3.94103097916 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 27), ('road (-0.1)', 24), ('sand (-1)', 172), ('wall (-5)', 1)])\n",
            "Training took 9.94872498512 secs\n",
            "('Path', [[1299, 133], [1300, 134], [1262, 75], [1216, 24]])\n",
            "\n",
            "Total Timesteps: 33522 Episode Num: 73 Episode Len: 551 Reward: -490.9 Time Taken: 9.66583299637 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 19), ('road (-0.1)', 29), ('sand (-1)', 502), ('wall (-5)', 1)])\n",
            "Training took 24.4190270901 secs\n",
            "('Path', [[297, 373], [298, 371], [245, 312], [194, 258], [149, 195], [101, 149], [41, 100]])\n",
            "\n",
            "Total Timesteps: 33527 Episode Num: 74 Episode Len: 5 Reward: -4.3 Time Taken: 0.106472015381 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 1), ('road (-0.1)', 3), ('wall (-5)', 1)])\n",
            "Training took 0.218156099319 secs\n",
            "('Path', [[29, 228], [27, 228]])\n",
            "\n",
            "Total Timesteps: 34367 Episode Num: 75 Episode Len: 840 Reward: -713.2 Time Taken: 14.765237093 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 33), ('road (-0.1)', 72), ('sand (-1)', 734), ('wall (-5)', 1)])\n",
            "Training took 37.2589669228 secs\n",
            "('Path', [[679, 466], [679, 464], [614, 414], [554, 368], [502, 305], [456, 257], [405, 212], [355, 152], [296, 94], [260, 47]])\n",
            "\n",
            "Total Timesteps: 35520 Episode Num: 76 Episode Len: 1153 Reward: -978.8 Time Taken: 20.2731909752 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 54), ('road (-0.1)', 78), ('sand (-1)', 1020), ('wall (-5)', 1)])\n",
            "Training took 51.1152260303 secs\n",
            "('Path', [[744, 621], [742, 622], [694, 565], [645, 519], [580, 470], [528, 416], [483, 363], [438, 306], [384, 260], [326, 209], [281, 146], [235, 100], [181, 53]])\n",
            "\n",
            "Total Timesteps: 35601 Episode Num: 77 Episode Len: 81 Reward: -58.0 Time Taken: 1.43966794014 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 9), ('road (-0.1)', 10), ('sand (-1)', 61), ('wall (-5)', 1)])\n",
            "Training took 3.58521699905 secs\n",
            "('Path', [[65, 407], [63, 407]])\n",
            "\n",
            "Total Timesteps: 36274 Episode Num: 78 Episode Len: 673 Reward: -499.7 Time Taken: 11.8046529293 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 54), ('road (-0.1)', 77), ('sand (-1)', 541), ('wall (-5)', 1)])\n",
            "Training took 29.8720309734 secs\n",
            "('Path', [[577, 380], [578, 381], [520, 315], [485, 265], [436, 219], [375, 179], [325, 114], [279, 61]])\n",
            "\n",
            "Total Timesteps: 36751 Episode Num: 79 Episode Len: 477 Reward: -366.1 Time Taken: 8.47403812408 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 30), ('road (-0.1)', 61), ('sand (-1)', 385), ('wall (-5)', 1)])\n",
            "Training took 21.1652739048 secs\n",
            "('Path', [[774, 278], [775, 276], [711, 219], [658, 164], [606, 102], [560, 56]])\n",
            "\n",
            "Total Timesteps: 37233 Episode Num: 80 Episode Len: 482 Reward: -384.8 Time Taken: 8.40215778351 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 29), ('road (-0.1)', 48), ('sand (-1)', 404), ('wall (-5)', 1)])\n",
            "Training took 21.3538630009 secs\n",
            "('Path', [[259, 366], [260, 365], [207, 313], [158, 259], [114, 210], [66, 169]])\n",
            "\n",
            "Total Timesteps: 37862 Episode Num: 81 Episode Len: 629 Reward: -513.8 Time Taken: 10.989525795 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 38), ('road (-0.1)', 48), ('sand (-1)', 542), ('wall (-5)', 1)])\n",
            "Training took 27.9113359451 secs\n",
            "('Path', [[437, 327], [436, 328], [402, 277], [360, 241], [295, 194], [245, 130], [195, 76], [149, 29]])\n",
            "\n",
            "Total Timesteps: 37928 Episode Num: 82 Episode Len: 66 Reward: -28.1 Time Taken: 1.16879487038 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 7), ('road (-0.1)', 31), ('sand (-1)', 27), ('wall (-5)', 1)])\n",
            "Training took 2.91833686829 secs\n",
            "('Path', [[619, 71], [617, 72]])\n",
            "\n",
            "Total Timesteps: 38739 Episode Num: 83 Episode Len: 811 Reward: -576.2 Time Taken: 14.0799908638 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 78), ('road (-0.1)', 92), ('sand (-1)', 640), ('wall (-5)', 1)])\n",
            "Training took 35.9707760811 secs\n",
            "('Path', [[685, 441], [686, 442], [639, 381], [593, 329], [550, 288], [496, 241], [442, 187], [394, 123], [348, 77], [295, 32]])\n",
            "\n",
            "Total Timesteps: 39140 Episode Num: 84 Episode Len: 401 Reward: -312.3 Time Taken: 7.08658194542 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 27), ('road (-0.1)', 43), ('sand (-1)', 330), ('wall (-5)', 1)])\n",
            "Training took 17.8026838303 secs\n",
            "('Path', [[224, 564], [224, 562], [173, 521], [107, 467], [61, 413], [20, 357]])\n",
            "\n",
            "Total Timesteps: 39277 Episode Num: 85 Episode Len: 137 Reward: -105.3 Time Taken: 2.39190888405 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 12), ('road (-0.1)', 13), ('sand (-1)', 111), ('wall (-5)', 1)])\n",
            "Training took 6.07042789459 secs\n",
            "('Path', [[83, 222], [83, 223], [44, 163]])\n",
            "\n",
            "Total Timesteps: 39364 Episode Num: 86 Episode Len: 87 Reward: -55.7 Time Taken: 1.54033279419 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 10), ('road (-0.1)', 17), ('sand (-1)', 59), ('wall (-5)', 1)])\n",
            "Training took 3.86074209213 secs\n",
            "('Path', [[945, 75], [946, 75]])\n",
            "\n",
            "Total Timesteps: 40543 Episode Num: 87 Episode Len: 1179 Reward: -979.9 Time Taken: 20.6022119522 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 57), ('road (-0.1)', 99), ('sand (-1)', 1022), ('wall (-5)', 1)])\n",
            "Training took 52.2922070026 secs\n",
            "('Path', [[721, 600], [719, 599], [672, 554], [618, 507], [561, 459], [515, 404], [469, 358], [416, 312], [361, 259], [302, 203], [257, 153], [204, 112], [152, 57]])\n",
            "\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -320.940000\n",
            "---------------------------------------\n",
            "Total Timesteps: 40674 Episode Num: 88 Episode Len: 131 Reward: -63.5 Time Taken: 2.28816890717 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 315), ('road (-0.1)', 519), ('sand (-1)', 3481), ('wall (-5)', 11)])\n",
            "Training took 5.81406807899 secs\n",
            "('Path', [[562, 85], [560, 84], [512, 35]])\n",
            "\n",
            "Total Timesteps: 40683 Episode Num: 89 Episode Len: 9 Reward: -1.4 Time Taken: 0.178138971329 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 4), ('road (-0.1)', 4), ('wall (-5)', 1)])\n",
            "Training took 0.393981933594 secs\n",
            "('Path', [[1408, 187], [1407, 188]])\n",
            "\n",
            "Total Timesteps: 40817 Episode Num: 90 Episode Len: 134 Reward: -97.8 Time Taken: 2.39681100845 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 12), ('road (-0.1)', 18), ('sand (-1)', 103), ('wall (-5)', 1)])\n",
            "Training took 5.9109480381 secs\n",
            "('Path', [[856, 100], [855, 101], [817, 34]])\n",
            "\n",
            "Total Timesteps: 40903 Episode Num: 91 Episode Len: 86 Reward: -66.8 Time Taken: 1.52114486694 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 8), ('road (-0.1)', 8), ('sand (-1)', 69), ('wall (-5)', 1)])\n",
            "Training took 3.7820520401 secs\n",
            "('Path', [[1001, 78], [1002, 78]])\n",
            "\n",
            "Total Timesteps: 41056 Episode Num: 92 Episode Len: 153 Reward: -134.4 Time Taken: 2.69760107994 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 5), ('road (-0.1)', 14), ('sand (-1)', 133), ('wall (-5)', 1)])\n",
            "Training took 6.79447412491 secs\n",
            "('Path', [[761, 116], [761, 114], [707, 53]])\n",
            "\n",
            "Total Timesteps: 41855 Episode Num: 93 Episode Len: 799 Reward: -613.6 Time Taken: 14.0044119358 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 56), ('road (-0.1)', 86), ('sand (-1)', 656), ('wall (-5)', 1)])\n",
            "Training took 35.3673670292 secs\n",
            "('Path', [[682, 441], [682, 439], [623, 391], [565, 345], [521, 279], [475, 232], [422, 187], [370, 124], [321, 70]])\n",
            "\n",
            "Total Timesteps: 42865 Episode Num: 94 Episode Len: 1010 Reward: -737.8 Time Taken: 17.589525938 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 85), ('road (-0.1)', 118), ('sand (-1)', 806), ('wall (-5)', 1)])\n",
            "Training took 44.867372036 secs\n",
            "('Path', [[1017, 515], [1018, 513], [965, 458], [903, 405], [865, 352], [817, 306], [760, 268], [700, 206], [654, 152], [601, 103], [567, 81], [510, 21]])\n",
            "\n",
            "Total Timesteps: 43110 Episode Num: 95 Episode Len: 245 Reward: -215.1 Time Taken: 4.39909982681 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 3), ('road (-0.1)', 31), ('sand (-1)', 210), ('wall (-5)', 1)])\n",
            "Training took 10.882078886 secs\n",
            "('Path', [[816, 159], [815, 157], [744, 100], [690, 38]])\n",
            "\n",
            "Total Timesteps: 44194 Episode Num: 96 Episode Len: 1084 Reward: -805.9 Time Taken: 19.2427680492 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 83), ('road (-0.1)', 129), ('sand (-1)', 871), ('wall (-5)', 1)])\n",
            "Training took 48.0540821552 secs\n",
            "('Path', [[853, 587], [852, 585], [790, 538], [736, 487], [684, 422], [632, 372], [590, 335], [528, 277], [476, 223], [430, 170], [384, 114], [330, 68]])\n",
            "\n",
            "Total Timesteps: 44918 Episode Num: 97 Episode Len: 724 Reward: -434.9 Time Taken: 12.8741259575 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 84), ('road (-0.1)', 139), ('sand (-1)', 500), ('wall (-5)', 1)])\n",
            "Training took 32.1223709583 secs\n",
            "('Path', [[865, 409], [863, 407], [815, 366], [761, 317], [705, 253], [662, 191], [615, 145], [551, 98], [478, 37]])\n",
            "\n",
            "Total Timesteps: 46044 Episode Num: 98 Episode Len: 1126 Reward: -1018.5 Time Taken: 19.7497751713 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 31), ('road (-0.1)', 55), ('sand (-1)', 1039), ('wall (-5)', 1)])\n",
            "Training took 49.9274349213 secs\n",
            "('Path', [[604, 615], [602, 616], [553, 563], [504, 518], [450, 469], [399, 415], [342, 365], [292, 315], [237, 269], [183, 215], [141, 151], [90, 103], [37, 57]])\n",
            "\n",
            "Total Timesteps: 46262 Episode Num: 99 Episode Len: 218 Reward: -194.3 Time Taken: 3.82092905045 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 8), ('road (-0.1)', 13), ('sand (-1)', 196), ('wall (-5)', 1)])\n",
            "Training took 9.66267490387 secs\n",
            "('Path', [[135, 353], [133, 353], [85, 299], [34, 253]])\n",
            "\n",
            "Total Timesteps: 46545 Episode Num: 100 Episode Len: 283 Reward: -236.1 Time Taken: 5.01615405083 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 16), ('road (-0.1)', 21), ('sand (-1)', 245), ('wall (-5)', 1)])\n",
            "Training took 12.5499351025 secs\n",
            "('Path', [[168, 381], [168, 382], [121, 322], [75, 273]])\n",
            "\n",
            "Total Timesteps: 47293 Episode Num: 101 Episode Len: 748 Reward: -531.4 Time Taken: 13.2988090515 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 68), ('road (-0.1)', 94), ('sand (-1)', 585), ('wall (-5)', 1)])\n",
            "Training took 33.1403520107 secs\n",
            "('Path', [[700, 400], [698, 401], [656, 347], [601, 307], [545, 255], [492, 201], [442, 141], [401, 102], [350, 48]])\n",
            "\n",
            "Total Timesteps: 48008 Episode Num: 102 Episode Len: 715 Reward: -616.1 Time Taken: 12.6463809013 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 33), ('road (-0.1)', 41), ('sand (-1)', 640), ('wall (-5)', 1)])\n",
            "Training took 31.7502200603 secs\n",
            "('Path', [[377, 473], [376, 474], [332, 415], [290, 360], [236, 313], [182, 261], [128, 198], [82, 148], [29, 97]])\n",
            "\n",
            "Total Timesteps: 48901 Episode Num: 103 Episode Len: 893 Reward: -632.3 Time Taken: 15.9618678093 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 77), ('road (-0.1)', 123), ('sand (-1)', 692), ('wall (-5)', 1)])\n",
            "Training took 39.5965688229 secs\n",
            "('Path', [[1082, 468], [1080, 467], [1039, 425], [979, 386], [931, 326], [880, 261], [831, 215], [771, 169], [702, 122], [658, 61]])\n",
            "\n",
            "Total Timesteps: 49323 Episode Num: 104 Episode Len: 422 Reward: -292.8 Time Taken: 7.40319681168 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 36), ('road (-0.1)', 68), ('sand (-1)', 317), ('wall (-5)', 1)])\n",
            "Training took 18.7310230732 secs\n",
            "('Path', [[876, 224], [874, 223], [821, 179], [759, 135], [698, 87], [656, 23]])\n",
            "\n",
            "Total Timesteps: 49335 Episode Num: 105 Episode Len: 12 Reward: -1.7 Time Taken: 0.23654794693 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 4), ('road (-0.1)', 7), ('wall (-5)', 1)])\n",
            "Training took 0.526606082916 secs\n",
            "('Path', [[234, 41], [235, 40]])\n",
            "\n",
            "Total Timesteps: 50119 Episode Num: 106 Episode Len: 784 Reward: -656.6 Time Taken: 13.7850489616 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 45), ('road (-0.1)', 46), ('sand (-1)', 692), ('wall (-5)', 1)])\n",
            "Training took 34.7303640842 secs\n",
            "('Path', [[398, 605], [398, 606], [352, 552], [307, 504], [255, 458], [201, 407], [156, 344], [107, 295], [58, 250]])\n",
            "\n",
            "Total Timesteps: 50191 Episode Num: 107 Episode Len: 72 Reward: -35.9 Time Taken: 1.29024410248 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 16), ('road (-0.1)', 9), ('sand (-1)', 46), ('wall (-5)', 1)])\n",
            "Training took 3.18500089645 secs\n",
            "('Path', [[45, 403], [43, 404]])\n",
            "\n",
            "Total Timesteps: 50291 Episode Num: 108 Episode Len: 100 Reward: -65.7 Time Taken: 1.82093095779 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 16), ('road (-0.1)', 7), ('sand (-1)', 76), ('wall (-5)', 1)])\n",
            "Training took 4.42209291458 secs\n",
            "('Path', [[984, 65], [982, 66]])\n",
            "\n",
            "Total Timesteps: 51200 Episode Num: 109 Episode Len: 909 Reward: -685.7 Time Taken: 16.1568338871 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 70), ('road (-0.1)', 97), ('sand (-1)', 741), ('wall (-5)', 1)])\n",
            "Training took 40.3176620007 secs\n",
            "('Path', [[1054, 474], [1055, 473], [1001, 421], [944, 360], [898, 311], [844, 262], [790, 212], [728, 159], [674, 101], [634, 61], [598, 25]])\n",
            "\n",
            "Total Timesteps: 51286 Episode Num: 110 Episode Len: 86 Reward: -35.3 Time Taken: 1.53195309639 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 17), ('road (-0.1)', 23), ('sand (-1)', 45), ('wall (-5)', 1)])\n",
            "Training took 3.7995800972 secs\n",
            "('Path', [[1009, 65], [1009, 66]])\n",
            "\n",
            "Total Timesteps: 52016 Episode Num: 111 Episode Len: 730 Reward: -650.6 Time Taken: 12.8634250164 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 21), ('road (-0.1)', 46), ('sand (-1)', 662), ('wall (-5)', 1)])\n",
            "Training took 32.3845100403 secs\n",
            "('Path', [[1164, 412], [1165, 413], [1116, 347], [1062, 302], [1020, 253], [965, 200], [911, 146], [859, 83], [806, 30]])\n",
            "\n",
            "Total Timesteps: 53060 Episode Num: 112 Episode Len: 1044 Reward: -769.0 Time Taken: 18.4654500484 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 81), ('road (-0.1)', 130), ('sand (-1)', 832), ('wall (-5)', 1)])\n",
            "Training took 46.2834749222 secs\n",
            "('Path', [[737, 557], [735, 557], [691, 505], [627, 456], [570, 404], [521, 349], [479, 290], [429, 244], [374, 194], [319, 141], [268, 88], [216, 44]])\n",
            "\n",
            "Total Timesteps: 53295 Episode Num: 113 Episode Len: 235 Reward: -185.3 Time Taken: 4.16238498688 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 21), ('road (-0.1)', 13), ('sand (-1)', 200), ('wall (-5)', 1)])\n",
            "Training took 10.433232069 secs\n",
            "('Path', [[447, 141], [446, 142], [412, 89], [368, 35]])\n",
            "\n",
            "Total Timesteps: 53298 Episode Num: 114 Episode Len: 3 Reward: -4.1 Time Taken: 0.0720438957214 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 1), ('road (-0.1)', 1), ('wall (-5)', 1)])\n",
            "Training took 0.129951000214 secs\n",
            "('Path', [[1136, 25], [1135, 23]])\n",
            "\n",
            "Total Timesteps: 53479 Episode Num: 115 Episode Len: 181 Reward: -152.6 Time Taken: 3.25742578506 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 9), ('road (-0.1)', 16), ('sand (-1)', 155), ('wall (-5)', 1)])\n",
            "Training took 8.01384305954 secs\n",
            "('Path', [[114, 397], [112, 396], [63, 349]])\n",
            "\n",
            "Total Timesteps: 54120 Episode Num: 116 Episode Len: 641 Reward: -560.0 Time Taken: 11.3807020187 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 29), ('road (-0.1)', 30), ('sand (-1)', 581), ('wall (-5)', 1)])\n",
            "Training took 28.3955161572 secs\n",
            "('Path', [[329, 361], [329, 362], [289, 301], [243, 253], [194, 208], [141, 149], [86, 94], [41, 44]])\n",
            "\n",
            "Total Timesteps: 54566 Episode Num: 117 Episode Len: 446 Reward: -325.9 Time Taken: 7.87418603897 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 40), ('road (-0.1)', 49), ('sand (-1)', 356), ('wall (-5)', 1)])\n",
            "Training took 19.7824280262 secs\n",
            "('Path', [[348, 247], [347, 248], [302, 191], [254, 139], [200, 92], [145, 38]])\n",
            "\n",
            "Total Timesteps: 55281 Episode Num: 118 Episode Len: 715 Reward: -589.7 Time Taken: 12.6557981968 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 39), ('road (-0.1)', 57), ('sand (-1)', 618), ('wall (-5)', 1)])\n",
            "Training took 31.6865921021 secs\n",
            "('Path', [[387, 583], [388, 583], [334, 517], [288, 463], [242, 416], [186, 359], [129, 316], [80, 261], [33, 212]])\n",
            "\n",
            "Total Timesteps: 55503 Episode Num: 119 Episode Len: 222 Reward: -194.3 Time Taken: 4.00628304482 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 10), ('road (-0.1)', 13), ('sand (-1)', 198), ('wall (-5)', 1)])\n",
            "Training took 9.86040997505 secs\n",
            "('Path', [[136, 325], [135, 323], [74, 272], [24, 210]])\n",
            "\n",
            "Total Timesteps: 56137 Episode Num: 120 Episode Len: 634 Reward: -446.8 Time Taken: 11.2678539753 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 56), ('road (-0.1)', 88), ('sand (-1)', 489), ('wall (-5)', 1)])\n",
            "Training took 28.089443922 secs\n",
            "('Path', [[659, 340], [658, 341], [607, 291], [560, 245], [506, 199], [440, 149], [405, 92], [362, 34]])\n",
            "\n",
            "Total Timesteps: 57194 Episode Num: 121 Episode Len: 1057 Reward: -866.1 Time Taken: 18.5556049347 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 43), ('road (-0.1)', 121), ('sand (-1)', 892), ('wall (-5)', 1)])\n",
            "Training took 46.8691220284 secs\n",
            "('Path', [[849, 566], [849, 564], [783, 513], [729, 459], [676, 404], [622, 365], [565, 326], [508, 264], [459, 209], [412, 155], [356, 107], [298, 57]])\n",
            "\n",
            "Total Timesteps: 57654 Episode Num: 122 Episode Len: 460 Reward: -376.2 Time Taken: 8.21281003952 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 25), ('road (-0.1)', 42), ('sand (-1)', 392), ('wall (-5)', 1)])\n",
            "Training took 20.4128060341 secs\n",
            "('Path', [[1076, 260], [1075, 258], [1021, 206], [966, 156], [916, 102], [863, 36]])\n",
            "\n",
            "Total Timesteps: 57765 Episode Num: 123 Episode Len: 111 Reward: -113.2 Time Taken: 1.97448611259 secs\n",
            "('Rewards Distribution: ', [('road (-0.1)', 2), ('sand (-1)', 108), ('wall (-5)', 1)])\n",
            "Training took 4.91704797745 secs\n",
            "('Path', [[79, 558], [77, 556], [25, 508]])\n",
            "\n",
            "Total Timesteps: 58562 Episode Num: 124 Episode Len: 797 Reward: -623.3 Time Taken: 14.1058650017 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 47), ('road (-0.1)', 93), ('sand (-1)', 656), ('wall (-5)', 1)])\n",
            "Training took 35.3491239548 secs\n",
            "('Path', [[661, 413], [659, 412], [608, 369], [548, 330], [493, 276], [446, 222], [400, 174], [345, 129], [291, 78]])\n",
            "\n",
            "Total Timesteps: 59762 Episode Num: 125 Episode Len: 1200 Reward: -808.9 Time Taken: 21.0457339287 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 117), ('road (-0.1)', 179), ('sand (-1)', 903), ('wall (-5)', 1)])\n",
            "Training took 53.2016210556 secs\n",
            "('Path', [[795, 621], [794, 619], [746, 579], [682, 526], [636, 464], [596, 417], [537, 370], [473, 325], [433, 265], [387, 214], [339, 171], [284, 120], [228, 79]])\n",
            "\n",
            "Total Timesteps: 60719 Episode Num: 126 Episode Len: 957 Reward: -621.8 Time Taken: 16.9526200294 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 103), ('road (-0.1)', 148), ('sand (-1)', 705), ('wall (-5)', 1)])\n",
            "Training took 42.4455471039 secs\n",
            "('Path', [[819, 508], [819, 509], [774, 447], [729, 398], [674, 352], [605, 314], [564, 255], [518, 204], [469, 158], [415, 109], [366, 46]])\n",
            "\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -347.520000\n",
            "---------------------------------------\n",
            "Total Timesteps: 60884 Episode Num: 127 Episode Len: 165 Reward: -66.2 Time Taken: 2.94799304008 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 453), ('road (-0.1)', 694), ('sand (-1)', 3870), ('wall (-5)', 11)])\n",
            "Training took 7.32081198692 secs\n",
            "('Path', [[643, 77], [642, 78], [624, 41]])\n",
            "\n",
            "Total Timesteps: 61102 Episode Num: 128 Episode Len: 218 Reward: -170.6 Time Taken: 3.93019390106 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 14), ('road (-0.1)', 26), ('sand (-1)', 177), ('wall (-5)', 1)])\n",
            "Training took 9.6433839798 secs\n",
            "('Path', [[1309, 138], [1310, 137], [1259, 77], [1207, 22]])\n",
            "\n",
            "Total Timesteps: 61492 Episode Num: 129 Episode Len: 390 Reward: -325.7 Time Taken: 6.9257311821 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 22), ('road (-0.1)', 27), ('sand (-1)', 340), ('wall (-5)', 1)])\n",
            "Training took 17.2229659557 secs\n",
            "('Path', [[210, 374], [210, 375], [174, 319], [133, 278], [72, 233]])\n",
            "\n",
            "Total Timesteps: 61540 Episode Num: 130 Episode Len: 48 Reward: -35.6 Time Taken: 0.870165109634 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 1), ('road (-0.1)', 16), ('sand (-1)', 30), ('wall (-5)', 1)])\n",
            "Training took 2.11963486671 secs\n",
            "('Path', [[497, 57], [497, 55]])\n",
            "\n",
            "Total Timesteps: 62575 Episode Num: 131 Episode Len: 1035 Reward: -868.3 Time Taken: 18.5492479801 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 48), ('road (-0.1)', 83), ('sand (-1)', 903), ('wall (-5)', 1)])\n",
            "Training took 45.9248468876 secs\n",
            "('Path', [[593, 525], [591, 526], [544, 476], [495, 430], [440, 381], [384, 327], [333, 282], [287, 236], [227, 196], [178, 136], [132, 82], [87, 35]])\n",
            "\n",
            "Total Timesteps: 62660 Episode Num: 132 Episode Len: 85 Reward: -67.2 Time Taken: 1.51107501984 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 10), ('road (-0.1)', 2), ('sand (-1)', 72), ('wall (-5)', 1)])\n",
            "Training took 3.76181006432 secs\n",
            "('Path', [[435, 69], [436, 70]])\n",
            "\n",
            "Total Timesteps: 62932 Episode Num: 133 Episode Len: 272 Reward: -210.1 Time Taken: 4.787088871 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 19), ('road (-0.1)', 31), ('sand (-1)', 221), ('wall (-5)', 1)])\n",
            "Training took 12.0731048584 secs\n",
            "('Path', [[161, 547], [162, 546], [105, 480], [58, 426]])\n",
            "\n",
            "Total Timesteps: 63279 Episode Num: 134 Episode Len: 347 Reward: -277.9 Time Taken: 6.14169096947 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 19), ('road (-0.1)', 39), ('sand (-1)', 288), ('wall (-5)', 1)])\n",
            "Training took 15.3932919502 secs\n",
            "('Path', [[331, 186], [329, 185], [278, 138], [217, 92], [163, 38]])\n",
            "\n",
            "Total Timesteps: 63290 Episode Num: 135 Episode Len: 11 Reward: 5 Time Taken: 0.21764922142 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 10), ('wall (-5)', 1)])\n",
            "Training took 0.483299016953 secs\n",
            "('Path', [[1064, 25], [1065, 26]])\n",
            "\n",
            "Total Timesteps: 63407 Episode Num: 136 Episode Len: 117 Reward: -105.3 Time Taken: 2.12827682495 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 2), ('road (-0.1)', 13), ('sand (-1)', 101), ('wall (-5)', 1)])\n",
            "Training took 5.18451905251 secs\n",
            "('Path', [[237, 94], [238, 92], [179, 31]])\n",
            "\n",
            "Total Timesteps: 64017 Episode Num: 137 Episode Len: 610 Reward: -528.3 Time Taken: 10.9846770763 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 28), ('road (-0.1)', 33), ('sand (-1)', 548), ('wall (-5)', 1)])\n",
            "Training took 27.0586688519 secs\n",
            "('Path', [[1266, 320], [1264, 320], [1228, 275], [1177, 229], [1122, 179], [1072, 125], [1019, 82], [974, 29]])\n",
            "\n",
            "Total Timesteps: 64220 Episode Num: 138 Episode Len: 203 Reward: -168.3 Time Taken: 3.61778187752 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 9), ('road (-0.1)', 23), ('sand (-1)', 170), ('wall (-5)', 1)])\n",
            "Training took 9.00748205185 secs\n",
            "('Path', [[133, 263], [134, 264], [73, 200], [23, 145]])\n",
            "\n",
            "Total Timesteps: 64310 Episode Num: 139 Episode Len: 90 Reward: -78.2 Time Taken: 1.59423089027 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 7), ('road (-0.1)', 2), ('sand (-1)', 80), ('wall (-5)', 1)])\n",
            "Training took 3.97964382172 secs\n",
            "('Path', [[62, 230], [63, 229]])\n",
            "\n",
            "Total Timesteps: 65350 Episode Num: 140 Episode Len: 1040 Reward: -873.2 Time Taken: 18.5537550449 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 44), ('road (-0.1)', 92), ('sand (-1)', 903), ('wall (-5)', 1)])\n",
            "Training took 46.1442639828 secs\n",
            "('Path', [[695, 550], [694, 551], [649, 496], [604, 449], [540, 401], [485, 349], [432, 295], [386, 246], [328, 208], [275, 147], [219, 91], [173, 40]])\n",
            "\n",
            "Total Timesteps: 65888 Episode Num: 141 Episode Len: 538 Reward: -469.8 Time Taken: 9.5094640255 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 19), ('road (-0.1)', 38), ('sand (-1)', 480), ('wall (-5)', 1)])\n",
            "Training took 23.8738780022 secs\n",
            "('Path', [[1088, 304], [1086, 303], [1038, 255], [978, 204], [924, 150], [872, 93], [833, 40]])\n",
            "\n",
            "Total Timesteps: 66582 Episode Num: 142 Episode Len: 694 Reward: -551.9 Time Taken: 12.247303009 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 42), ('road (-0.1)', 69), ('sand (-1)', 582), ('wall (-5)', 1)])\n",
            "Training took 30.7608850002 secs\n",
            "('Path', [[373, 461], [373, 459], [316, 408], [259, 355], [212, 301], [167, 254], [112, 213], [61, 156]])\n",
            "\n",
            "Total Timesteps: 67087 Episode Num: 143 Episode Len: 505 Reward: -408.2 Time Taken: 8.92365193367 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 27), ('road (-0.1)', 52), ('sand (-1)', 425), ('wall (-5)', 1)])\n",
            "Training took 22.4343440533 secs\n",
            "('Path', [[1099, 291], [1098, 289], [1035, 229], [981, 177], [933, 123], [887, 73], [834, 23]])\n",
            "\n",
            "Total Timesteps: 67663 Episode Num: 144 Episode Len: 576 Reward: -363.5 Time Taken: 10.1640059948 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 70), ('road (-0.1)', 85), ('sand (-1)', 420), ('wall (-5)', 1)])\n",
            "Training took 25.5220799446 secs\n",
            "('Path', [[790, 293], [790, 291], [719, 248], [670, 183], [618, 121], [586, 91], [553, 63]])\n",
            "\n",
            "Total Timesteps: 68381 Episode Num: 145 Episode Len: 718 Reward: -503.1 Time Taken: 12.694535017 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 64), ('road (-0.1)', 101), ('sand (-1)', 552), ('wall (-5)', 1)])\n",
            "Training took 31.86292696 secs\n",
            "('Path', [[721, 397], [719, 395], [670, 350], [616, 292], [562, 238], [517, 186], [466, 144], [410, 92], [357, 27]])\n",
            "\n",
            "Total Timesteps: 68390 Episode Num: 146 Episode Len: 9 Reward: -4.7 Time Taken: 0.184202194214 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 1), ('road (-0.1)', 7), ('wall (-5)', 1)])\n",
            "Training took 0.395066022873 secs\n",
            "('Path', [[1156, 34], [1155, 32]])\n",
            "\n",
            "Total Timesteps: 68947 Episode Num: 147 Episode Len: 557 Reward: -518.1 Time Taken: 9.90477800369 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 12), ('road (-0.1)', 21), ('sand (-1)', 523), ('wall (-5)', 1)])\n",
            "Training took 24.7318279743 secs\n",
            "('Path', [[293, 553], [294, 554], [243, 500], [198, 448], [155, 410], [92, 353], [39, 299]])\n",
            "\n",
            "Total Timesteps: 69507 Episode Num: 148 Episode Len: 560 Reward: -452.6 Time Taken: 10.0456008911 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 35), ('road (-0.1)', 46), ('sand (-1)', 478), ('wall (-5)', 1)])\n",
            "Training took 24.8115229607 secs\n",
            "('Path', [[530, 314], [531, 315], [488, 256], [442, 203], [392, 162], [337, 111], [283, 56]])\n",
            "\n",
            "Total Timesteps: 70636 Episode Num: 149 Episode Len: 1129 Reward: -1001.1 Time Taken: 19.7512459755 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 34), ('road (-0.1)', 71), ('sand (-1)', 1023), ('wall (-5)', 1)])\n",
            "Training took 50.1000030041 secs\n",
            "('Path', [[605, 578], [603, 578], [552, 537], [500, 491], [445, 439], [396, 385], [349, 334], [299, 288], [245, 239], [189, 183], [141, 130], [91, 85], [37, 38]])\n",
            "\n",
            "Total Timesteps: 70644 Episode Num: 150 Episode Len: 8 Reward: -4.6 Time Taken: 0.16827082634 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 1), ('road (-0.1)', 6), ('wall (-5)', 1)])\n",
            "Training took 0.344186067581 secs\n",
            "('Path', [[1058, 34], [1058, 32]])\n",
            "\n",
            "Total Timesteps: 70987 Episode Num: 151 Episode Len: 343 Reward: -267.7 Time Taken: 6.13632798195 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 14), ('road (-0.1)', 57), ('sand (-1)', 271), ('wall (-5)', 1)])\n",
            "Training took 15.2297270298 secs\n",
            "('Path', [[766, 245], [768, 245], [711, 179], [664, 115], [609, 57]])\n",
            "\n",
            "Total Timesteps: 71080 Episode Num: 152 Episode Len: 93 Reward: -49.3 Time Taken: 1.66046214104 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 18), ('road (-0.1)', 13), ('sand (-1)', 61), ('wall (-5)', 1)])\n",
            "Training took 4.12129378319 secs\n",
            "('Path', [[929, 77], [930, 78]])\n",
            "\n",
            "Total Timesteps: 71747 Episode Num: 153 Episode Len: 667 Reward: -444.3 Time Taken: 11.8007099628 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 76), ('road (-0.1)', 83), ('sand (-1)', 507), ('wall (-5)', 1)])\n",
            "Training took 29.5799229145 secs\n",
            "('Path', [[326, 589], [326, 587], [288, 565], [250, 514], [204, 460], [158, 413], [106, 369], [52, 317]])\n",
            "\n",
            "Total Timesteps: 72428 Episode Num: 154 Episode Len: 681 Reward: -526.5 Time Taken: 12.0776479244 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 50), ('road (-0.1)', 65), ('sand (-1)', 565), ('wall (-5)', 1)])\n",
            "Training took 30.2322449684 secs\n",
            "('Path', [[1088, 366], [1087, 364], [1036, 321], [982, 270], [936, 206], [891, 156], [842, 110], [779, 70]])\n",
            "\n",
            "Total Timesteps: 73442 Episode Num: 155 Episode Len: 1014 Reward: -737.8 Time Taken: 17.9410748482 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 78), ('road (-0.1)', 138), ('sand (-1)', 797), ('wall (-5)', 1)])\n",
            "Training took 44.9347569942 secs\n",
            "('Path', [[719, 513], [717, 511], [661, 476], [607, 428], [551, 374], [501, 318], [460, 275], [406, 230], [342, 182], [305, 122], [255, 75], [198, 32]])\n",
            "\n",
            "Total Timesteps: 74620 Episode Num: 156 Episode Len: 1178 Reward: -1012.8 Time Taken: 20.6796090603 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 45), ('road (-0.1)', 88), ('sand (-1)', 1044), ('wall (-5)', 1)])\n",
            "Training took 52.2057168484 secs\n",
            "('Path', [[707, 603], [706, 604], [672, 554], [625, 508], [562, 466], [508, 412], [462, 358], [412, 310], [364, 273], [298, 220], [250, 165], [205, 109], [151, 60]])\n",
            "\n",
            "Total Timesteps: 75048 Episode Num: 157 Episode Len: 428 Reward: -319.7 Time Taken: 7.51943922043 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 35), ('road (-0.1)', 47), ('sand (-1)', 345), ('wall (-5)', 1)])\n",
            "Training took 19.0014929771 secs\n",
            "('Path', [[1086, 245], [1085, 246], [1039, 194], [992, 148], [938, 102], [885, 47]])\n",
            "\n",
            "Total Timesteps: 75297 Episode Num: 158 Episode Len: 249 Reward: -200.5 Time Taken: 4.39713096619 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 15), ('road (-0.1)', 25), ('sand (-1)', 208), ('wall (-5)', 1)])\n",
            "Training took 11.028318882 secs\n",
            "('Path', [[432, 143], [430, 142], [380, 96], [326, 49]])\n",
            "\n",
            "Total Timesteps: 75396 Episode Num: 159 Episode Len: 99 Reward: -68.3 Time Taken: 1.84453701973 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 16), ('road (-0.1)', 3), ('sand (-1)', 79), ('wall (-5)', 1)])\n",
            "Training took 4.38677620888 secs\n",
            "('Path', [[704, 69], [703, 70]])\n",
            "\n",
            "Total Timesteps: 76220 Episode Num: 160 Episode Len: 824 Reward: -648.1 Time Taken: 14.4717900753 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 49), ('road (-0.1)', 91), ('sand (-1)', 683), ('wall (-5)', 1)])\n",
            "Training took 36.5231280327 secs\n",
            "('Path', [[592, 439], [591, 440], [548, 383], [502, 337], [443, 290], [389, 236], [335, 177], [295, 132], [228, 87], [173, 36]])\n",
            "\n",
            "Total Timesteps: 76932 Episode Num: 161 Episode Len: 712 Reward: -553.7 Time Taken: 12.675951004 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 42), ('road (-0.1)', 87), ('sand (-1)', 582), ('wall (-5)', 1)])\n",
            "Training took 31.5889370441 secs\n",
            "('Path', [[764, 392], [763, 393], [721, 329], [668, 280], [609, 229], [546, 176], [495, 122], [449, 72], [398, 30]])\n",
            "\n",
            "Total Timesteps: 78111 Episode Num: 162 Episode Len: 1179 Reward: -899.2 Time Taken: 20.970913887 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 78), ('road (-0.1)', 142), ('sand (-1)', 958), ('wall (-5)', 1)])\n",
            "Training took 52.2828838825 secs\n",
            "('Path', [[772, 602], [772, 603], [730, 544], [692, 504], [631, 459], [575, 416], [524, 362], [475, 310], [427, 264], [373, 217], [315, 162], [265, 107], [208, 61]])\n",
            "\n",
            "Total Timesteps: 78795 Episode Num: 163 Episode Len: 684 Reward: -515.9 Time Taken: 12.144012928 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 46), ('road (-0.1)', 89), ('sand (-1)', 548), ('wall (-5)', 1)])\n",
            "Training took 30.340170145 secs\n",
            "('Path', [[663, 366], [661, 366], [614, 316], [552, 277], [498, 226], [449, 172], [403, 122], [350, 67]])\n",
            "\n",
            "Total Timesteps: 79195 Episode Num: 164 Episode Len: 400 Reward: -380.4 Time Taken: 7.23709392548 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 1), ('road (-0.1)', 24), ('sand (-1)', 374), ('wall (-5)', 1)])\n",
            "Training took 17.7625529766 secs\n",
            "('Path', [[1054, 244], [1054, 242], [999, 193], [945, 139], [894, 74]])\n",
            "\n",
            "Total Timesteps: 79408 Episode Num: 165 Episode Len: 213 Reward: -171.6 Time Taken: 3.88144803047 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 11), ('road (-0.1)', 26), ('sand (-1)', 175), ('wall (-5)', 1)])\n",
            "Training took 9.43744301796 secs\n",
            "('Path', [[131, 341], [131, 342], [85, 287], [40, 240]])\n",
            "\n",
            "Total Timesteps: 79875 Episode Num: 166 Episode Len: 467 Reward: -305.0 Time Taken: 8.38984107971 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 47), ('road (-0.1)', 80), ('sand (-1)', 339), ('wall (-5)', 1)])\n",
            "Training took 20.7106108665 secs\n",
            "('Path', [[887, 258], [887, 256], [826, 203], [768, 149], [730, 99], [680, 49]])\n",
            "\n",
            "Total Timesteps: 79900 Episode Num: 167 Episode Len: 25 Reward: -5.8 Time Taken: 0.467669010162 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 8), ('road (-0.1)', 8), ('sand (-1)', 8), ('wall (-5)', 1)])\n",
            "Training took 1.10710287094 secs\n",
            "('Path', [[1273, 38], [1272, 39]])\n",
            "\n",
            "Total Timesteps: 80701 Episode Num: 168 Episode Len: 801 Reward: -643.9 Time Taken: 14.2698168755 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 36), ('road (-0.1)', 99), ('sand (-1)', 665), ('wall (-5)', 1)])\n",
            "Training took 35.504224062 secs\n",
            "('Path', [[1129, 439], [1128, 437], [1069, 387], [1010, 335], [961, 280], [912, 220], [856, 183], [788, 136], [730, 78], [688, 20]])\n",
            "\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -339.640000\n",
            "---------------------------------------\n",
            "Total Timesteps: 81426 Episode Num: 169 Episode Len: 725 Reward: -662.8 Time Taken: 12.8290059566 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 274), ('road (-0.1)', 452), ('sand (-1)', 4233), ('wall (-5)', 11)])\n",
            "Training took 32.0823910236 secs\n",
            "('Path', [[400, 615], [398, 614], [347, 570], [294, 524], [240, 471], [192, 417], [145, 367], [83, 320], [29, 270]])\n",
            "\n",
            "Total Timesteps: 82141 Episode Num: 170 Episode Len: 715 Reward: -548.5 Time Taken: 12.7345321178 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 47), ('road (-0.1)', 85), ('sand (-1)', 582), ('wall (-5)', 1)])\n",
            "Training took 31.7299559116 secs\n",
            "('Path', [[989, 385], [987, 384], [942, 342], [888, 295], [836, 240], [786, 181], [737, 144], [678, 88], [628, 25]])\n",
            "\n",
            "Total Timesteps: 82586 Episode Num: 171 Episode Len: 445 Reward: -320.5 Time Taken: 7.9450879097 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 35), ('road (-0.1)', 65), ('sand (-1)', 344), ('wall (-5)', 1)])\n",
            "Training took 19.7851159573 secs\n",
            "('Path', [[258, 590], [259, 588], [202, 524], [141, 474], [95, 421], [44, 375]])\n",
            "\n",
            "Total Timesteps: 83322 Episode Num: 172 Episode Len: 736 Reward: -578.3 Time Taken: 13.4116361141 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 48), ('road (-0.1)', 73), ('sand (-1)', 614), ('wall (-5)', 1)])\n",
            "Training took 32.6619300842 secs\n",
            "('Path', [[661, 410], [662, 410], [605, 347], [558, 297], [512, 251], [458, 205], [402, 141], [348, 80], [309, 39]])\n",
            "\n",
            "Total Timesteps: 83483 Episode Num: 173 Episode Len: 161 Reward: -154.3 Time Taken: 2.91766214371 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 4), ('road (-0.1)', 3), ('sand (-1)', 153), ('wall (-5)', 1)])\n",
            "Training took 7.1659719944 secs\n",
            "('Path', [[211, 114], [212, 113], [162, 55]])\n",
            "\n",
            "Total Timesteps: 84037 Episode Num: 174 Episode Len: 554 Reward: -296.5 Time Taken: 9.74467992783 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 79), ('road (-0.1)', 115), ('sand (-1)', 359), ('wall (-5)', 1)])\n",
            "Training took 24.5724718571 secs\n",
            "('Path', [[768, 299], [767, 300], [723, 238], [678, 194], [624, 149], [560, 100], [507, 36]])\n",
            "\n",
            "Total Timesteps: 84197 Episode Num: 175 Episode Len: 160 Reward: -109.1 Time Taken: 2.91790294647 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 18), ('road (-0.1)', 21), ('sand (-1)', 120), ('wall (-5)', 1)])\n",
            "Training took 7.0925090313 secs\n",
            "('Path', [[746, 110], [747, 108], [694, 55]])\n",
            "\n",
            "Total Timesteps: 84621 Episode Num: 176 Episode Len: 424 Reward: -283.8 Time Taken: 7.78202414513 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 37), ('road (-0.1)', 78), ('sand (-1)', 308), ('wall (-5)', 1)])\n",
            "Training took 18.8379158974 secs\n",
            "('Path', [[722, 232], [720, 230], [669, 184], [606, 141], [541, 77], [506, 24]])\n",
            "\n",
            "Total Timesteps: 85288 Episode Num: 177 Episode Len: 667 Reward: -581.6 Time Taken: 11.8845059872 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 24), ('road (-0.1)', 46), ('sand (-1)', 596), ('wall (-5)', 1)])\n",
            "Training took 29.5894238949 secs\n",
            "('Path', [[372, 421], [372, 419], [305, 373], [255, 309], [208, 255], [166, 198], [113, 151], [53, 98]])\n",
            "\n",
            "Total Timesteps: 85500 Episode Num: 178 Episode Len: 212 Reward: -193.1 Time Taken: 3.78150391579 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 2), ('road (-0.1)', 21), ('sand (-1)', 188), ('wall (-5)', 1)])\n",
            "Training took 9.41770219803 secs\n",
            "('Path', [[1407, 134], [1405, 134], [1358, 86], [1295, 40]])\n",
            "\n",
            "Total Timesteps: 85589 Episode Num: 179 Episode Len: 89 Reward: -5.4 Time Taken: 1.59512901306 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 33), ('road (-0.1)', 24), ('sand (-1)', 31), ('wall (-5)', 1)])\n",
            "Training took 3.9403860569 secs\n",
            "('Path', [[513, 58], [514, 59]])\n",
            "\n",
            "Total Timesteps: 86714 Episode Num: 180 Episode Len: 1125 Reward: -982.7 Time Taken: 19.9767789841 secs\n",
            "('Rewards Distribution: ', [('road (+1)', 43), ('road (-0.1)', 67), ('sand (-1)', 1014), ('wall (-5)', 1)])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-214-c1272cd1f4e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training took {} secs\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-199-7d7278230edc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer, iterations, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Still once every two iterations, we update the weights of the Actor target by polyak averaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_param\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m           \u001b[0mtarget_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtarget_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Still once every two iterations, we update the weights of the Critic target by polyak averaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}